
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem'; 

# Installing the required tools

Before getting started with SageMaker HyperPod, we will configure our environment with the required tools. 

<details>
<summary>SageMaker Studio Integration (Optional)</summary>
:::note
This guide provides step-by-step instructions for setting up Amazon SageMaker Studio with Hyperpod on Amazon EKS, including FSx Lustre storage configuration..
:::

# SageMaker Studio + Hyperpod Integration Guide

![SageMaker Studio with Hyperpod integration](/img/14-studio-integration/01-studio-hyperpod-architecture.png)

## Table of Contents

1. [Prerequisites](#prerequisites)
2. [EKS Cluster Setup](#eks-cluster-setup)
3. [FSx for Lustre Configuration](#fsx-for-lustre-configuration)
5. [SageMaker Studio Domain Setup](#sagemaker-studio-domain-setup)
6. [SageMaker Studio IDE Configuration](#sagemaker-studio-ide-configuration)
7. [Setup EKS Access Entry](#setup-eks-access-entry)
8. [Setup EKS Cluster connection](#setup-eks-cluster-connection)
9. [Optional: Setup Hyperpod Task Governance and CloudWatch Observability](#optional-setup-hyperpod-task-governance-and-cloudwatch-observability)
10. [Optional: MLFlow Setup](#optional-mlflow-setup)

## Prerequisites

Before starting, ensure you have:

- AWS CLI configured with appropriate permissions
- Access to AWS Management Console
- Familiarity with Amazon EKS, SageMaker, and FSx Lustre

***

## EKS Cluster Setup

To create an Amazon EKS cluster, and setup a SageMaker Hyperpod one, you can follow one of these steps:

1. Option 1: [Initial Cluster Setup](./initial-cluster-setup.md)
2. Option 2: Using CloudFormation (see Infrastructure as Code section)

***

## FSx for Lustre Configuration

To create an Amazon FSx for Lustre volume, for being used by both EKS pods and SageMaker Studio, you can follow the steps [Set up your shared file system](./Set%20up%20your%20shared%20file%20system.md). The choice of Dynamic vs Static provisioning is yours!

***

## SageMaker Studio Domain Setup

You can deploy the CloudFormation template from the [awsome-distributed-training repository](https://github.com/aws-samples/awsome-distributed-training/blob/main/1.architectures/7.sagemaker-hyperpod-eks/cfn-templates/sagemaker-studio-fsx-stack.yaml), which creates the following resources:

1. SageMaker Studio domain
2. Lifecycle configurations for installing necessary packages for Studio IDE, such as *kubectl* and *jq*. Lifecycle configurations will be created for both JupyterLab and Code Editor
4. A Lambda function that:
    1. Associates the created `security-group-for-inbound-nfs` security group to the Studio domain
    2. Associates the `security-group-for-inbound-nfs` security group to the FSx for Lustre ENIs
    3. **Optional**: If  **SharedFSx** is set to **True**, creates the partition *shared* in the FSx for Lustre volume, and associates it to the Studio domain

![Shared FSx Partition](/img/14-studio-integration/07-fsx-shared.png)

5. If **SharedFSx** is set to **False**, a Lambda function that:
    1. Creates the partition */\{user_profile_name\}*, and associates it to the Studio user profile
    2. Creates an Event bridge rule that invokes the previously defined Lambda function each time a new user is created. 

![Partitioned FSx](/img/14-studio-integration/08-fsx-partitioned.png)

The CloudFormation template requires the following parameters:

1. EKSClusterName: Name of the EKS Cluster
2. ExistingFSxLustreId: Id of the created FSx for Lustre volume
3. FSxClaimName: Name of the claim created for the FSx for Lustre volume
4. ExistingVpcId: Dropdown menu for selecting the EKS cluster VPC
5. ExistingSubnetIds: Dropdown menu for selecting the EKS cluster **Private Subnet IDs**.

:::warning Important
While specifying the `ExistingSubnetIds`, please pass in only the subnet IDs associated with your ***HyperPod*** cluster, and not your EKS cluster. You can find this in your environment variables (`env_vars`) that you used to create the cluster, as `PRIVATE_SUBNET_ID`. Alternatively, you can check your console for private subnets with names `<PREFIX> Private Subnet 1` (correct) vs. `<PREFIX> EKS Cluster Private Subnet 1`.

Do not attempt to delete the CloudFormation stack before all EFA network interfaces have been removed, as they are dependencies of your private subnet and will preclude it from being deleted as a stack resource. This may take several minutes as the HyperPod nodes are shut down. 

:::


***

## SageMaker Studio IDE Configuration

As an admin user, once your SageMaker Studio Domain is provisioned, you may go in and create users as you see fit.

:::note
This step *DOES NOT* assume that you already have a Studio Domain. To create one, check out the next section titled **"SageMaker Studio Domain Setup"**.
![alt text](/img/14-studio-integration/09-studio-user.png)
:::

You can now select your preferred IDE from SageMaker Studio. 

![SageMaker Studio Home](/img/14-studio-integration/02-studio-home.png)

For the purpose of this workshop, we are going to create a Code Editor environment.

From the top-left menu:

1. Click on **Code Editor**
2. Click on **Create Code Editor Space**
3. Enter a name
4. Click on **Create Space**
5. From the **Attach custom filesystem - optional** dropdown menu, select the FSx for Lustre volume
6. From the **Lifecycle configuration** dropdown menu, select the available lifecycle configuration

![Code Editor setup](/img/14-studio-integration/03-codeditor-fsx.png)

Click on **Run Space**. Wait until the space is created, then click **Open Code Editor**

To verify that your file system was mounted, you can check if you have a path mounted in the Code Editor space `custom-file-system/fsx_lustre/<FSX_ID>`:

![Code Editor setup](/img/14-studio-integration/10-filesystem-check.png)

You can also run:
```bash
df -h
```

If you set `SharedFSx` to `False`, you can verify separate partitions for two users.
Example output from user1:
```
Filesystem                      Size  Used Avail Use% Mounted on
overlay                          37G  494M   37G   2% /
tmpfs                            64M     0   64M   0% /dev
tmpfs                           1.9G     0  1.9G   0% /sys/fs/cgroup
shm                             392M     0  392M   0% /dev/shm
/dev/nvme1n1                    5.0G  529M  4.5G  11% /home/sagemaker-user
/dev/nvme0n1p1                  180G   31G  150G  18% /opt/.sagemakerinternal
10.1.53.46@tcp:/ylacfb4v/aman1  1.2T  7.5M  1.2T   1% /mnt/custom-file-systems/fsx_lustre/fs-0104f3de83efe0f33
127.0.0.1:/                     8.0E     0  8.0E   0% /mnt/custom-file-systems/efs/fs-052756a07c3a5ba97_fsap-0b5e6e7c68f22fee3
tmpfs                           1.9G     0  1.9G   0% /proc/acpi
tmpfs                           1.9G     0  1.9G   0% /sys/firmware
```

Example output from user2:
```
Filesystem                      Size  Used Avail Use% Mounted on
overlay                          37G  478M   37G   2% /
tmpfs                            64M     0   64M   0% /dev
tmpfs                           1.9G     0  1.9G   0% /sys/fs/cgroup
shm                             392M     0  392M   0% /dev/shm
/dev/nvme0n1p1                  180G   31G  150G  18% /opt/.sagemakerinternal
/dev/nvme1n1                    5.0G  529M  4.5G  11% /home/sagemaker-user
127.0.0.1:/                     8.0E     0  8.0E   0% /mnt/custom-file-systems/efs/fs-052756a07c3a5ba97_fsap-0a323a3e5a27e1bdc
10.1.53.46@tcp:/ylacfb4v/aman2  1.2T  7.5M  1.2T   1% /mnt/custom-file-systems/fsx_lustre/fs-0104f3de83efe0f33
tmpfs                           1.9G     0  1.9G   0% /proc/acpi
tmpfs                           1.9G     0  1.9G   0% /sys/firmware
```

The difference here is the mountpoint for FSxl (`ylacfb4v`) has separate partitions set up. You can then `cd /mnt/custom-file-systems/fsx_lustre/fs-0104f3de83efe0f33` and write from each user and verify that the other user isn't able to see those files!

Alternatively, if you set `SharedFSx` to `True`, you can check the the mount using `df -h`, and it will show something like:
```
Filesystem                       Size  Used Avail Use% Mounted on
overlay                           37G  478M   37G   2% /
tmpfs                             64M     0   64M   0% /dev
tmpfs                            1.9G     0  1.9G   0% /sys/fs/cgroup
shm                              392M     0  392M   0% /dev/shm
/dev/nvme0n1p1                   180G   31G  150G  18% /opt/.sagemakerinternal
/dev/nvme1n1                     5.0G  529M  4.5G  11% /home/sagemaker-user
10.1.53.46@tcp:/ylacfb4v/shared  1.2T  7.5M  1.2T   1% /mnt/custom-file-systems/fsx_lustre/fs-0104f3de83efe0f33
127.0.0.1:/                      8.0E     0  8.0E   0% /mnt/custom-file-systems/efs/fs-0e16e272aba907ad3_fsap-08ae9b9f68be028d7
tmpfs                            1.9G     0  1.9G   0% /proc/acpi
tmpfs                            1.9G     0  1.9G   0% /sys/firmware
```
with the `/shared` partition.


***

## Setup EKS Access Entry

To allow users to deploy training workloads on Hyperpod, you should setup EKS Access Entry for the SageMaker Studio IAM Role.

Run the following commands:

```bash
export EKS_CLUSTER_NAME=<YOUR_CLUSTER_NAME>
```

Replace **YOUR_CLUSTER_NAME** With the name of the EKS cluster.

```bash
CALLER_IDENTITY=$(aws sts get-caller-identity --output json)
ACCOUNT_ID=$(echo "$CALLER_IDENTITY" | jq -r .Account)
USER_ARN=$(echo "$CALLER_IDENTITY" | jq -r .Arn)
PRINCIPAL_TYPE=$(echo "$USER_ARN" | cut -d':' -f6 | cut -d'/' -f1)
USER_NAME=$(echo "$USER_ARN" | cut -d'/' -f2)
ROLE_NAME=$(echo "$USER_ARN" | cut -d'/' -f2)
USER_ARN="arn:aws:iam::${ACCOUNT_ID}:role/${ROLE_NAME}"
```

Create an EKS Access Entry:

```bash
aws eks create-access-entry \
  --cluster-name "$EKS_CLUSTER_NAME" \
  --principal-arn "$USER_ARN" \
  --type "STANDARD"
```

Associate an IAM policy to the access entry:

```bash
aws eks associate-access-policy \
  --cluster-name "$EKS_CLUSTER_NAME" \
  --principal-arn "$USER_ARN" \
  --policy-arn "arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy" \
  --access-scope '{"type": "cluster"}'
```

You are now ready to setup the cluster connection.

If you run 
```bash
kubectl get svc
```

you should see an output similar to:
```
NAME             TYPE        CLUSTER-IP   EXTERNAL-IP PORT(S)   AGE
svc/kubernetes   ClusterIP   10.100.0.1   <none>      443/TCP   1m
```

***

## Setup EKS Cluster connection

**Important:** Follow the steps in [Verifying cluster connection to EKS](./Verifying%20cluster%20connection%20to%20EKS.md) until "Verify helm Chart Installation".

You should now be able to operate with the EKS cluster.

![SageMaker Studio with Hyperpod integration](/img/14-studio-integration/06-get-pod.png)

***

## Optional: Setup Hyperpod Task Governance and CloudWatch Observability

1. For Hyperpod Task Governance, If not previously done, follow the steps in [Task Governance](/04-add-ons/Task%20Governance/Task%20Governance%20for%20Training.md)
2. For CloudWatch Observability insights, if not done previously, follow the steps in [Container Insights](/04-add-ons/Observability/Container%20Insights/Container%20Insights.md)

Run the following command:

```bash
export EKS_CLUSTER_NAME=<EKS_CLUSTER_NAME>
export EKS_CLUSTER_EXECUTION_ROLE_NAME=<EKS_CLUSTER_EXECUTION_ROLE_NAME>
```

Replace:
- **EKS_CLUSTER_NAME**: EKS Cluster name
- **EKS_CLUSTER_EXECUTION_ROLE_NAME**: IAM Role name used by the EKS Cluster

```bash
aws iam attach-role-policy \
  --role-name $EKS_CLUSTER_EXECUTION_ROLE_NAME \
  --policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy
```

Optional: You can install the CloudWatch adds-on through AWS SDK:

```bash
aws eks create-addon \
--addon-name amazon-cloudwatch-observability \
--cluster-name $EKS_CLUSTER_NAME
```

For additional information, please refer to [Upgrading to Container Insights with enhanced observability for Amazon EKS in CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-upgrade-enhanced.html)

***

## Optional: MLFlow Setup

Follow the AWS documentation: [Assign IAM roles to Kubernetes service accounts](https://docs.aws.amazon.com/eks/latest/userguide/associate-service-account-role.html)

**Note**: You can perform these steps from SageMaker Studio. The Studio Domain execution role is configured with the necessary IAM Policy. If you need to edit the role, ensure these steps are performed by an Admin user.

Run the following commands:

```bash
export EKS_CLUSTER_NAME=<EKS_CLUSTER_NAME>
export EKS_MLFLOW_POLICY_ARN=<EKS_MLFLOW_POLICY_ARN>
```

Replace:
- **EKS_CLUSTER_NAME**: EKS Cluster name
- **EKS_MLFLOW_POLICY_ARN**: MLflow policy ARN from the output of the Cloudformation template

![SageMaker Studio CFN Output](/img/14-studio-integration/11-cfn-output.png)

```bash
eksctl create iamserviceaccount \
  --name eks-hyperpod-sa \
  --namespace default \
  --cluster $EKS_CLUSTER_NAME \
  --role-name eks-hyperpod-mlflow-role \
  --attach-policy-arn $EKS_MLFLOW_POLICY_ARN \
  --approve
```

```bash
kubectl apply -f - <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: eks-hyperpod-sa
  namespace: default
EOF
```

</details>

<details>
<summary>Install the AWS CLI</summary>

:::note
The AWS CLI comes pre-installed on AWS CloudShell.
:::

<Tabs>
    <TabItem value="linux" label="Linux_x86_64">
        ```bash
        curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
        unzip awscliv2.zip
        sudo ./aws/install --update
        ```
    </TabItem>
    <TabItem value="linux-arm" label="Linux_ARM64">
        ```bash
        curl "https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip" -o "awscliv2.zip"
        unzip awscliv2.zip
        sudo ./aws/install
        ```
    </TabItem>
    <TabItem value="macos" label="MacOS">
        ```bash
        curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg"
        sudo installer -pkg AWSCLIV2.pkg -target /
        ```
    </TabItem>
</Tabs>

</details>

<details>
<summary>Configure AWS Credentials</summary>

Please refer to [this documentation](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-authentication.html) to understand the different ways to acquire AWS Credentials to use AWS CLI.

For simplicity and to demonstrate the process of configuring AWS credentials for the CLI, we are going to use long-term access keys for designated IAM Users.

:::caution Important
To maintain a proper security posture we recommend either using short-term credentials or setting up AWS IAM Identity Center (formerly AWS SSO) for short-term credentials.
:::

**1. Acquire AWS access long-term credentials**

Please visit [this documentation](https://docs.aws.amazon.com/cli/latest/userguide/cli-authentication-user.html#cli-authentication-user-get) to learn how to acquire these credentials from the AWS console.

**2. Configure AWS CLI**

Using the credentials you fetched above, use `aws configure` to add the credentials to your terminal. See [configure aws credentials](https://docs.aws.amazon.com/cli/latest/userguide/cli-authentication-user.html#cli-authentication-user-configure.title) for more details.

```bash
aws configure
AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE
AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
Default region name [None]: us-west-2
Default output format [None]: json
```

**3. Set AWS Region**

Next you can set the environment variable for `AWS_REGION` to ensure it points to the region where you intend to stand up your infrastructure. AWS CLI provides command line arguments as [documented here](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-options.html) to override certain variables.

```bash
export AWS_REGION=us-west-2
```

For more information on the aws configure cli command please refer to this [CLI reference documentation](https://docs.aws.amazon.com/cli/latest/reference/configure/) 

</details>

<details>
<summary>Install Kubectl (for EKS only)</summary>

You will use kubectl throughout the workshop to interact with the EKS cluster Kubernetes API server. The following commands correspond with **Linux installations**. See the [Kubernetes documentation](https://kubernetes.io/docs/tasks/tools/) for official steps on how to install kubectl. 

<Tabs>
  <TabItem value="linux-x86" label="Linux (x86_64)">
    ```bash
    curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
    echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
    # If valid, output is kubectl: OK
    sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
    ```
    > Note: If you do not have root access on the target system, you can still install kubectl to the `~/.local/bin` directory: 

    ```bash
    chmod +x kubectl
    mkdir -p ~/.local/bin
    mv ./kubectl ~/.local/bin/kubectl
    # and then append (or prepend) ~/.local/bin to $PATH
    ```

    Test to ensure the version you installed is up-to-date:

    ```bash
    kubectl version --client
    ```
  </TabItem>
  <TabItem value="linux-arm" label="Linux (arm64)">
    ```bash
    curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl"
    curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl.sha256"
    echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
    # If valid, output is kubectl: OK
    sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
    ```
    > Note: If you do not have root access on the target system, you can still install kubectl to the `~/.local/bin` directory: 

    ```bash
    chmod +x kubectl
    mkdir -p ~/.local/bin
    mv ./kubectl ~/.local/bin/kubectl
    # and then append (or prepend) ~/.local/bin to $PATH
    ```

    Test to ensure the version you installed is up-to-date:

    ```bash
    kubectl version --client
    ```
  </TabItem>
  <TabItem value="macos" label="macOS (arm64)">
    
    ```bash
    curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/arm64/kubectl"
    curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/arm64/kubectl.sha256"
    echo "$(cat kubectl.sha256)  kubectl" | shasum -a 256 --check
    # If valid, output is kubectl: OK
    
    # make the kubectl binary executable:
    chmod +x ./kubectl
    # move the kubectl binary to a file location on your system PATH
    sudo mv ./kubectl /usr/local/bin/kubectl
    sudo chown root: /usr/local/bin/kubectl
    ```
    Test to ensure the version you installed is up-to-date:

    ```bash
    kubectl version --client
    ```
    After installing and validating kubectl, delete the checksum file:
    ```bash
    rm kubectl.sha256
    ```
  </TabItem>
</Tabs>

</details>

<details>
<summary>Install Helm (for EKS only)</summary>

[Helm](https://helm.sh/) is a package manager for Kubernetes that will be used to install various dependencies using [Charts](https://helm.sh/docs/topics/charts/), which bundle together all the resources needed to deploy an application to a Kubernetes cluster.

```bash
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
```

</details>

<details>
<summary>Install eksctl (for EKS only)</summary>

You can use eksctl to [create an IAM OIDC provider](https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html) and install CSI drivers. The following commands correspond with Unix installations. See the [eksctl documentation](https://docs.aws.amazon.com/eks/latest/eksctl/installation.html) for alternative installation options.

```bash
# for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
ARCH=amd64
PLATFORM=$(uname -s)_$ARCH
curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
# (Optional) Verify checksum
curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check
tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
sudo mv /tmp/eksctl /usr/local/bin
```

</details>

<details>
<summary>Install Terraform</summary>

If you plan to [use Terraform](/docs/infrastructure-as-a-code/terraform/) to deploy the workshop infrastructure, see the [Terraform documentation](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli) for installation instructions. 

</details>
