---
apiVersion: v1
kind: Pod
metadata:
  name: lmi-inference
  labels:
    app: lmi-inference
spec:
  containers:
    - name: lmi-inference
      image: deepjavalibrary/djl-serving:0.28.0-lmi
      resources:
        limits:
          nvidia.com/gpu: 2
        requests:
          cpu: "8"
          memory: 64Gi
          nvidia.com/gpu: 2
          ephemeral-storage: 20Gi
      ports:
        - containerPort: 80
          name: http
      volumeMounts:
        - name: model
          mountPath: /opt/ml/model
        - name: shm
          mountPath: /dev/shm
      env:
        - name: HUGGING_FACE_HUB_TOKEN
          value: "ADD Key"
        - name: OPTION_MODEL_ID
          value: "TheBloke/Llama-2-7B-Chat-fp16"
  volumes:
    - name: model
      hostPath:
       path: /opt/dlami/nvme
       type: DirectoryOrCreate
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 12Gi
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  restartPolicy: Never
---
apiVersion: v1
kind: Service
metadata:
  name: lmi-inference
  annotations:
    # NLB specific annotations
    service.beta.kubernetes.io/aws-load-balancer-type: "external"
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "ip"
    service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
spec:
  ports:
    - port: 8080
      protocol: TCP
      targetPort: http
  selector:
    app: lmi-inference
  type: LoadBalancer