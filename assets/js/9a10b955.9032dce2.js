"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5307],{7356:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"validation-and-testing/performance-testing/gpu-stress-testing","title":"GPU Stress Testing","description":"GPU stress testing validates hardware stability, thermal management, and performance consistency by putting GPUs under sustained computational load. This guide focuses on \\"burning\\" GPUs to test their limits and detect potential hardware issues.","source":"@site/docs/06-validation-and-testing/performance-testing/gpu-stress-testing.md","sourceDirName":"06-validation-and-testing/performance-testing","slug":"/validation-and-testing/performance-testing/gpu-stress-testing","permalink":"/ai-on-sagemaker-hyperpod/docs/validation-and-testing/performance-testing/gpu-stress-testing","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"GPU Stress Testing","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"NCCL Performance Tests","permalink":"/ai-on-sagemaker-hyperpod/docs/validation-and-testing/performance-testing/nccl-tests"},"next":{"title":"NCCOM Tests (Trainium)","permalink":"/ai-on-sagemaker-hyperpod/docs/validation-and-testing/performance-testing/nccom-tests"}}');var t=s(4848),r=s(8453);const a={title:"GPU Stress Testing",sidebar_position:2},l="GPU Stress Testing and Validation",o={},d=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"NVIDIA GPU Testing",id:"nvidia-gpu-testing",level:3},{value:"NVIDIA GPU Stress Testing",id:"nvidia-gpu-stress-testing",level:2},{value:"1. DCGM Diagnostic Tests",id:"1-dcgm-diagnostic-tests",level:3},{value:"2. GPU Burn Tool",id:"2-gpu-burn-tool",level:3},{value:"Monitoring During Stress Tests",id:"monitoring-during-stress-tests",level:2},{value:"Real-time GPU Monitoring",id:"real-time-gpu-monitoring",level:3},{value:"DCGM Monitoring",id:"dcgm-monitoring",level:3},{value:"Stress Test Analysis",id:"stress-test-analysis",level:2},{value:"Temperature Analysis",id:"temperature-analysis",level:3},{value:"Power Analysis",id:"power-analysis",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Pre-Stress Checklist",id:"1-pre-stress-checklist",level:3},{value:"2. During Stress Testing",id:"2-during-stress-testing",level:3},{value:"3. Post-Stress Analysis",id:"3-post-stress-analysis",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"High Temperatures",id:"high-temperatures",level:3},{value:"Memory Errors",id:"memory-errors",level:3},{value:"Performance Inconsistencies",id:"performance-inconsistencies",level:3},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"gpu-stress-testing-and-validation",children:"GPU Stress Testing and Validation"})}),"\n",(0,t.jsx)(n.p,{children:'GPU stress testing validates hardware stability, thermal management, and performance consistency by putting GPUs under sustained computational load. This guide focuses on "burning" GPUs to test their limits and detect potential hardware issues.'}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"GPU stress testing provides:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hardware stability validation"})," under sustained load"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Thermal throttling detection"})," and mitigation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory error detection"})," and validation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Power delivery validation"})," under peak loads"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Performance consistency"})," verification across all GPUs"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.h3,{id:"nvidia-gpu-testing",children:"NVIDIA GPU Testing"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"NVIDIA GPUs (P4d, P5, P6e instances)"}),"\n",(0,t.jsx)(n.li,{children:"NVIDIA drivers and CUDA toolkit"}),"\n",(0,t.jsx)(n.li,{children:"DCGM (Data Center GPU Manager) - pre-installed on HyperPod"}),"\n",(0,t.jsx)(n.li,{children:"Administrative access for hardware monitoring"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"nvidia-gpu-stress-testing",children:"NVIDIA GPU Stress Testing"}),"\n",(0,t.jsx)(n.h3,{id:"1-dcgm-diagnostic-tests",children:"1. DCGM Diagnostic Tests"}),"\n",(0,t.jsx)(n.p,{children:"DCGM provides comprehensive GPU diagnostics and stress testing capabilities:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Initialize DCGM and discover GPUs\ndcgmi discovery -l\n\n# Create a group for testing (optional)\ndcgmi group -c stress_test_group\ndcgmi group -g 1 -a 0,1,2,3,4,5,6,7  # Add all GPUs to group\n\n# Run different levels of diagnostics\ndcgmi diag -g 1 -r 1  # Level 1 (quick)\ndcgmi diag -g 1 -r 2  # Level 2 (medium stress)  \ndcgmi diag -g 1 -r 3  # Level 3 (intensive)\n\n# Monitor GPU health\ndcgmi health -g 1 -c\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-gpu-burn-tool",children:"2. GPU Burn Tool"}),"\n",(0,t.jsxs)(n.p,{children:["GPU Burn is a simple, effective tool for maximizing GPU usage using the ",(0,t.jsx)(n.a,{href:"https://github.com/wilicc/gpu-burn",children:"gpu-burn repository"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Install GPU Burn\ngit clone https://github.com/wilicc/gpu-burn.git\ncd gpu-burn\nmake\n\n# Run stress test for different durations\n./gpu_burn 60    # 1 minute test\n./gpu_burn 300   # 5 minute test\n./gpu_burn 1800  # 30 minute test\n\n# Run on specific GPUs\nCUDA_VISIBLE_DEVICES=0,1 ./gpu_burn 300\n"})}),"\n",(0,t.jsx)(n.h2,{id:"monitoring-during-stress-tests",children:"Monitoring During Stress Tests"}),"\n",(0,t.jsx)(n.h3,{id:"real-time-gpu-monitoring",children:"Real-time GPU Monitoring"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Monitor GPU metrics continuously\nwatch -n 1 nvidia-smi\n\n# Monitor with detailed metrics and logging\nnvidia-smi --query-gpu=timestamp,name,temperature.gpu,power.draw,memory.used,memory.total,utilization.gpu \\\n           --format=csv -l 1 > gpu_stress_log.csv\n\n# Monitor for thermal throttling\nwatch -n 1 \'nvidia-smi --query-gpu=temperature.gpu,power.draw --format=csv,noheader,nounits | awk -F, "{if(\\$1>85) print \\"WARNING: GPU temp \\"\\$1\\"\xb0C\\"}"\'\n'})}),"\n",(0,t.jsx)(n.h3,{id:"dcgm-monitoring",children:"DCGM Monitoring"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Start DCGM daemon (if not running)\nsudo nv-hostengine\n\n# Monitor GPU health continuously\ndcgmi health -c\n\n# Monitor detailed metrics\ndcgmi dmon -e 1001,1002,1003,1004,1005,1006 -c 100\n"})}),"\n",(0,t.jsx)(n.h2,{id:"stress-test-analysis",children:"Stress Test Analysis"}),"\n",(0,t.jsx)(n.h3,{id:"temperature-analysis",children:"Temperature Analysis"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Extract temperature statistics from nvidia-smi CSV\nawk -F\',\' \'NR>1 {sum+=$3; if($3>max) max=$3; if(min=="" || $3<min) min=$3} END {print "Temp - Min:"min"\xb0C, Max:"max"\xb0C, Avg:"sum/(NR-1)"\xb0C"}\' gpu_stress_log.csv\n\n# Check for thermal throttling events\nawk -F\',\' \'NR>1 && $3>85 {print "WARNING: High temperature "$3"\xb0C at "$1}\' gpu_stress_log.csv\n'})}),"\n",(0,t.jsx)(n.h3,{id:"power-analysis",children:"Power Analysis"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Extract power statistics\nawk -F\',\' \'NR>1 {sum+=$4; if($4>max) max=$4; if(min=="" || $4<min) min=$4} END {print "Power - Min:"min"W, Max:"max"W, Avg:"sum/(NR-1)"W"}\' gpu_stress_log.csv\n'})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsx)(n.h3,{id:"1-pre-stress-checklist",children:"1. Pre-Stress Checklist"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 Verify cooling systems are operational"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Check power delivery capacity"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Ensure monitoring tools are configured"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Set up temperature alerts"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Plan for graceful shutdown procedures"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-during-stress-testing",children:"2. During Stress Testing"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Monitor temperatures continuously (keep below 85\xb0C)"}),"\n",(0,t.jsx)(n.li,{children:"Watch for power throttling"}),"\n",(0,t.jsx)(n.li,{children:"Check for memory errors"}),"\n",(0,t.jsx)(n.li,{children:"Validate performance consistency"}),"\n",(0,t.jsx)(n.li,{children:"Be ready to stop tests if temperatures spike"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-post-stress-analysis",children:"3. Post-Stress Analysis"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Review thermal profiles for hot spots"}),"\n",(0,t.jsx)(n.li,{children:"Check for hardware errors in logs"}),"\n",(0,t.jsx)(n.li,{children:"Validate all GPUs performed consistently"}),"\n",(0,t.jsx)(n.li,{children:"Document any issues found"}),"\n",(0,t.jsx)(n.li,{children:"Plan remediation for problematic hardware"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsx)(n.h3,{id:"high-temperatures",children:"High Temperatures"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check cooling system status\nsensors  # If available\nnvidia-smi --query-gpu=temperature.gpu --format=csv\n\n# Reduce stress test intensity\n./gpu_burn 60  # Shorter duration\n"})}),"\n",(0,t.jsx)(n.h3,{id:"memory-errors",children:"Memory Errors"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check for GPU memory errors\nnvidia-smi --query-gpu=memory.ecc.errors.corrected.total,memory.ecc.errors.uncorrected.total --format=csv\n\n# Run DCGM memory test\ndcgmi diag -r 2  # Includes memory tests\n"})}),"\n",(0,t.jsx)(n.h3,{id:"performance-inconsistencies",children:"Performance Inconsistencies"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check GPU clocks\nnvidia-smi --query-gpu=clocks.gr,clocks.mem --format=csv\n\n# Monitor for throttling\nnvidia-smi --query-gpu=clocks_throttle_reasons.active --format=csv\n"})}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"After successful GPU stress testing:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Document baseline thermal and performance characteristics"}),"\n",(0,t.jsx)(n.li,{children:"Set up continuous monitoring and alerting"}),"\n",(0,t.jsx)(n.li,{children:"Establish regular stress testing schedules"}),"\n",(0,t.jsx)(n.li,{children:"Create procedures for handling hardware failures"}),"\n",(0,t.jsx)(n.li,{children:"Proceed with confidence to production workloads"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>l});var i=s(6540);const t={},r=i.createContext(t);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);