"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5685],{999:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/trn1-71450bb2a8e0922f022c3299b207551f.png"},7152:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"slurm-blueprints/fine-tuning/peft/lora/lora","title":"LoRA - Tranium","description":"This example showcases how to train Llama 3 models using AWS Trainium instances and \ud83e\udd17 Optimum Neuron. \ud83e\udd17 Optimum Neuron is the interface between the \ud83e\udd17 Transformers library and AWS Accelerators including AWS Trainium and AWS Inferentia. It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks.","source":"@site/docs/02-slurm-blueprints/fine-tuning/peft/lora/lora.md","sourceDirName":"02-slurm-blueprints/fine-tuning/peft/lora","slug":"/slurm-blueprints/fine-tuning/peft/lora/","permalink":"/docs/slurm-blueprints/fine-tuning/peft/lora/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"LoRA - Tranium","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Fine Tuning","permalink":"/docs/category/fine-tuning-1"},"next":{"title":"QLoRA (Quantized LoRA) Fine-tuning","permalink":"/docs/slurm-blueprints/fine-tuning/peft/qlora/"}}');var s=t(4848),r=t(8453);const o={title:"LoRA - Tranium",sidebar_position:1},a="PEFT fine tuning of Llama 3 (trn1.32xlarge)",l={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Setup",id:"setup",level:2},{value:"Step 1: Download training scripts",id:"step-1-download-training-scripts",level:3},{value:"Step 2: Setup Python Environment",id:"step-2-setup-python-environment",level:3},{value:"Step 3: Download the model",id:"step-3-download-the-model",level:3},{value:"Training",id:"training",level:2},{value:"Step 1: Compile the model",id:"step-1-compile-the-model",level:3},{value:"Step 2: Fine Tuning",id:"step-2-fine-tuning",level:3},{value:"Step 3: Model Weight Consolidation",id:"step-3-model-weight-consolidation",level:3},{value:"Step 4: Merge LoRA Weights",id:"step-4-merge-lora-weights",level:3},{value:"Step 5: Validate your trained model",id:"step-5-validate-your-trained-model",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"peft-fine-tuning-of-llama-3-trn132xlarge",children:"PEFT fine tuning of Llama 3 (trn1.32xlarge)"})}),"\n",(0,s.jsx)(n.p,{children:"This example showcases how to train Llama 3 models using AWS Trainium instances and \ud83e\udd17 Optimum Neuron. \ud83e\udd17 Optimum Neuron is the interface between the \ud83e\udd17 Transformers library and AWS Accelerators including AWS Trainium and AWS Inferentia. It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks."}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.p,{children:["Before running this training, you'll need to create a SageMaker HyperPod cluster with at least 1 trn1.32xlarge / trn1n.32xlarge instance group. Instructions can be found in the ",(0,s.jsx)(n.a,{href:"/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup",children:"Cluster Setup"})," section."]}),"\n",(0,s.jsx)(n.p,{children:"You will also need to complete the following prerequisites for configuring and deploying your SageMaker HyperPod cluster for fine tuning:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Submit a service quota increase request to get access to Trainium instances in your AWS Region. You will need to request an increase for Amazon EC2 Trn1 instances, ml.trn1.32xlarge or ml.trn1n.32xlarge."}),"\n",(0,s.jsx)(n.li,{children:"Locally, install the AWS Command Line Interface (AWS CLI); the required minimum version needed is 2.14.3."}),"\n",(0,s.jsx)(n.li,{children:"Locally, Install the AWS Systems Manager Session Manager Plugin in order to SSH into your cluster."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Additionally, since Llama 3 is a gated model users have to register in Hugging Face and obtain an ",(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"access token"})," before running this example. You will also need to review and accept the license agreement on the ",(0,s.jsx)(n.a,{href:"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct",children:"meta-llama/Meta-llama-3-8B-Instruct"})," model page."]}),"\n",(0,s.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,s.jsxs)(n.p,{children:["In this section, we will setup our training environment on the cluster. Begin by logging into your cluster by following the ",(0,s.jsx)(n.a,{href:"/docs/getting-started/orchestrated-by-slurm/ssh-into-hyperpod",children:"SSH into Cluster"})," section."]}),"\n",(0,s.jsx)(n.h3,{id:"step-1-download-training-scripts",children:"Step 1: Download training scripts"}),"\n",(0,s.jsx)(n.p,{children:"Begin by downloading the training scripts from the aws-awesome-distributed repo:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/\ngit clone https://github.com/aws-samples/awsome-distributed-training\n\nmkdir ~/peft_ft \ncd ~/peft_ft\ncp -r ~/awsome-distributed-training/3.test_cases/pytorch/optimum-neuron/llama3/slurm/fine-tuning/submit_jobs .\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-setup-python-environment",children:"Step 2: Setup Python Environment"}),"\n",(0,s.jsx)(n.p,{children:"Setup a virtual python environment and install your training dependencies. Make sure this repo is stored on the shared FSx volume of your cluster so all nodes have access to it."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sbatch submit_jobs/0.create_env.sh\n"})}),"\n",(0,s.jsx)(n.p,{children:"View the logs created by the scripts in this lab by running this command below. You can update it for the step you are currently running:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"tail -f logs/0.create_env.out \n"})}),"\n",(0,s.jsx)(n.p,{children:"Before proceeding to the next step throughout this lab, check if the current job has finished by running:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"squeue\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-download-the-model",children:"Step 3: Download the model"}),"\n",(0,s.jsx)(n.p,{children:"Next, you will download the model to your FSx file volume. Begin by logging into Hugging Face using your access token mentioned in the prerequisite steps. With your access token set, you should now be able to download the model."}),"\n",(0,s.jsxs)(n.p,{children:["First modify the ",(0,s.jsx)(n.code,{children:"submit_jobs/1.download_model.sh"})," script to include the Hugging Face access token before running it:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'export HF_TOKEN="<Your Hugging Face Token>"\n'})}),"\n",(0,s.jsx)(n.p,{children:"Then trigger the script to download the Llama3 model."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sbatch submit_jobs/1.download_model.sh\n"})}),"\n",(0,s.jsx)(n.p,{children:"Now that your SageMaker HyperPod cluster is deployed and your environment is setup up, you can start preparing to execute your fine tuning job."}),"\n",(0,s.jsx)(n.h2,{id:"training",children:"Training"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Tranium",src:t(999).A+"",width:"768",height:"386"})}),"\n",(0,s.jsxs)(n.p,{children:["In this section, you will begin training your Llama 3 model on a Trainium ",(0,s.jsx)(n.code,{children:"trn1.32xlarge"})," instance."]}),"\n",(0,s.jsx)(n.h3,{id:"step-1-compile-the-model",children:"Step 1: Compile the model"}),"\n",(0,s.jsxs)(n.p,{children:["Before you begin training on Trainium with Neuron, you will need to pre-compile your model with the ",(0,s.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/training/pytorch-neuron-parallel-compile.html",children:"neuron_parallel_compile CLI"})," which reduces the compilatin time during execution. This will trace through the model\u2019s training code and apply optimizations to improve performance."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sbatch submit_jobs/2.compile_model.sh\n"})}),"\n",(0,s.jsx)(n.p,{children:"The compilation process will generate NEFF (Neuron Executable File Format) files that will speed up your model\u2019s fine tuning job."}),"\n",(0,s.jsx)(n.h3,{id:"step-2-fine-tuning",children:"Step 2: Fine Tuning"}),"\n",(0,s.jsx)(n.p,{children:"With your model compiled, you can now begin fine tuning your Llama 3 model."}),"\n",(0,s.jsxs)(n.p,{children:["For the purposes of this workshop, we will use the ",(0,s.jsx)(n.a,{href:"https://huggingface.co/datasets/databricks/databricks-dolly-15k",children:"dolly 15k dataset"}),". As part of the training process, the script below will download the dataset and format it in a way that the model expects. Each data point will contain an ",(0,s.jsx)(n.strong,{children:"instruction"})," that guides the model\u2019s task, optional ",(0,s.jsx)(n.strong,{children:"context"})," that provides background information, and ",(0,s.jsx)(n.strong,{children:"response"})," that represent the desired output."]}),"\n",(0,s.jsx)(n.p,{children:"Now submit the fine tuning job:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sbatch submit_jobs/3.finetune.sh\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-model-weight-consolidation",children:"Step 3: Model Weight Consolidation"}),"\n",(0,s.jsxs)(n.p,{children:["After training has completed, you will have a new directory for your model checkpoints. This directory will contain the model checkpoint shards from each neuron device that were generated during training. Use the model consolidation script to combine the shards into a single ",(0,s.jsx)(n.code,{children:"model.safetensors"})," file."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sbatch submit_jobs/4.model_consolidation.sh\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"model.safetensors"})," file will contain the LoRA weights of your model that were updated during training."]}),"\n",(0,s.jsx)(n.h3,{id:"step-4-merge-lora-weights",children:"Step 4: Merge LoRA Weights"}),"\n",(0,s.jsx)(n.p,{children:"After consolidating the model shards, merge the LoRA adapter weights back to your base Llama 3 model:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sbatch submit_jobs/5.merge_lora_weights.sh\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Your final fine tuned model weights will be saved to the final_model_path directory. You can find or update the path in the script ",(0,s.jsx)(n.code,{children:"submit_jobs/5.merge_lora_weights.sh"})," using the argument ",(0,s.jsx)(n.code,{children:"--final_model_path"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"step-5-validate-your-trained-model",children:"Step 5: Validate your trained model"}),"\n",(0,s.jsx)(n.p,{children:"Now that your model is fine tuned, see how its generations differ from the base model for the dolly-15k dataset."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sbatch submit_jobs/6.inference.sh\n"})}),"\n",(0,s.jsx)(n.p,{children:"This will generate a prediction for the question \u201cWho are you?\u201d, comparing the response of the base model to the fine tuned model. It will also pass a system prompt to the model to always respond like a pirate."}),"\n",(0,s.jsx)(n.p,{children:"Before fine tuning:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"{\n    'role': 'assistant', \n    'content': \"Arrrr, me hearty! Me name be Captain Chat, the scurviest pirate chatbot to \n    ever sail the Seven Seas! Me be here to regale ye with tales o' adventure, answer yer \n    questions, and swab the decks o' yer doubts! So hoist the colors, me matey, and let's \n    set sail fer a swashbucklin' good time!\"\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:"After fine tuning:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"{\n    'role': 'assistant', \n    'content': \"Arrr, shiver me timbers! Me be Captain Chat, the scurviest pirate chatbot to ever sail the Seven Seas! Me been programmin' me brain \n    with the finest pirate lingo and booty-ful banter to make ye feel like ye just stumbled\n    upon a chest overflowin' with golden doubloons! So hoist the colors, me hearty, and \n    let's set sail fer a swashbucklin' good time!\"\n } \n"})}),"\n",(0,s.jsx)(n.p,{children:"And that's it! You've successfully fine tuned a Llama 3 model on Amazon SageMaker HyperPod using PEFT with Neuron."}),"\n",(0,s.jsx)(n.hr,{})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var i=t(6540);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);