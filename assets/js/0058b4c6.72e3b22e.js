"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[849],{6164:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Introduction","href":"/docs/Introduction","docId":"Introduction","unlisted":false},{"type":"category","label":"Getting Started","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Orchestrated by EKS","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"New cluster creation experience","href":"/docs/getting-started/orchestrated-by-eks/initial-cluster-setup","docId":"getting-started/orchestrated-by-eks/initial-cluster-setup","unlisted":false},{"type":"link","label":"Reviewing the cluster console","href":"/docs/getting-started/orchestrated-by-eks/Reviewing the cluster console","docId":"getting-started/orchestrated-by-eks/Reviewing the cluster console","unlisted":false},{"type":"link","label":"SageMaker Studio Integration","href":"/docs/getting-started/orchestrated-by-eks/sagemaker-studio-integration","docId":"getting-started/orchestrated-by-eks/sagemaker-studio-integration","unlisted":false},{"type":"link","label":"Verifying cluster connection to EKS","href":"/docs/getting-started/orchestrated-by-eks/Verifying cluster connection to EKS","docId":"getting-started/orchestrated-by-eks/Verifying cluster connection to EKS","unlisted":false},{"type":"link","label":"Additional Information","href":"/docs/getting-started/orchestrated-by-eks/additional-information","docId":"getting-started/orchestrated-by-eks/additional-information","unlisted":false},{"type":"link","label":"Set up your shared file system","href":"/docs/getting-started/orchestrated-by-eks/Set up your shared file system","docId":"getting-started/orchestrated-by-eks/Set up your shared file system","unlisted":false},{"type":"link","label":"Adding a Data Repository Association","href":"/docs/getting-started/orchestrated-by-eks/Adding a Data Repository Assocation","docId":"getting-started/orchestrated-by-eks/Adding a Data Repository Assocation","unlisted":false},{"type":"link","label":"Set up an Amazon S3 mountpoint","href":"/docs/getting-started/orchestrated-by-eks/Set up an Amazon S3 mountpoint","docId":"getting-started/orchestrated-by-eks/Set up an Amazon S3 mountpoint","unlisted":false}],"href":"/docs/category/orchestrated-by-eks"},{"type":"category","label":"Orchestrated by SLURM","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"New cluster creation experience","href":"/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup","docId":"getting-started/orchestrated-by-slurm/initial-cluster-setup","unlisted":false},{"type":"link","label":"View the AWS Console","href":"/docs/getting-started/orchestrated-by-slurm/View the AWS Console","docId":"getting-started/orchestrated-by-slurm/View the AWS Console","unlisted":false},{"type":"link","label":"SageMaker Studio Integration","href":"/docs/getting-started/orchestrated-by-slurm/sagemaker-studio-integration","docId":"getting-started/orchestrated-by-slurm/sagemaker-studio-integration","unlisted":false},{"type":"link","label":"SSH Into Your HyperPod Cluster","href":"/docs/getting-started/orchestrated-by-slurm/ssh-into-hyperpod","docId":"getting-started/orchestrated-by-slurm/ssh-into-hyperpod","unlisted":false},{"type":"link","label":"Basic Slurm Commands","href":"/docs/getting-started/orchestrated-by-slurm/slurm-basics","docId":"getting-started/orchestrated-by-slurm/slurm-basics","unlisted":false}],"href":"/docs/category/orchestrated-by-slurm"},{"type":"link","label":"Installing the required tools","href":"/docs/getting-started/install-pre-requisites","docId":"getting-started/install-pre-requisites","unlisted":false}],"href":"/docs/category/getting-started"},{"type":"category","label":"EKS Blueprints","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Training","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"DDP","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Distributed Data Parallel (DDP)","href":"/docs/eks-blueprints/training/ddp/distributed-data-parallel","docId":"eks-blueprints/training/ddp/distributed-data-parallel","unlisted":false}]},{"type":"category","label":"FSDP","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Fully Sharded Data Parallelism (FSDP)","href":"/docs/eks-blueprints/training/fsdp/fully-sharded-data-parallel","docId":"eks-blueprints/training/fsdp/fully-sharded-data-parallel","unlisted":false}]},{"type":"category","label":"Megatron-LM","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"NVIDIA Megatron-LM","href":"/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme","docId":"eks-blueprints/training/megatron-lm/megatron-lm-readme","unlisted":false}]},{"type":"category","label":"Ray Train","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Ray Train","href":"/docs/eks-blueprints/training/ray-train/ray-train-readme","docId":"eks-blueprints/training/ray-train/ray-train-readme","unlisted":false}]},{"type":"category","label":"Trainium","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"AWS Trainium","href":"/docs/eks-blueprints/training/trainium/aws-trainium","docId":"eks-blueprints/training/trainium/aws-trainium","unlisted":false}]}],"href":"/docs/category/training"},{"type":"category","label":"Fine Tuning","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"PEFT","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"LoRA - Trainium","href":"/docs/eks-blueprints/fine-tuning/peft/low-rank-adaptation","docId":"eks-blueprints/fine-tuning/peft/low-rank-adaptation","unlisted":false},{"type":"link","label":"QLoRA (Quantized LoRA)","href":"/docs/eks-blueprints/fine-tuning/peft/quantisized-lora","docId":"eks-blueprints/fine-tuning/peft/quantisized-lora","unlisted":false}]},{"type":"category","label":"Preference Alignment","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"DPO","href":"/docs/eks-blueprints/fine-tuning/preference-alignment/dpo","docId":"eks-blueprints/fine-tuning/preference-alignment/dpo","unlisted":false},{"type":"link","label":"PPO","href":"/docs/eks-blueprints/fine-tuning/preference-alignment/ppo","docId":"eks-blueprints/fine-tuning/preference-alignment/ppo","unlisted":false},{"type":"link","label":"GRPO","href":"/docs/eks-blueprints/fine-tuning/preference-alignment/grpo","docId":"eks-blueprints/fine-tuning/preference-alignment/grpo","unlisted":false}]}],"href":"/docs/category/fine-tuning"},{"type":"category","label":"Inference","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Inference Operator","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"SageMaker JumpStart","href":"/docs/eks-blueprints/inference/inference-operator/sagemaker-jumpstart","docId":"eks-blueprints/inference/inference-operator/sagemaker-jumpstart","unlisted":false},{"type":"link","label":"Deploying model from S3 or FSX","href":"/docs/eks-blueprints/inference/inference-operator/amazon-s3-and-amazon-fsx","docId":"eks-blueprints/inference/inference-operator/amazon-s3-and-amazon-fsx","unlisted":false}]},{"type":"category","label":"Load Balancer Inference","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Mistral 7B with Load Balancer","href":"/docs/eks-blueprints/inference/load-balancer-inference/inference-with-loadbalancer","docId":"eks-blueprints/inference/load-balancer-inference/inference-with-loadbalancer","unlisted":false}]},{"type":"category","label":"Ray Service","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Serving Stable Diffusion Model for Inference with Ray Serve","href":"/docs/eks-blueprints/inference/ray-service/ray-service-readme","docId":"eks-blueprints/inference/ray-service/ray-service-readme","unlisted":false}]}],"href":"/docs/category/inference"}],"href":"/docs/category/eks-blueprints"},{"type":"category","label":"SLURM Blueprints","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Training","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"DDP","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"PyTorch DDP on CPU","href":"/docs/slurm-blueprints/training/ddp/distributed-data-parallel","docId":"slurm-blueprints/training/ddp/distributed-data-parallel","unlisted":false}]},{"type":"category","label":"FSDP","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Fully Sharded Data Parallel","href":"/docs/slurm-blueprints/training/fsdp/fully-sharded-data-parallel","docId":"slurm-blueprints/training/fsdp/fully-sharded-data-parallel","unlisted":false}]},{"type":"category","label":"Megatron-LM","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"NVIDIA Megatron-LM","href":"/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme","docId":"slurm-blueprints/training/megatron-lm/megatron-lm-readme","unlisted":false}]},{"type":"category","label":"Trainium","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Llama-3 70B (trn1.32xlarge) using NxD","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Setting up the software stack","href":"/docs/slurm-blueprints/training/trainium/Llama3-70B/prep","docId":"slurm-blueprints/training/trainium/Llama3-70B/prep","unlisted":false},{"type":"link","label":"Downloading the Llama3-70b model","href":"/docs/slurm-blueprints/training/trainium/Llama3-70B/download-model","docId":"slurm-blueprints/training/trainium/Llama3-70B/download-model","unlisted":false},{"type":"link","label":"Downloading the Wiki-corpus datasets","href":"/docs/slurm-blueprints/training/trainium/Llama3-70B/download-dataset","docId":"slurm-blueprints/training/trainium/Llama3-70B/download-dataset","unlisted":false},{"type":"link","label":"Running Continual Pre-training with NeuronX Distributed","href":"/docs/slurm-blueprints/training/trainium/Llama3-70B/training","docId":"slurm-blueprints/training/trainium/Llama3-70B/training","unlisted":false}],"href":"/docs/slurm-blueprints/training/trainium/Llama3-70B/"}]}],"href":"/docs/category/training-1"},{"type":"category","label":"Fine Tuning","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"PEFT","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"LoRA","href":"/docs/slurm-blueprints/fine-tuning/peft/lora/","docId":"slurm-blueprints/fine-tuning/peft/lora/lora","unlisted":false},{"type":"link","label":"QLoRA","href":"/docs/slurm-blueprints/fine-tuning/peft/qlora/","docId":"slurm-blueprints/fine-tuning/peft/qlora/qlora","unlisted":false}]},{"type":"category","label":"Preference Alignment","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"DPO","href":"/docs/slurm-blueprints/fine-tuning/preference-aligment/dpo","docId":"slurm-blueprints/fine-tuning/preference-aligment/dpo","unlisted":false},{"type":"link","label":"PPO","href":"/docs/slurm-blueprints/fine-tuning/preference-aligment/ppo","docId":"slurm-blueprints/fine-tuning/preference-aligment/ppo","unlisted":false},{"type":"link","label":"GRPO","href":"/docs/slurm-blueprints/fine-tuning/preference-aligment/grpo","docId":"slurm-blueprints/fine-tuning/preference-aligment/grpo","unlisted":false}]}],"href":"/docs/category/fine-tuning-1"}],"href":"/docs/category/slurm-blueprints"},{"type":"category","label":"Sagemaker Hyperpod Recipes","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"SageMaker HyperPod Recipes Overview","href":"/docs/hyperpod-recipes/index.en","docId":"hyperpod-recipes/index.en","unlisted":false},{"type":"category","label":"EKS HyperPod Recipes","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Setup and Launch training - EKS","href":"/docs/hyperpod-recipes/eks-hyperpod-recipes/hyperpod-recipes","docId":"hyperpod-recipes/eks-hyperpod-recipes/hyperpod-recipes","unlisted":false}]},{"type":"category","label":"Slurm HyperPod Recipes","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Setup and Launch training - Slurm","href":"/docs/hyperpod-recipes/slurm-hyperpod-recipes/setup-and-train","docId":"hyperpod-recipes/slurm-hyperpod-recipes/setup-and-train","unlisted":false}]}],"href":"/docs/category/sagemaker-hyperpod-recipes"},{"type":"category","label":"Add-Ons","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Observability","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"One-Click Observability with AMP and AMG (EKS only)","href":"/docs/add-ons/Observability/one-click-observability-eks/","docId":"add-ons/Observability/one-click-observability-eks/one-click-observability-eks","unlisted":false},{"type":"link","label":"Observability with AMP and AMG (Slurm only)","href":"/docs/add-ons/Observability/observability-slurm/","docId":"add-ons/Observability/observability-slurm/observability-slurm","unlisted":false},{"type":"link","label":"Amazon CloudWatch Container Insights (EKS only)","href":"/docs/add-ons/Observability/Container Insights/","docId":"add-ons/Observability/Container Insights/Container Insights","unlisted":false},{"type":"link","label":"SageMaker Managed MLflow","href":"/docs/add-ons/Observability/MLFlow/","docId":"add-ons/Observability/MLFlow/MLFlow","unlisted":false},{"type":"link","label":"Weights & Biases","href":"/docs/add-ons/Observability/Weights and Biases/","docId":"add-ons/Observability/Weights and Biases/Weights and Biases","unlisted":false}],"href":"/docs/add-ons/Observability/"},{"type":"category","label":"Task Governance","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Task Governance","href":"/docs/add-ons/Task Governance/Task Governance for Training","docId":"add-ons/Task Governance/Task Governance for Training","unlisted":false}],"href":"/docs/category/task-governance"},{"type":"category","label":"HyperPod Training Operator","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"HyperPod Training Operator Overview","href":"/docs/add-ons/hp-training-operator/overview","docId":"add-ons/hp-training-operator/overview","unlisted":false},{"type":"link","label":"Installation and Usage Guide","href":"/docs/add-ons/hp-training-operator/installation-and-usage","docId":"add-ons/hp-training-operator/installation-and-usage","unlisted":false}]},{"type":"link","label":"Installing the Hyperpod CLI","href":"/docs/add-ons/installing-the-hyperpod-cli","docId":"add-ons/installing-the-hyperpod-cli","unlisted":false},{"type":"category","label":"Integrations","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Ray","href":"/docs/add-ons/integrations/Ray/","docId":"add-ons/integrations/Ray/Ray","unlisted":false},{"type":"link","label":"SkyPilot","href":"/docs/add-ons/integrations/skypilot/","docId":"add-ons/integrations/skypilot/SkyPilot","unlisted":false}],"href":"/docs/category/integrations"},{"type":"category","label":"Utilities","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"FinOps","href":"/docs/add-ons/Utilities/FinOps/","docId":"add-ons/Utilities/FinOps/FinOps","unlisted":false},{"type":"link","label":"Log Analysis","href":"/docs/add-ons/Utilities/Log Analysis/","docId":"add-ons/Utilities/Log Analysis/Log Analysis","unlisted":false},{"type":"link","label":"Resource Monitoring","href":"/docs/add-ons/Utilities/Resource Monitoring/","docId":"add-ons/Utilities/Resource Monitoring/Resource Monitoring","unlisted":false}]}],"href":"/docs/category/add-ons"},{"type":"category","label":"Validation and Testing","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Environment Validation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"PyTorch Environment Validation","href":"/docs/validation-and-testing/environment-validation/pytorch-environment-validation","docId":"validation-and-testing/environment-validation/pytorch-environment-validation","unlisted":false},{"type":"link","label":"EFA and Network Stack Validation","href":"/docs/validation-and-testing/environment-validation/efa-validation","docId":"validation-and-testing/environment-validation/efa-validation","unlisted":false}],"href":"/docs/category/environment-validation"},{"type":"category","label":"NCCL & CUDA Validation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Troubleshoot NCCL and CUDA","href":"/docs/validation-and-testing/nccl-cuda-validation/Troubleshoot NCCL and CUDA","docId":"validation-and-testing/nccl-cuda-validation/Troubleshoot NCCL and CUDA","unlisted":false}],"href":"/docs/category/nccl--cuda-validation"},{"type":"category","label":"Performance Testing","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"NCCL Performance Tests","href":"/docs/validation-and-testing/performance-testing/nccl-tests","docId":"validation-and-testing/performance-testing/nccl-tests","unlisted":false},{"type":"link","label":"GPU Stress Testing","href":"/docs/validation-and-testing/performance-testing/gpu-stress-testing","docId":"validation-and-testing/performance-testing/gpu-stress-testing","unlisted":false},{"type":"link","label":"NCCOM Tests (Trainium)","href":"/docs/validation-and-testing/performance-testing/nccom-tests","docId":"validation-and-testing/performance-testing/nccom-tests","unlisted":false}],"href":"/docs/category/performance-testing"},{"type":"category","label":"HyperPod Resiliency","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Resiliency Overview","href":"/docs/validation-and-testing/resiliency/overview","docId":"validation-and-testing/resiliency/overview","unlisted":false},{"type":"link","label":"Testing Resiliency with HyperPod EKS","href":"/docs/validation-and-testing/resiliency/eks-resiliency","docId":"validation-and-testing/resiliency/eks-resiliency","unlisted":false},{"type":"link","label":"Testing Resiliency with HyperPod Slurm","href":"/docs/validation-and-testing/resiliency/slurm-resiliency","docId":"validation-and-testing/resiliency/slurm-resiliency","unlisted":false}],"href":"/docs/category/hyperpod-resiliency"}],"href":"/docs/category/validation-and-testing"},{"type":"category","label":"Infrastructure as a Code","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Terraform Deployment","href":"/docs/infrastructure-as-a-code/terraform/","docId":"infrastructure-as-a-code/terraform/terraform","unlisted":false},{"type":"link","label":"Amazon CloudFormation","href":"/docs/infrastructure-as-a-code/amazon-cloudFormation/","docId":"infrastructure-as-a-code/amazon-cloudFormation/amazon-cloudformation","unlisted":false}],"href":"/docs/category/infrastructure-as-a-code"},{"type":"category","label":"Tips","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Common","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Link FSx Filesystems to Amazon S3 (Data Repository Association)","href":"/docs/Tips/Common/link-fsx-to-S3","docId":"Tips/Common/link-fsx-to-S3","unlisted":false},{"type":"link","label":"Mount additional FSx Filesystem","href":"/docs/Tips/Common/mount-additional-fsx","docId":"Tips/Common/mount-additional-fsx","unlisted":false},{"type":"link","label":"Downsize existing FSx Volume","href":"/docs/Tips/Common/downsize-fsx","docId":"Tips/Common/downsize-fsx","unlisted":false},{"type":"link","label":"FSx for Lustre Best practices","href":"/docs/Tips/Common/Fsx for Lustre best practices","docId":"Tips/Common/Fsx for Lustre best practices","unlisted":false},{"type":"link","label":"FSx for Lustre performance","href":"/docs/Tips/Common/Fsx for Lustre performance","docId":"Tips/Common/Fsx for Lustre performance","unlisted":false},{"type":"link","label":"FSx for Lustre compression","href":"/docs/Tips/Common/Fsx for Lustre compression","docId":"Tips/Common/Fsx for Lustre compression","unlisted":false},{"type":"link","label":"Upload files to Amazon s3","href":"/docs/Tips/Common/move-files-s3","docId":"Tips/Common/move-files-s3","unlisted":false}]},{"type":"category","label":"EKS","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"SSM Login","href":"/docs/Tips/EKS/ssm-login","docId":"Tips/EKS/ssm-login","unlisted":false},{"type":"link","label":"Add users to your EKS HyperPod cluster","href":"/docs/Tips/EKS/add-users","docId":"Tips/EKS/add-users","unlisted":false},{"type":"link","label":"Receive cluster status/health events","href":"/docs/Tips/EKS/event-bridge","docId":"Tips/EKS/event-bridge","unlisted":false}]},{"type":"category","label":"Slurm","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Containers","href":"/docs/Tips/Slurm/containers","docId":"Tips/Slurm/containers","unlisted":false},{"type":"link","label":"Troubleshooting","href":"/docs/Tips/Slurm/troubleshooting","docId":"Tips/Slurm/troubleshooting","unlisted":false},{"type":"link","label":"Bastion Host","href":"/docs/Tips/Slurm/bastion-host","docId":"Tips/Slurm/bastion-host","unlisted":false},{"type":"link","label":"Login Node","href":"/docs/Tips/Slurm/login-node","docId":"Tips/Slurm/login-node","unlisted":false},{"type":"link","label":"Gres (--gpus)","href":"/docs/Tips/Slurm/gres","docId":"Tips/Slurm/gres","unlisted":false},{"type":"link","label":"Diagnose GPU Failures","href":"/docs/Tips/Slurm/diagnose-gpus","docId":"Tips/Slurm/diagnose-gpus","unlisted":false},{"type":"link","label":"Heterogenous Cluster","href":"/docs/Tips/Slurm/heterogenous-cluster","docId":"Tips/Slurm/heterogenous-cluster","unlisted":false},{"type":"link","label":"Configure Cgroups for Slurm","href":"/docs/Tips/Slurm/enable-cgroups","docId":"Tips/Slurm/enable-cgroups","unlisted":false},{"type":"link","label":"Enable Slurm epilog Script","href":"/docs/Tips/Slurm/slurm-epilogue","docId":"Tips/Slurm/slurm-epilogue","unlisted":false},{"type":"link","label":"Delete Cluster Nodes","href":"/docs/Tips/Slurm/delete-cluster-nodes","docId":"Tips/Slurm/delete-cluster-nodes","unlisted":false},{"type":"link","label":"Troubleshoot IAM Permissions","href":"/docs/Tips/Slurm/troubeshoot-permissions","docId":"Tips/Slurm/troubeshoot-permissions","unlisted":false}]}],"href":"/docs/category/tips"}]},"docs":{"add-ons/hp-training-operator/installation-and-usage":{"id":"add-ons/hp-training-operator/installation-and-usage","title":"Installation and Usage Guide","description":"This guide covers the installation of the HyperPod training operator and provides examples for running distributed training jobs using examples from the awsome-distributed-training repository.","sidebar":"tutorialSidebar"},"add-ons/hp-training-operator/overview":{"id":"add-ons/hp-training-operator/overview","title":"HyperPod Training Operator Overview","description":"The Amazon SageMaker HyperPod training operator helps you accelerate generative AI model development by efficiently managing distributed training across large GPU clusters. It introduces intelligent fault recovery, hang job detection, and process-level management capabilities that minimize training disruptions and reduce costs.","sidebar":"tutorialSidebar"},"add-ons/installing-the-hyperpod-cli":{"id":"add-ons/installing-the-hyperpod-cli","title":"Installing the Hyperpod CLI","description":"This tool is currently under maintenance. Some features may not work as expected. Please check the official repository for the latest updates.","sidebar":"tutorialSidebar"},"add-ons/integrations/Ray/Ray":{"id":"add-ons/integrations/Ray/Ray","title":"Ray","description":"","sidebar":"tutorialSidebar"},"add-ons/integrations/skypilot/SkyPilot":{"id":"add-ons/integrations/skypilot/SkyPilot","title":"SkyPilot","description":"Setup SkyPilot","sidebar":"tutorialSidebar"},"add-ons/Observability/Container Insights/Container Insights":{"id":"add-ons/Observability/Container Insights/Container Insights","title":"Amazon CloudWatch Container Insights (EKS only)","description":"CloudWatch Container Insights can be used to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices.  Container Insights is available for Amazon Elastic Kubernetes Service (Amazon EKS) and helps collect metrics from cluster deployed on EKS.","sidebar":"tutorialSidebar"},"add-ons/Observability/MLFlow/MLFlow":{"id":"add-ons/Observability/MLFlow/MLFlow","title":"SageMaker Managed MLflow","description":"Amazon SageMaker offers a managed MLflow capability for machine learning (ML) and generative AI experimentation. This capability makes it easy for data scientists to use MLflow on SageMaker for model training, registration, and deployment. Admins can quickly set up secure and scalable MLflow environments on AWS. Data scientists and ML developers can efficiently track ML experiments and find the right model for a business problem.","sidebar":"tutorialSidebar"},"add-ons/Observability/Observability":{"id":"add-ons/Observability/Observability","title":"Observability","description":"Observability is a foundational element of a well-architected EKS/Slurm environment. AWS provides native (CloudWatch) and open source managed (Amazon Managed Service for Prometheus (AMP), Amazon Managed Grafana (AMG) and AWS Distro for OpenTelemetry) solutions for monitoring, logging and alarming of EKS environments.","sidebar":"tutorialSidebar"},"add-ons/Observability/observability-slurm/observability-slurm":{"id":"add-ons/Observability/observability-slurm/observability-slurm","title":"Observability with AMP and AMG (Slurm only)","description":"Overview","sidebar":"tutorialSidebar"},"add-ons/Observability/one-click-observability-eks/one-click-observability-eks":{"id":"add-ons/Observability/one-click-observability-eks/one-click-observability-eks","title":"One-Click Observability with AMP and AMG (EKS only)","description":"Amazon SageMaker HyperPod (SageMaker HyperPod) provides a comprehensive, out-of-the-box dashboard that gives you insights into foundation model (FM) development tasks and cluster resources.","sidebar":"tutorialSidebar"},"add-ons/Observability/Weights and Biases/Weights and Biases":{"id":"add-ons/Observability/Weights and Biases/Weights and Biases","title":"Weights & Biases","description":"This section is under development. W&B for observability documentation will be available soon.","sidebar":"tutorialSidebar"},"add-ons/Task Governance/Task Governance for Training":{"id":"add-ons/Task Governance/Task Governance for Training","title":"Task Governance","description":"SageMaker HyperPod task governance is a management system designed to streamline resource allocation and ensure efficient utilization of compute resources across teams and projects for your Amazon EKS clusters. It provides administrators with the capability to set priority levels for various tasks, allocate compute resources for each team, determine how idle compute is borrowed and lent between teams, and configure whether a team can preempt its own tasks.","sidebar":"tutorialSidebar"},"add-ons/Utilities/FinOps/FinOps":{"id":"add-ons/Utilities/FinOps/FinOps","title":"FinOps","description":"","sidebar":"tutorialSidebar"},"add-ons/Utilities/Log Analysis/Log Analysis":{"id":"add-ons/Utilities/Log Analysis/Log Analysis","title":"Log Analysis","description":"","sidebar":"tutorialSidebar"},"add-ons/Utilities/Resource Monitoring/Resource Monitoring":{"id":"add-ons/Utilities/Resource Monitoring/Resource Monitoring","title":"Resource Monitoring","description":"","sidebar":"tutorialSidebar"},"eks-blueprints/fine-tuning/peft/low-rank-adaptation":{"id":"eks-blueprints/fine-tuning/peft/low-rank-adaptation","title":"LoRA - Trainium","description":"This example showcases how to train llama 3.1 models using AWS Trainium instances and Huggingface Optimum Neuron. \ud83e\udd17 Optimum Neuron is the interface between the \ud83e\udd17 Transformers library and AWS Accelerators including AWS Trainium and AWS Inferentia. It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks.","sidebar":"tutorialSidebar"},"eks-blueprints/fine-tuning/peft/quantisized-lora":{"id":"eks-blueprints/fine-tuning/peft/quantisized-lora","title":"QLoRA (Quantized LoRA)","description":"This section is under development. QLoRA fine-tuning documentation will be available soon.","sidebar":"tutorialSidebar"},"eks-blueprints/fine-tuning/preference-alignment/dpo":{"id":"eks-blueprints/fine-tuning/preference-alignment/dpo","title":"dpo","description":"This section is under development. DPO fine-tuning documentation will be available soon.","sidebar":"tutorialSidebar"},"eks-blueprints/fine-tuning/preference-alignment/grpo":{"id":"eks-blueprints/fine-tuning/preference-alignment/grpo","title":"grpo","description":"This section is under development. GRPO fine-tuning documentation will be available soon.","sidebar":"tutorialSidebar"},"eks-blueprints/fine-tuning/preference-alignment/ppo":{"id":"eks-blueprints/fine-tuning/preference-alignment/ppo","title":"ppo","description":"This section is under development. PPO fine-tuning documentation will be available soon.","sidebar":"tutorialSidebar"},"eks-blueprints/inference/inference-operator/amazon-s3-and-amazon-fsx":{"id":"eks-blueprints/inference/inference-operator/amazon-s3-and-amazon-fsx","title":"Deploying model from S3 or FSX","description":"You can deploy model artifacts directly from S3 or FSX to your HyperPod cluster using the InferenceEndpointConfig resource. The inference operator will use the S3 CSI driver to provide the model files to the pods in the cluster. Using this configuration, the operator will download the files located under the prefix deepseek15b as set by the modelLocation parameter.","sidebar":"tutorialSidebar"},"eks-blueprints/inference/inference-operator/sagemaker-jumpstart":{"id":"eks-blueprints/inference/inference-operator/sagemaker-jumpstart","title":"SageMaker JumpStart","description":"Amazon SageMaker JumpStart provides pretrained, open-source models for a wide range of problem types to help you get started with machine learning. You can incrementally train and tune these models before deployment. JumpStart also provides solution templates that set up infrastructure for common use cases, and executable example notebooks for machine learning with SageMaker AI.","sidebar":"tutorialSidebar"},"eks-blueprints/inference/load-balancer-inference/inference-with-loadbalancer":{"id":"eks-blueprints/inference/load-balancer-inference/inference-with-loadbalancer","title":"Mistral 7B Inference with Load Balancer","description":"This guide demonstrates how to deploy Mistral 7B for inference using Hugging Face\'s Text Generation Inference (TGI) container and expose it through an AWS Load Balancer on SageMaker HyperPod EKS.","sidebar":"tutorialSidebar"},"eks-blueprints/inference/ray-service/ray-service-readme":{"id":"eks-blueprints/inference/ray-service/ray-service-readme","title":"Serving Stable Diffusion Model for Inference with Ray Serve","description":"Ray Serve is a scalable model serving library for building online inference APIs. Serve is framework-agnostic, so you can use a single toolkit to serve everything from deep learning models built with frameworks like PyTorch, TensorFlow, and Keras, to Scikit-Learn models, to arbitrary Python business logic.","sidebar":"tutorialSidebar"},"eks-blueprints/training/ddp/distributed-data-parallel":{"id":"eks-blueprints/training/ddp/distributed-data-parallel","title":"Distributed Data Parallel (DDP)","description":"This guide provides step-by-step instructions for setting up Distributed Data Parallel (DDP) training on EKS using PyTorch.","sidebar":"tutorialSidebar"},"eks-blueprints/training/fsdp/fully-sharded-data-parallel":{"id":"eks-blueprints/training/fsdp/fully-sharded-data-parallel","title":"Fully Sharded Data Parallelism (FSDP)","description":"This example showcases an easy way to get started with multi node FSDP training on Amazon EKS on SageMaker HyperPod. It is designed to be as simple as possible, requires no data preparation, and uses a docker image.","sidebar":"tutorialSidebar"},"eks-blueprints/training/megatron-lm/megatron-lm-readme":{"id":"eks-blueprints/training/megatron-lm/megatron-lm-readme","title":"NVIDIA Megatron-LM","description":"MegatronLM is a framework from Nvidia designed for training large language models (LLMs). We recommend reading the following papers to understand the various tuning options available:","sidebar":"tutorialSidebar"},"eks-blueprints/training/ray-train/ray-train-readme":{"id":"eks-blueprints/training/ray-train/ray-train-readme","title":"Ray Train","description":"Ray is an open-source distributed computing framework designed to run highly scalable and parallel Python applications. Ray manages, executes, and optimizes compute needs across AI workloads. It unifies infrastructure via a single, flexible framework\u2014enabling any AI workload from data processing to model training to model serving and beyond.","sidebar":"tutorialSidebar"},"eks-blueprints/training/trainium/aws-trainium":{"id":"eks-blueprints/training/trainium/aws-trainium","title":"AWS Trainium","description":"In this section, we showcase how to pre-train Llama3.1-8B, Llama3 8B model using Trn1.32xlarge/Trn1n.32xlarge instances using the Neuron Distributed library. To train the LLama model in this example, we will apply the following optimizations using the Neuron Distributed library:","sidebar":"tutorialSidebar"},"getting-started/install-pre-requisites":{"id":"getting-started/install-pre-requisites","title":"Installing the required tools","description":"Before getting started with SageMaker HyperPod, we will configure our environment with the required tools.","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/Adding a Data Repository Assocation":{"id":"getting-started/orchestrated-by-eks/Adding a Data Repository Assocation","title":"Adding a Data Repository Association","description":"Amazon S3 Data Repository Association","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/additional-information":{"id":"getting-started/orchestrated-by-eks/additional-information","title":"Additional Information","description":"What Environment Variables are Set?","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/initial-cluster-setup":{"id":"getting-started/orchestrated-by-eks/initial-cluster-setup","title":"New cluster creation experience","description":"Initial cluster setup","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/Reviewing the cluster console":{"id":"getting-started/orchestrated-by-eks/Reviewing the cluster console","title":"Reviewing the cluster console","description":":image[SageMaker Logo]{src=\\"/img/01-cluster/sagemaker_logo.png\\" width=128}","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/sagemaker-studio-integration":{"id":"getting-started/orchestrated-by-eks/sagemaker-studio-integration","title":"SageMaker Studio Integration","description":"This guide provides step-by-step instructions for setting up Amazon SageMaker Studio with Hyperpod on Amazon EKS, including FSx Lustre storage configuration.","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/Set up an Amazon S3 mountpoint":{"id":"getting-started/orchestrated-by-eks/Set up an Amazon S3 mountpoint","title":"Set up an Amazon S3 mountpoint","description":"With the Mountpoint for Amazon S3 Container Storage Interface (CSI) driver, your Kubernetes applications can access Amazon S3 objects through a file system interface, achieving high aggregate throughput without changing any application code. Built on Mountpoint for Amazon S3, the CSI driver presents an Amazon S3 bucket as a volume that can be accessed by containers in Amazon EKS and self-managed Kubernetes clusters. This section shows you how to deploy the Mountpoint for Amazon S3 CSI driver to your Amazon EKS cluster.","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/Set up your shared file system":{"id":"getting-started/orchestrated-by-eks/Set up your shared file system","title":"Set up your shared file system","description":"Install the Amazon FSx for Lustre CSI Driver","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/Verifying cluster connection to EKS":{"id":"getting-started/orchestrated-by-eks/Verifying cluster connection to EKS","title":"Verifying cluster connection to EKS","description":"Source HyperPod Environment Variables","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-slurm/initial-cluster-setup":{"id":"getting-started/orchestrated-by-slurm/initial-cluster-setup","title":"New cluster creation experience","description":"SageMaker HyperPod now provides a new cluster creation experience that sets up all the resources needed for large-scale AI/ML workloads, including, networking, storage, compute, and IAM permissions in just a few clicks. The new cluster creation experience for SageMaker HyperPod introduces dual quick and custom setup paths that simplify getting started for both beginners and advanced AWS customers.","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-slurm/sagemaker-studio-integration":{"id":"getting-started/orchestrated-by-slurm/sagemaker-studio-integration","title":"SageMaker Studio Integration","description":"This guide provides step-by-step instructions for setting up Amazon SageMaker Studio with Amazon SageMaker Hyperpod SLURM, including FSx Lustre storage configuration.","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-slurm/slurm-basics":{"id":"getting-started/orchestrated-by-slurm/slurm-basics","title":"Basic Slurm Commands","description":"Now that you\'ve created and set up the cluster, you will go through some of the commands you\'ll use to run Llama7b on the cluster.","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-slurm/ssh-into-hyperpod":{"id":"getting-started/orchestrated-by-slurm/ssh-into-hyperpod","title":"SSH Into Your HyperPod Cluster","description":"Login to your cluster","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-slurm/View the AWS Console":{"id":"getting-started/orchestrated-by-slurm/View the AWS Console","title":"View the AWS Console","description":"Now that we\'ve created a cluster, we can monitor the status in the SageMaker console, this will show us cluster status, running instances, node groups, and allow us to easy modify the cluster.","sidebar":"tutorialSidebar"},"hyperpod-recipes/eks-hyperpod-recipes/hyperpod-recipes":{"id":"hyperpod-recipes/eks-hyperpod-recipes/hyperpod-recipes","title":"Setup and Launch training - EKS","description":"Prerequisites","sidebar":"tutorialSidebar"},"hyperpod-recipes/index.en":{"id":"hyperpod-recipes/index.en","title":"SageMaker HyperPod Recipes Overview","description":"Amazon SageMaker HyperPod recipes help you get started with training and fine-tuning publicly available foundation models. The recipes provide a pre-packaged set of training stack configurations that enable state-of-art training performance on SageMaker HyperPod. You can also easily switch between GPU-based instances and TRN-based instances with a simple recipe change.","sidebar":"tutorialSidebar"},"hyperpod-recipes/slurm-hyperpod-recipes/setup-and-train":{"id":"hyperpod-recipes/slurm-hyperpod-recipes/setup-and-train","title":"Setup and Launch training - Slurm","description":"Prerequisites","sidebar":"tutorialSidebar"},"infrastructure-as-a-code/amazon-cloudFormation/amazon-cloudformation":{"id":"infrastructure-as-a-code/amazon-cloudFormation/amazon-cloudformation","title":"CloudFormation Deployment","description":"This guide covers deploying SageMaker HyperPod infrastructure using CloudFormation templates. CloudFormation templates are available for both EKS and Slurm orchestration types, providing Infrastructure as Code (IaC) solutions.","sidebar":"tutorialSidebar"},"infrastructure-as-a-code/terraform/terraform":{"id":"infrastructure-as-a-code/terraform/terraform","title":"Terraform Deployment","description":"This guide covers deploying SageMaker HyperPod infrastructure using Terraform modules from the awsome-distributed-training repository. Terraform modules are available for both EKS and Slurm orchestration types.","sidebar":"tutorialSidebar"},"Introduction":{"id":"Introduction","title":"Introduction","description":"\ud83d\udca1 Optimized Blueprints for deploying high performance clusters to train, fine tune, and host (inference) models on Amazon Sagemaker Hyperpod","sidebar":"tutorialSidebar"},"slurm-blueprints/fine-tuning/peft/lora/lora":{"id":"slurm-blueprints/fine-tuning/peft/lora/lora","title":"LoRA - Tranium","description":"This example showcases how to train Llama 3 models using AWS Trainium instances and \ud83e\udd17 Optimum Neuron. \ud83e\udd17 Optimum Neuron is the interface between the \ud83e\udd17 Transformers library and AWS Accelerators including AWS Trainium and AWS Inferentia. It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks.","sidebar":"tutorialSidebar"},"slurm-blueprints/fine-tuning/peft/qlora/qlora":{"id":"slurm-blueprints/fine-tuning/peft/qlora/qlora","title":"QLoRA (Quantized LoRA) Fine-tuning","description":"This section is under development. QLoRA fine-tuning documentation will be available soon.","sidebar":"tutorialSidebar"},"slurm-blueprints/fine-tuning/preference-aligment/dpo":{"id":"slurm-blueprints/fine-tuning/preference-aligment/dpo","title":"dpo","description":"This section is under development. DPO fine-tuning documentation will be available soon.","sidebar":"tutorialSidebar"},"slurm-blueprints/fine-tuning/preference-aligment/grpo":{"id":"slurm-blueprints/fine-tuning/preference-aligment/grpo","title":"grpo","description":"This section is under development. GRPO fine-tuning documentation will be available soon.","sidebar":"tutorialSidebar"},"slurm-blueprints/fine-tuning/preference-aligment/ppo":{"id":"slurm-blueprints/fine-tuning/preference-aligment/ppo","title":"ppo","description":"This section is under development. PPO fine-tuning documentation will be available soon.","sidebar":"tutorialSidebar"},"slurm-blueprints/training/ddp/distributed-data-parallel":{"id":"slurm-blueprints/training/ddp/distributed-data-parallel","title":"PyTorch DDP on CPU","description":"This example showcases CPU PyTorch DDP environment setup utilizing two different approaches for managing the software environment, Anaconda and Docker:","sidebar":"tutorialSidebar"},"slurm-blueprints/training/fsdp/fully-sharded-data-parallel":{"id":"slurm-blueprints/training/fsdp/fully-sharded-data-parallel","title":"Fully Sharded Data Parallel","description":"These scripts provide an easy way to get started with multinode FSDP training on Slurm. It is designed to be as simple as possible, requires no data preparation, and uses a simple Conda environment.","sidebar":"tutorialSidebar"},"slurm-blueprints/training/megatron-lm/megatron-lm-readme":{"id":"slurm-blueprints/training/megatron-lm/megatron-lm-readme","title":"NVIDIA Megatron-LM","description":"MegatronLM is a framework from Nvidia that can be used to train LLMs. We recommend that you read papers on the framework to know the different knobs you can tune and in particular these articles:","sidebar":"tutorialSidebar"},"slurm-blueprints/training/trainium/Llama3-70B/download-dataset":{"id":"slurm-blueprints/training/trainium/Llama3-70B/download-dataset","title":"Downloading the Wiki-corpus datasets","description":"In this section, we will download and preprocess the wiki-corpus and tokenize it for training.","sidebar":"tutorialSidebar"},"slurm-blueprints/training/trainium/Llama3-70B/download-model":{"id":"slurm-blueprints/training/trainium/Llama3-70B/download-model","title":"Downloading the Llama3-70b model","description":"In this section, we will download the Llama3 model and the llama tokenizer. We will then also prepare the model for the Neuron runtime by converting the model weights to be pre-sharded based on the parallel processing configuration (i.e., the degrees of the model parallelism axes).","sidebar":"tutorialSidebar"},"slurm-blueprints/training/trainium/Llama3-70B/Llama3-70B":{"id":"slurm-blueprints/training/trainium/Llama3-70B/Llama3-70B","title":"Llama-3 70B (trn1.32xlarge) using NxD","description":"Llama","sidebar":"tutorialSidebar"},"slurm-blueprints/training/trainium/Llama3-70B/prep":{"id":"slurm-blueprints/training/trainium/Llama3-70B/prep","title":"Setting up the software stack","description":"Tranium","sidebar":"tutorialSidebar"},"slurm-blueprints/training/trainium/Llama3-70B/training":{"id":"slurm-blueprints/training/trainium/Llama3-70B/training","title":"Running Continual Pre-training with NeuronX Distributed","description":"Okay, now that we\'ve pre-processed the data and the model checkpoints, we are ready to submit a continual pre-training job. We have two sub-directories under /fsx/ubuntu/llama:","sidebar":"tutorialSidebar"},"Tips/Common/downsize-fsx":{"id":"Tips/Common/downsize-fsx","title":"Downsize existing FSx Volume","description":"Instructions to downsize an existing FSxL Filesystem using DRA import/export","sidebar":"tutorialSidebar"},"Tips/Common/Fsx for Lustre best practices":{"id":"Tips/Common/Fsx for Lustre best practices","title":"FSx for Lustre Best practices","description":"Amazon FSx for Lustre, built on Lustre, the popular high-performance file system, provides scale-out performance that increases linearly with a \ufb01le system\u2019s size. Lustre file systems scale horizontally across multiple file servers and disks. This scaling gives each client direct access to the data stored on each disk to remove many of the bottlenecks present in traditional file systems. Amazon FSx for Lustre builds on Lustre\'s scalable architecture to support high levels of performance across large numbers of clients.","sidebar":"tutorialSidebar"},"Tips/Common/Fsx for Lustre compression":{"id":"Tips/Common/Fsx for Lustre compression","title":"FSx for Lustre compression","description":"This section will enable data compression for the Amazon FSx for Lustre file system.","sidebar":"tutorialSidebar"},"Tips/Common/Fsx for Lustre performance":{"id":"Tips/Common/Fsx for Lustre performance","title":"FSx for Lustre performance","description":"In this section you\'ll run IOPS and throughput performance tests agains the FSx for Lustre file system using IOR framework.","sidebar":"tutorialSidebar"},"Tips/Common/link-fsx-to-S3":{"id":"Tips/Common/link-fsx-to-S3","title":"Link FSx Filesystems to Amazon S3 (Data Repository Association)","description":"Once a filesystem has been created in Amazon FSx, you can now link it to an S3 Bucket. This allows you to sync data back and forth between the filesystem and S3. It also allows you to delete the filesystem and preserve it\u2019s content on S3.","sidebar":"tutorialSidebar"},"Tips/Common/mount-additional-fsx":{"id":"Tips/Common/mount-additional-fsx","title":"Mount additional FSx Filesystem","description":"Ok, so let\'s say you want to mount an additional FSx Lustre filesystem, to do this you\'ll need to modify your lifecycle scripts:","sidebar":"tutorialSidebar"},"Tips/Common/move-files-s3":{"id":"Tips/Common/move-files-s3","title":"Upload files to Amazon s3","description":"You may wish to move files from your hyperpod cluster to an s3 bucket within your account for any number of reasons, including:","sidebar":"tutorialSidebar"},"Tips/EKS/add-users":{"id":"Tips/EKS/add-users","title":"Add users to your EKS HyperPod cluster","description":"If you have multiple users trying to get access to your EKS cluster, you would need to set up IAM Access Entries for EKS. This section of the workshop details the steps you can use for this purpose.","sidebar":"tutorialSidebar"},"Tips/EKS/event-bridge":{"id":"Tips/EKS/event-bridge","title":"Receive cluster status/health events","description":"Integration with Amazon EventBridge","sidebar":"tutorialSidebar"},"Tips/EKS/ssm-login":{"id":"Tips/EKS/ssm-login","title":"SSM Login","description":"Log into cluster instances using AWS Systems Manager","sidebar":"tutorialSidebar"},"Tips/Slurm/bastion-host":{"id":"Tips/Slurm/bastion-host","title":"Bastion Host","description":"Ok so what if we want to access our cluster with normal ssh and not ssm?","sidebar":"tutorialSidebar"},"Tips/Slurm/containers":{"id":"Tips/Slurm/containers","title":"Containers","description":"Containers","sidebar":"tutorialSidebar"},"Tips/Slurm/delete-cluster-nodes":{"id":"Tips/Slurm/delete-cluster-nodes","title":"Delete Cluster Nodes","description":"The SageMaker BatchDeleteClusterNode API allows you to delete specific nodes within a SageMaker HyperPod cluster. BatchDeleteClusterNodes accepts a cluster name and a list of node IDs.","sidebar":"tutorialSidebar"},"Tips/Slurm/diagnose-gpus":{"id":"Tips/Slurm/diagnose-gpus","title":"Diagnose GPU Failures","description":"To diagnose a node with a bad gpu ip-10-1-69-242 on SageMaker HyperPod, do the following:","sidebar":"tutorialSidebar"},"Tips/Slurm/enable-cgroups":{"id":"Tips/Slurm/enable-cgroups","title":"Configure Cgroups for Slurm","description":"Cgroups is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, etc.) of a collection of processes. In traditional environments, Cgroups allow system administrators to allocate resources such as CPU time, system memory, disk bandwidth, etc., among user-defined groups of tasks (processes) running on a system. We can configure Slurm to use Cgroups to constrain resources at the Slurm job and task level. A popular usecase for implementing Cgroups with Slurm is to use Process tracking proctrac/cgroup to isolate processes to a slurm job, thus ensuring all processes created by the job are contained within a cgroup, which helps in monitoring and controlling the resource usage by the job. It also helps in cleaning up processes after the job ends, ensuring that there are no \\"zombie\\" processes left running on the system.","sidebar":"tutorialSidebar"},"Tips/Slurm/gres":{"id":"Tips/Slurm/gres","title":"Gres (--gpus)","description":"This section describes how to setup Slurm Gres which allows scheduling jobs based on the number of gpu\'s needed i.e. --gpus=4. Please see the below note before proceeding with the setup:","sidebar":"tutorialSidebar"},"Tips/Slurm/heterogenous-cluster":{"id":"Tips/Slurm/heterogenous-cluster","title":"Heterogenous Cluster","description":"Adding Worker Groups to an existing cluster","sidebar":"tutorialSidebar"},"Tips/Slurm/login-node":{"id":"Tips/Slurm/login-node","title":"Login Node","description":"Login nodes allow users to login to the cluster, submit jobs, and view and manipulate data without running on the critical slurmctld scheduler node. This also allows you to run monitoring servers like aim, Tensorboard, or Grafana/Prometheus.","sidebar":"tutorialSidebar"},"Tips/Slurm/slurm-epilogue":{"id":"Tips/Slurm/slurm-epilogue","title":"Enable Slurm epilog Script","description":"Slurm epilog scripts can be used to perform tasks automatically after a job completes on a cluster. Implementing Slurm epilog scripts allows users and administrators to automate essential post-job tasks, such as resource cleanup, logging and monitoring, notifcations, and data management:","sidebar":"tutorialSidebar"},"Tips/Slurm/troubeshoot-permissions":{"id":"Tips/Slurm/troubeshoot-permissions","title":"Troubleshoot IAM Permissions","description":"Resolving AWS Configure Permissions Issues on HyperPod Nodes","sidebar":"tutorialSidebar"},"Tips/Slurm/troubleshooting":{"id":"Tips/Slurm/troubleshooting","title":"Troubleshooting","description":"So your cluster failed to create, what do you do now?","sidebar":"tutorialSidebar"},"validation-and-testing/environment-validation/efa-validation":{"id":"validation-and-testing/environment-validation/efa-validation","title":"EFA and Network Stack Validation","description":"This validation script checks the versions and configuration of the Elastic Fabric Adapter (EFA) network stack, including EFA installer, libfabric, AWS OFI NCCL, NCCL, and CUDA components. This is essential for ensuring optimal network performance in distributed training workloads.","sidebar":"tutorialSidebar"},"validation-and-testing/environment-validation/pytorch-environment-validation":{"id":"validation-and-testing/environment-validation/pytorch-environment-validation","title":"PyTorch Environment Validation","description":"This validation script runs a comprehensive PyTorch environment check to screen for NCCL, MPI, OpenMP, CUDA, and other critical components on your HyperPod cluster. The script executes once per instance and helps verify that your environment is properly configured for distributed training.","sidebar":"tutorialSidebar"},"validation-and-testing/nccl-cuda-validation/Troubleshoot NCCL and CUDA":{"id":"validation-and-testing/nccl-cuda-validation/Troubleshoot NCCL and CUDA","title":"Troubleshoot NCCL and CUDA","description":"There are moments where you are stuck either because things are not working or the performnace is not what you expected. Most, not always, it will be an issue with libraries and drivers. For GPU-based workloads, those issues can show up more frequently as there are many bits and pieces that need to be working together. A simple mismatch of a library version or not-optimized driver version for that specific librabry version can break things.","sidebar":"tutorialSidebar"},"validation-and-testing/performance-testing/gpu-stress-testing":{"id":"validation-and-testing/performance-testing/gpu-stress-testing","title":"GPU Stress Testing","description":"GPU stress testing validates hardware stability, thermal management, and performance consistency by putting GPUs under sustained computational load. This guide focuses on \\"burning\\" GPUs to test their limits and detect potential hardware issues.","sidebar":"tutorialSidebar"},"validation-and-testing/performance-testing/nccl-tests":{"id":"validation-and-testing/performance-testing/nccl-tests","title":"NCCL Performance Tests","description":"The NCCL Tests are a comprehensive testing suite that evaluates network performance between GPU instances using the NVIDIA Collective Communication Library. This is essential for validating cluster performance and troubleshooting issues before starting distributed training workloads.","sidebar":"tutorialSidebar"},"validation-and-testing/performance-testing/nccom-tests":{"id":"validation-and-testing/performance-testing/nccom-tests","title":"NCCOM Tests (Trainium)","description":"nccom-test is a benchmarking tool for evaluating the performance of Collective Communication operations on Trainium instances (trn1 and inf2). It provides a fast way to validate your Neuron environment before running complex distributed training workloads.","sidebar":"tutorialSidebar"},"validation-and-testing/resiliency/eks-resiliency":{"id":"validation-and-testing/resiliency/eks-resiliency","title":"Testing Resiliency with HyperPod EKS","description":"This guide demonstrates how to test and validate the resiliency features of SageMaker HyperPod when using EKS as the orchestrator. You\'ll learn how to monitor node health, manually trigger node replacement/reboot, simulate failures, and test job auto-resume functionality.","sidebar":"tutorialSidebar"},"validation-and-testing/resiliency/overview":{"id":"validation-and-testing/resiliency/overview","title":"Resiliency Overview","description":"SageMaker HyperPod is built for resilient training with comprehensive health monitoring and automatic recovery capabilities. This section provides an overview of the resiliency features that apply to both HyperPod EKS and HyperPod Slurm orchestrators.","sidebar":"tutorialSidebar"},"validation-and-testing/resiliency/slurm-resiliency":{"id":"validation-and-testing/resiliency/slurm-resiliency","title":"Testing Resiliency with HyperPod Slurm","description":"This guide demonstrates how to test and validate the resiliency features of SageMaker HyperPod when using Slurm as the orchestrator. You\'ll learn how to submit resilient training jobs, inject failures, monitor cluster recovery, and manually replace nodes.","sidebar":"tutorialSidebar"}}}}')}}]);