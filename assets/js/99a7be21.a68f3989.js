"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[7685],{6429:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"eks-blueprints/training/fsdp/fully-sharded-data-parallel","title":"Fully Sharded Data Parallelism (FSDP)","description":"This example showcases an easy way to get started with multi node FSDP training on Amazon EKS on SageMaker HyperPod. It is designed to be as simple as possible, requires no data preparation, and uses a docker image.","source":"@site/docs/01-eks-blueprints/training/fsdp/fully-sharded-data-parallel.md","sourceDirName":"01-eks-blueprints/training/fsdp","slug":"/eks-blueprints/training/fsdp/fully-sharded-data-parallel","permalink":"/docs/eks-blueprints/training/fsdp/fully-sharded-data-parallel","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Fully Sharded Data Parallelism (FSDP)","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Distributed Data Parallel (DDP)","permalink":"/docs/eks-blueprints/training/ddp/distributed-data-parallel"},"next":{"title":"NVIDIA Megatron-LM","permalink":"/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme"}}');var t=s(4848),r=s(8453);const i={title:"Fully Sharded Data Parallelism (FSDP)",sidebar_position:1},o="Get Started Training Llama 2 with PyTorch FSDP in 5 Minutes",l={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Infrastructure Requirements",id:"infrastructure-requirements",level:3},{value:"Development Environment",id:"development-environment",level:3},{value:"AWS Permissions",id:"aws-permissions",level:3},{value:"Cluster Validation",id:"cluster-validation",level:3},{value:"Verified Instance Types and Counts",id:"verified-instance-types-and-counts",level:3},{value:"Model Size Configurations",id:"model-size-configurations",level:3},{value:"Step 1: Setup the Docker Image",id:"step-1-setup-the-docker-image",level:2},{value:"1.1 Clone the Repository",id:"11-clone-the-repository",level:3},{value:"1.2 Build a Docker Image",id:"12-build-a-docker-image",level:3},{value:"1.3 Push the Image to Amazon ECR",id:"13-push-the-image-to-amazon-ecr",level:3},{value:"Step 2: Data and HuggingFace Setup",id:"step-2-data-and-huggingface-setup",level:2},{value:"2.1 Understanding the Dataset",id:"21-understanding-the-dataset",level:3},{value:"2.2 Create HuggingFace Token",id:"22-create-huggingface-token",level:3},{value:"Step 3: Start Your Training Run",id:"step-3-start-your-training-run",level:2},{value:"3.1 Install envsubst",id:"31-install-envsubst",level:3},{value:"3.2 Generate Manifest from Template",id:"32-generate-manifest-from-template",level:3},{value:"3.3 Deploy the Training Job",id:"33-deploy-the-training-job",level:3},{value:"3.4 Monitor Your Training Job",id:"34-monitor-your-training-job",level:3},{value:"3.5 Stop the Training",id:"35-stop-the-training",level:3},{value:"Alternative: Start Training with the HyperPod CLI",id:"alternative-start-training-with-the-hyperpod-cli",level:2},{value:"Set environment variables",id:"set-environment-variables",level:3},{value:"Generate a job configuration file",id:"generate-a-job-configuration-file",level:3},{value:"Start training job",id:"start-training-job",level:3},{value:"Monitor",id:"monitor",level:3},{value:"Troubleshoot",id:"troubleshoot",level:3},{value:"Stop",id:"stop",level:3}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components},{Details:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"get-started-training-llama-2-with-pytorch-fsdp-in-5-minutes",children:"Get Started Training Llama 2 with PyTorch FSDP in 5 Minutes"})}),"\n",(0,t.jsxs)(n.p,{children:["This example showcases an easy way to get started with multi node ",(0,t.jsx)(n.a,{href:"https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html",children:"FSDP"})," training on Amazon EKS on SageMaker HyperPod. It is designed to be as simple as possible, requires no data preparation, and uses a docker image."]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before starting, ensure you have completed the following setup:"}),"\n",(0,t.jsx)(n.h3,{id:"infrastructure-requirements",children:"Infrastructure Requirements"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"SageMaker HyperPod EKS cluster"})," deployed and running"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"GPU node groups"})," with appropriate instance types (e.g., ml.g5.8xlarge, ml.p5en.48xlarge)"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"GPU device plugin"})," installed on the cluster"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"EFA device plugin"})," installed for high-performance networking"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Kubeflow Training Operator"})," installed on the cluster"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"development-environment",children:"Development Environment"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"AWS CLI v2"})," installed and configured with appropriate permissions"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"kubectl"})," installed and configured to access your EKS cluster"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Docker"})," installed on your development machine (x86-64 based)"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"envsubst"})," utility for template processing"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Git"})," for cloning repositories"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"HuggingFace account and token"})," for dataset access"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"aws-permissions",children:"AWS Permissions"}),"\n",(0,t.jsx)(n.p,{children:"Your AWS credentials should have permissions for:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Amazon ECR"})," - push/pull container images"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Amazon EKS"})," - access cluster resources"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Amazon EC2"})," - describe instances and availability zones"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"AWS STS"})," - get caller identity"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"cluster-validation",children:"Cluster Validation"}),"\n",(0,t.jsx)(n.p,{children:"Verify your cluster is ready:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Check cluster status and GPU availability\nkubectl get nodes "-o=custom-columns=NAME:.metadata.name,INSTANCETYPE:.metadata.labels.node\\.kubernetes\\.io/instance-type,GPU:.status.allocatable.nvidia\\.com/gpu,EFA:.status.allocatable.vpc\\.amazonaws\\.com/efa"\n\n# Verify Kubeflow Training Operator is running\nkubectl get pods -n kubeflow\n\n# Check GPU device plugin\nkubectl get daemonset -n kube-system | grep nvidia\n\n# Verify EFA device plugin\nkubectl get daemonset -n kube-system | grep aws-efa\n\n# Verify you can create resources\nkubectl auth can-i create pytorchjobs\n'})}),"\n",(0,t.jsx)(n.h3,{id:"verified-instance-types-and-counts",children:"Verified Instance Types and Counts"}),"\n",(0,t.jsx)(n.p,{children:"This example has been verified with:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ml.p5en.48xlarge x 2"})," - High-performance training setup"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Please note you can change the model size to accommodate for other instance types."}),"\n",(0,t.jsx)(n.h3,{id:"model-size-configurations",children:"Model Size Configurations"}),"\n",(0,t.jsxs)(n.p,{children:["The following table shows the parameters for different Llama model sizes based on the ",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2307.09288",children:"Llama 2"})," and ",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2407.21783",children:"Llama 3"})," papers:"]}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Parameter"}),(0,t.jsx)(n.th,{children:"Llama 2 7B"}),(0,t.jsx)(n.th,{children:"Llama 2 13B"}),(0,t.jsx)(n.th,{children:"Llama 2 70B"}),(0,t.jsx)(n.th,{children:"Llama 3.1 8B"}),(0,t.jsx)(n.th,{children:"Llama 3.1 70B"}),(0,t.jsx)(n.th,{children:"Llama 3.2 1B"}),(0,t.jsx)(n.th,{children:"Llama 3.2 3B"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"intermediate_size"})}),(0,t.jsx)(n.td,{children:"11008"}),(0,t.jsx)(n.td,{children:"13824"}),(0,t.jsx)(n.td,{children:"28672"}),(0,t.jsx)(n.td,{children:"14336"}),(0,t.jsx)(n.td,{children:"28672"}),(0,t.jsx)(n.td,{children:"8192"}),(0,t.jsx)(n.td,{children:"11008"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"num_key_value_heads"})}),(0,t.jsx)(n.td,{children:"32"}),(0,t.jsx)(n.td,{children:"40"}),(0,t.jsx)(n.td,{children:"8"}),(0,t.jsx)(n.td,{children:"8"}),(0,t.jsx)(n.td,{children:"8"}),(0,t.jsx)(n.td,{children:"8"}),(0,t.jsx)(n.td,{children:"8"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"hidden_width"})}),(0,t.jsx)(n.td,{children:"4096"}),(0,t.jsx)(n.td,{children:"5120"}),(0,t.jsx)(n.td,{children:"8192"}),(0,t.jsx)(n.td,{children:"4096"}),(0,t.jsx)(n.td,{children:"8192"}),(0,t.jsx)(n.td,{children:"2048"}),(0,t.jsx)(n.td,{children:"3072"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"num_layers"})}),(0,t.jsx)(n.td,{children:"32"}),(0,t.jsx)(n.td,{children:"40"}),(0,t.jsx)(n.td,{children:"80"}),(0,t.jsx)(n.td,{children:"32"}),(0,t.jsx)(n.td,{children:"80"}),(0,t.jsx)(n.td,{children:"16"}),(0,t.jsx)(n.td,{children:"28"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"num_heads"})}),(0,t.jsx)(n.td,{children:"32"}),(0,t.jsx)(n.td,{children:"40"}),(0,t.jsx)(n.td,{children:"64"}),(0,t.jsx)(n.td,{children:"32"}),(0,t.jsx)(n.td,{children:"64"}),(0,t.jsx)(n.td,{children:"32"}),(0,t.jsx)(n.td,{children:"24"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"max_context_length"})}),(0,t.jsx)(n.td,{children:"4096"}),(0,t.jsx)(n.td,{children:"4096"}),(0,t.jsx)(n.td,{children:"4096"}),(0,t.jsx)(n.td,{children:"8192"}),(0,t.jsx)(n.td,{children:"8192"}),(0,t.jsx)(n.td,{children:"8192"}),(0,t.jsx)(n.td,{children:"8192"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"These configurations can be used to adjust the model parameters in your training scripts based on your compute requirements and available instance types."}),"\n",(0,t.jsx)(n.h2,{id:"step-1-setup-the-docker-image",children:"Step 1: Setup the Docker Image"}),"\n",(0,t.jsx)(n.h3,{id:"11-clone-the-repository",children:"1.1 Clone the Repository"}),"\n",(0,t.jsx)(n.p,{children:"The first step is to get the FSDP training code and Docker configuration. We'll clone the AWS distributed training examples repository which contains pre-built PyTorch FSDP examples optimized for Kubernetes."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ~\ngit clone https://github.com/aws-samples/awsome-distributed-training/\ncd awsome-distributed-training/3.test_cases/pytorch/FSDP\n"})}),"\n",(0,t.jsx)(n.h3,{id:"12-build-a-docker-image",children:"1.2 Build a Docker Image"}),"\n",(0,t.jsx)(n.p,{children:"Now we'll build a container image that includes PyTorch, FSDP training code, and all necessary dependencies. First, we need to authenticate with the public ECR registry to access base images."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/hpc-cloud\nexport REGION=$(aws ec2 describe-availability-zones --output text --query 'AvailabilityZones[0].[RegionName]')\nexport ACCOUNT=$(aws sts get-caller-identity --query Account --output text)\nexport REGISTRY=${ACCOUNT}.dkr.ecr.${REGION}.amazonaws.com/\n"})}),"\n",(0,t.jsx)(n.p,{children:"Build the container image:"}),"\n",(0,t.jsxs)(n.p,{children:["If you are on a Mac, use ",(0,t.jsx)(n.code,{children:"buildx"})," to target ",(0,t.jsx)(n.code,{children:"linux/amd64"})," architecture:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"docker buildx build --platform linux/amd64 -t ${REGISTRY}fsdp:pytorch2.5.1 .\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Alternatively,"})," if you are running in a SageMaker Studio environment"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"docker build $DOCKER_NETWORK -t ${REGISTRY}fsdp:pytorch2.5.1 .    \n"})}),"\n",(0,t.jsxs)(s,{children:[(0,t.jsx)("summary",{children:"Why $DOCKER_NETWORK?"}),(0,t.jsxs)(n.p,{children:["The environment variable ",(0,t.jsx)("code",{children:"$DOCKER_NETWORK"})," is set to ",(0,t.jsx)("code",{children:"--network=sagemaker"})," only if you deployed the SageMaker Studio Code Editor CloudFormation stack in the ",(0,t.jsx)("a",{href:"/docs/category/getting-started",children:"Set Up Your Development Environment"})," section. This is necessary because SageMaker Studio uses a specific network configuration for its containers. Otherwise, it remains unset."]})]}),"\n",(0,t.jsx)(n.p,{children:"Building the image can take 5~7 minutes. If successful, you should see the following success message at the end:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Successfully built 123ab12345cd\nSuccessfully tagged 123456789012.dkr.ecr.us-west-2.amazonaws.com/fsdp:pytorch2.5.1\n"})}),"\n",(0,t.jsx)(n.h3,{id:"13-push-the-image-to-amazon-ecr",children:"1.3 Push the Image to Amazon ECR"}),"\n",(0,t.jsx)(n.p,{children:"In this step we create a container registry if one does not exist, and push the container image to it. This makes the image available to your EKS cluster nodes."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Create registry if needed\nREGISTRY_COUNT=$(aws ecr describe-repositories | grep "fsdp" | wc -l)\nif [ "$REGISTRY_COUNT" -eq 0 ]; then\n    aws ecr create-repository --repository-name fsdp\nfi\n\n# Login to registry\necho "Logging in to $REGISTRY ..."\naws ecr get-login-password | docker login --username AWS --password-stdin $REGISTRY\n\n# Push image to registry\ndocker image push ${REGISTRY}fsdp:pytorch2.5.1\n'})}),"\n",(0,t.jsx)(n.p,{children:"Pushing the image may take some time depending on your network bandwidth. If you use EC2 / CloudShell as your development machine, it will take 6~8 minutes."}),"\n",(0,t.jsx)(n.h2,{id:"step-2-data-and-huggingface-setup",children:"Step 2: Data and HuggingFace Setup"}),"\n",(0,t.jsx)(n.h3,{id:"21-understanding-the-dataset",children:"2.1 Understanding the Dataset"}),"\n",(0,t.jsxs)(n.p,{children:["For this example, we'll be using the ",(0,t.jsx)(n.a,{href:"https://huggingface.co/datasets/allenai/c4",children:"allenai/c4"})," dataset. Instead of downloading the whole thing, the ",(0,t.jsx)(n.code,{children:"create_streaming_dataloaders"})," function will stream the dataset from ",(0,t.jsx)(n.a,{href:"https://huggingface.co/datasets",children:"HuggingFace"}),", so there's no data prep required for running this training."]}),"\n",(0,t.jsxs)(n.p,{children:["If you'd like to instead use your own dataset, you can do so by ",(0,t.jsx)(n.a,{href:"https://huggingface.co/docs/datasets/create_dataset",children:"formatting it as a HuggingFace dataset"}),", and passing its location to the ",(0,t.jsx)(n.code,{children:"--dataset_path"})," argument."]}),"\n",(0,t.jsx)(n.h3,{id:"22-create-huggingface-token",children:"2.2 Create HuggingFace Token"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"For this dataset, we will need a Hugging Face access token"}),". First, create a ",(0,t.jsx)(n.a,{href:"https://huggingface.co/welcome",children:"Hugging Face account"}),". Then ",(0,t.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"generate your access token with read permissions"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"We will reference this token in the next step by setting it as an environment variable."}),"\n",(0,t.jsx)(n.h2,{id:"step-3-start-your-training-run",children:"Step 3: Start Your Training Run"}),"\n",(0,t.jsx)(n.h3,{id:"31-install-envsubst",children:"3.1 Install envsubst"}),"\n",(0,t.jsxs)(n.p,{children:["This example uses ",(0,t.jsx)(n.a,{href:"https://github.com/a8m/envsubst",children:(0,t.jsx)(n.code,{children:"envsubst"})})," to generate a Kubernetes manifest file from a template file and parameters. If you don't have ",(0,t.jsx)(n.code,{children:"envsubst"})," on your development environment, install it by following the ",(0,t.jsx)(n.a,{href:"https://github.com/a8m/envsubst?tab=readme-ov-file#installation",children:"Installation instruction"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"32-generate-manifest-from-template",children:"3.2 Generate Manifest from Template"}),"\n",(0,t.jsxs)(n.p,{children:["With the ",(0,t.jsx)(n.code,{children:"envsubst"})," command, generate ",(0,t.jsx)(n.code,{children:"fsdp.yaml"})," from ",(0,t.jsx)(n.code,{children:"fsdp.yaml-template"}),". Please configure instance type, number of nodes, number of GPUs, number of EFAs, based on your cluster's specification."]}),"\n",(0,t.jsx)(n.p,{children:"You can check your cluster's specification by running the following command:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'kubectl get nodes "-o=custom-columns=NAME:.metadata.name,INSTANCETYPE:.metadata.labels.node\\.kubernetes\\.io/instance-type,GPU:.status.allocatable.nvidia\\.com/gpu,EFA:.status.allocatable.vpc\\.amazonaws\\.com/efa"\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"NAME                           INSTANCETYPE    GPU   EFA\nhyperpod-i-055aeff9546187dee   ml.g5.8xlarge   1     1\nhyperpod-i-09662f64f615c96f5   ml.g5.8xlarge   1     1\nhyperpod-i-099e2a84aba621d52   ml.g5.8xlarge   1     1\nhyperpod-i-0a6fea3329235be91   ml.g5.8xlarge   1     1\nhyperpod-i-0ac3feb733dc0f00e   ml.g5.8xlarge   1     1\nhyperpod-i-0bf7dce836e063fa6   ml.g5.8xlarge   1     1\nhyperpod-i-0ddf28f3ff2870f1b   ml.g5.8xlarge   1     1\nhyperpod-i-0fe48912b03d2c22e   ml.g5.8xlarge   1     1\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Change directories to the ",(0,t.jsx)(n.code,{children:"kubernetes"})," directory:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd kubernetes/\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Set environment variables and run ",(0,t.jsx)(n.code,{children:"envsubst"})," to generate ",(0,t.jsx)(n.code,{children:"fsdp.yaml"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"For ml.g5.8xlarge x 8:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"export IMAGE_URI=${REGISTRY}fsdp:pytorch2.5.1\nexport INSTANCE_TYPE=ml.g5.8xlarge\nexport NUM_NODES=8\nexport GPU_PER_NODE=1\nexport EFA_PER_NODE=1\nexport FI_PROVIDER=efa\nexport HF_TOKEN=<Your HuggingFace Token>\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cat fsdp.yaml-template | envsubst > fsdp.yaml\n"})}),"\n",(0,t.jsx)(n.h3,{id:"33-deploy-the-training-job",children:"3.3 Deploy the Training Job"}),"\n",(0,t.jsxs)(n.p,{children:["Now the manifest file ",(0,t.jsx)(n.code,{children:"fsdp.yaml"})," is generated, and you are ready to deploy the training workload. Run the following command to deploy the training workload."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f ./fsdp.yaml\n"})}),"\n",(0,t.jsx)(n.p,{children:"You should see the following message:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"pytorchjob.kubeflow.org/fsdp created\n"})}),"\n",(0,t.jsx)(n.h3,{id:"34-monitor-your-training-job",children:"3.4 Monitor Your Training Job"}),"\n",(0,t.jsx)(n.p,{children:"To see the status of your job, use the commands below:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get pytorchjob\nkubectl get pods\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"NAME   STATE     AGE\nfsdp   Running   5m\n\nNAME                    READY   STATUS              RESTARTS   AGE\netcd-7787559c74-pw4jp   1/1     Running             0          74s\nfsdp-worker-0           0/1     ContainerCreating   0          74s\nfsdp-worker-1           0/1     ContainerCreating   0          74s\nfsdp-worker-2           0/1     ContainerCreating   0          74s\nfsdp-worker-3           0/1     ContainerCreating   0          74s\nfsdp-worker-4           0/1     ContainerCreating   0          74s\nfsdp-worker-5           0/1     ContainerCreating   0          74s\nfsdp-worker-6           0/1     ContainerCreating   0          74s\nfsdp-worker-7           0/1     ContainerCreating   0          74s\n"})}),"\n",(0,t.jsxs)(n.p,{children:["When you run for the first time, it takes 3~4 minutes until the Pod statuses change from ",(0,t.jsx)(n.code,{children:"ContainerCreating"})," to ",(0,t.jsx)(n.code,{children:"Running"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"NAME                    READY   STATUS    RESTARTS   AGE\netcd-7787559c74-pw4jp   1/1     Running   0          3m43s\nfsdp-worker-0           1/1     Running   0          3m43s\nfsdp-worker-1           1/1     Running   0          3m43s\nfsdp-worker-2           1/1     Running   0          3m43s\nfsdp-worker-3           1/1     Running   0          3m43s\nfsdp-worker-4           1/1     Running   0          3m43s\nfsdp-worker-5           1/1     Running   0          3m43s\nfsdp-worker-6           1/1     Running   0          3m43s\nfsdp-worker-7           1/1     Running   0          3m43s\n"})}),"\n",(0,t.jsx)(n.p,{children:"Each of the pods produces job logs. One of the pods is elected master during job initialization. Only this pod will show the progress of the training job in its log. To find out which pod is currently the master, run the command below:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl logs fsdp-worker-0 | grep master_addr=\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"[2024-06-25 22:20:17,556] torch.distributed.elastic.agent.server.api: [INFO]   master_addr=fsdp-worker-1\n"})}),"\n",(0,t.jsx)(n.p,{children:"This shows that the pod fsdp-worker-1 is currently the master. To look at the current job logs, use the command below:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl logs -f fsdp-worker-1\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"    :\n2024-06-25 22:22:36 I [train.py:102] Batch 0 Loss: 11.63946, Speed: 0.27 samples/sec, lr: 0.000006\n2024-06-25 22:22:57 I [train.py:102] Batch 1 Loss: 11.66096, Speed: 0.39 samples/sec, lr: 0.000013\n2024-06-25 22:23:17 I [train.py:102] Batch 2 Loss: 11.56659, Speed: 0.40 samples/sec, lr: 0.000019\n2024-06-25 22:23:37 I [train.py:102] Batch 3 Loss: 11.14039, Speed: 0.40 samples/sec, lr: 0.000025\n    :\n"})}),"\n",(0,t.jsxs)(n.p,{children:["You can execute ",(0,t.jsx)(n.code,{children:"nvtop"})," command inside a running container within a Pod to see GPU utilization:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl exec -it fsdp-worker-4 -- nvtop\n"})}),"\n",(0,t.jsx)(n.h3,{id:"35-stop-the-training",children:"3.5 Stop the Training"}),"\n",(0,t.jsx)(n.p,{children:"To stop the current training job, use the following command:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl delete -f ./fsdp.yaml\n"})}),"\n",(0,t.jsx)(n.h2,{id:"alternative-start-training-with-the-hyperpod-cli",children:"Alternative: Start Training with the HyperPod CLI"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note:"}),"\nThis page shows how to run the sample application with HyperPod CLI, instead of ",(0,t.jsx)(n.code,{children:"kubectl"}),". If you didn't install the HyperPod CLI, see the ",(0,t.jsx)(n.a,{href:"/docs/add-ons/installing-the-hyperpod-cli",children:"Install HyperPod CLI"})," page."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"set-environment-variables",children:"Set environment variables"}),"\n",(0,t.jsx)(n.p,{children:"Check your cluster's specification by running following command:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'kubectl get nodes "-o=custom-columns=NAME:.metadata.name,INSTANCETYPE:.metadata.labels.node\\.kubernetes\\.io/instance-type,GPU:.status.allocatable.nvidia\\.com/gpu,EFA:.status.allocatable.vpc\\.amazonaws\\.com/efa"\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"NAME                           INSTANCETYPE    GPU   EFA\nhyperpod-i-055aeff9546187dee   ml.g5.8xlarge   1     1\nhyperpod-i-09662f64f615c96f5   ml.g5.8xlarge   1     1\nhyperpod-i-099e2a84aba621d52   ml.g5.8xlarge   1     1\nhyperpod-i-0a6fea3329235be91   ml.g5.8xlarge   1     1\nhyperpod-i-0ac3feb733dc0f00e   ml.g5.8xlarge   1     1\nhyperpod-i-0bf7dce836e063fa6   ml.g5.8xlarge   1     1\nhyperpod-i-0ddf28f3ff2870f1b   ml.g5.8xlarge   1     1\nhyperpod-i-0fe48912b03d2c22e   ml.g5.8xlarge   1     1\n"})}),"\n",(0,t.jsx)(n.p,{children:"Set following environment variables based on your cluster configuration."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"export IMAGE_URI=${REGISTRY}fsdp:pytorch2.5.1\nexport INSTANCE_TYPE=ml.g5.8xlarge\nexport NUM_NODES=8\nexport GPU_PER_NODE=1\n"})}),"\n",(0,t.jsx)(n.h3,{id:"generate-a-job-configuration-file",children:"Generate a job configuration file"}),"\n",(0,t.jsxs)(n.p,{children:["Run following command to generate a job configuration file (",(0,t.jsx)(n.code,{children:"hpcli-fsdp.yaml"}),") for HyperPod CLI."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'cat > hpcli-fsdp.yaml << EOL\ndefaults:\n - override hydra/job_logging: stdout\n\nhydra:\n run:\n  dir: .\n output_subdir: null\n\ntraining_cfg:\n entry_script: /fsdp/train.py\n script_args:\n    - --max_context_width: 4096\n    - --num_key_value_heads: 32\n    - --intermediate_size: 11008\n    - --hidden_width: 4096\n    - --num_layers: 32\n    - --num_heads: 32\n    - --model_type: llama_v2\n    - --tokenizer: hf-internal-testing/llama-tokenizer\n    - --checkpoint_freq: 5000\n    - --validation_freq: 500\n    - --max_steps: 5000\n    - --checkpoint_dir: /checkpoints\n    - --dataset: allenai/c4\n    - --dataset_config_name: en\n    - --resume_from_checkpoint: /checkpoints\n    - --train_batch_size: 1\n    - --val_batch_size: 1\n    - --sharding_strategy: full\n    - --offload_activation: 1\n\n run:\n  name: fsdp\n  nodes: ${NUM_NODES}\n  ntasks_per_node: ${GPU_PER_NODE}\ncluster:\n cluster_type: k8s\n instance_type: ${INSTANCE_TYPE}\n cluster_config:\n  service_account_name: null\n\n  volumes:\n    - volumeName: local\n      hostPath: "/mnt/k8s-disks/0"\n      mountPath: "/local"\n\n  namespace: kubeflow\n  label_selector:\n      required:\n          sagemaker.amazonaws.com/node-health-status:\n              - Schedulable\n      preferred:\n          sagemaker.amazonaws.com/deep-health-check-status:\n              - Passed\n      weights:\n          - 100\n  pullPolicy: Always\n  restartPolicy: OnFailure\n\n  annotations:\n    sagemaker.amazonaws.com/enable-job-auto-resume: True\n    sagemaker.amazonaws.com/job-max-retry-count: 10\n\nbase_results_dir: ./result\ncontainer: ${IMAGE_URI}\n\nenv_vars:\n LOGLEVEL: DEBUG\n TORCH_DISTRIBUTED_DEBUG: DETAIL\n TORCH_NCCL_ENABLE_MONITORING: 1\n TORCH_NCCL_TRACE_BUFFER_SIZE: 20000\n TORCH_NCCL_DUMP_ON_TIMEOUT: 1\n TORCH_NCCL_DEBUG_INFO_TEMP_FILE: /local/nccl_trace_rank_\n PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"\n NCCL_DEBUG: INFO\n NCCL_SOCKET_IFNAME: ^lo\n TORCH_NCCL_ASYNC_ERROR_HANDLING: 1\nEOL\n'})}),"\n",(0,t.jsx)(n.h3,{id:"start-training-job",children:"Start training job"}),"\n",(0,t.jsxs)(n.p,{children:["Now the job configuration file ",(0,t.jsx)(n.code,{children:"hpcli-fsdp.yaml"})," is generated, and you are ready to start the training job."]}),"\n",(0,t.jsxs)(n.p,{children:["Before startuing the job, you need to select the cluster with ",(0,t.jsx)(n.code,{children:"hyperpod connect-cluster"})," command."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"hyperpod connect-cluster --cluster-name ml-cluster\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Then run ",(0,t.jsx)(n.code,{children:"hyperpod start-job"})," command to start the job."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"hyperpod start-job --config-file ./hpcli-fsdp.yaml\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'{\n "Console URL": "https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/cluster-management/ml-cluster"\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"monitor",children:"Monitor"}),"\n",(0,t.jsx)(n.p,{children:"To see the status of your job, use the commands below:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"hyperpod get-job --job-name fsdp -n kubeflow\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n "Name": "fsdp",\n "Namespace": "kubeflow",\n "Label": {\n  "app": "fsdp",\n  "app.kubernetes.io/managed-by": "Helm"\n },\n "CreationTimestamp": "2024-09-26T01:06:51Z",\n "Status": {\n  "conditions": [\n   {\n    "lastTransitionTime": "2024-09-26T01:06:51Z",\n    "lastUpdateTime": "2024-09-26T01:06:51Z",\n    "message": "PyTorchJob fsdp is created.",\n    "reason": "PyTorchJobCreated",\n    "status": "True",\n    "type": "Created"\n   },\n   {\n    "lastTransitionTime": "2024-09-26T01:07:02Z",\n    "lastUpdateTime": "2024-09-26T01:07:02Z",\n    "message": "PyTorchJob kubeflow/fsdp is running.",\n    "reason": "PyTorchJobRunning",\n    "status": "True",\n    "type": "Running"\n   }\n  ],\n  "replicaStatuses": {\n   "Worker": {\n    "active": 8,\n    "selector": "training.kubeflow.org/job-name=fsdp,training.kubeflow.org/operator-name=pytorchjob-controller,training.kubeflow.org/replica-type=worker"\n   }\n  },\n  "startTime": "2024-09-26T01:07:00Z"\n },\n "ConsoleURL": "https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/cluster-management/k8-g5-8x-4"\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:["If you need more detailed information of the job, you can use ",(0,t.jsx)(n.code,{children:"--verbose"})," option."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"hyperpod get-job --job-name fsdp -n kubeflow --verbose\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n "Name": "fsdp",\n "Namespace": "kubeflow",\n "Label": {\n  "app": "fsdp",\n  "app.kubernetes.io/managed-by": "Helm"\n },\n "Annotations": {\n  "meta.helm.sh/release-name": "fsdp",\n  "meta.helm.sh/release-namespace": "kubeflow",\n  "sagemaker.amazonaws.com/enable-job-auto-resume": "true",\n  "sagemaker.amazonaws.com/job-max-retry-count": "10"\n },\n "Metadata": {\n  "CreationTimestamp": "2024-09-26T01:06:51Z",\n  "Generation": 1,\n  "ResourceVersion": "4240104",\n  "UID": "39364a40-70c7-4d03-abab-160c124e7367"\n },\n "Kind": "PyTorchJob",\n "ApiVersion": "kubeflow.org/v1",\n "Spec": {\n  "pytorchReplicaSpecs": {\n   "Worker": {\n    "replicas": 8,\n    "template": {\n     "spec": {\n      "affinity": {\n       "nodeAffinity": {\n        "preferredDuringSchedulingIgnoredDuringExecution": [\n         {\n          "preference": {\n           "matchExpressions": [\n            {\n             "key": "sagemaker.amazonaws.com/deep-health-check-status",\n             "operator": "In",\n             "values": [\n              "Passed"\n             ]\n            }\n           ]\n          },\n          "weight": 100\n         }\n        ],\n        "requiredDuringSchedulingIgnoredDuringExecution": {\n         "nodeSelectorTerms": [\n          {\n           "matchExpressions": [\n            {\n             "key": "sagemaker.amazonaws.com/node-health-status",\n             "operator": "In",\n             "values": [\n              "Schedulable"\n             ]\n            }\n           ]\n          }\n         ]\n        }\n       }\n      },\n      "containers": [\n       {\n        "command": [\n         "/etc/config/train-script.sh"\n        ],\n        "env": [\n         {\n          "name": "CUDA_DEVICE_MAX_CONNECTIONS",\n          "value": "1"\n         },\n         {\n          "name": "CUDA_VISIBLE_DEVICES",\n          "value": "0"\n         },\n         {\n          "name": "FI_EFA_FORK_SAFE",\n          "value": "1"\n         },\n         {\n          "name": "FI_PROVIDER",\n          "value": "efa"\n         },\n         {\n          "name": "LOGLEVEL",\n          "value": "DEBUG"\n         },\n         {\n          "name": "NCCL_DEBUG",\n          "value": "INFO"\n         },\n         {\n          "name": "NCCL_IGNORE_DISABLED_P2P",\n          "value": "1"\n         },\n         {\n          "name": "NCCL_PROTO",\n          "value": "simple"\n         },\n         {\n          "name": "NCCL_SOCKET_IFNAME",\n          "value": "^lo,docker0"\n         },\n         {\n          "name": "PYTORCH_CUDA_ALLOC_CONF",\n          "value": "expandable_segments:True"\n         },\n         {\n          "name": "TORCH_DISTRIBUTED_DEBUG",\n          "value": "DETAIL"\n         },\n         {\n          "name": "TORCH_DIST_INIT_BARRIER",\n          "value": "1"\n         },\n         {\n          "name": "TORCH_NCCL_ASYNC_ERROR_HANDLING",\n          "value": "1"\n         },\n         {\n          "name": "TORCH_NCCL_DEBUG_INFO_TEMP_FILE",\n          "value": "/local/nccl_trace_rank_"\n         },\n         {\n          "name": "TORCH_NCCL_DUMP_ON_TIMEOUT",\n          "value": "1"\n         },\n         {\n          "name": "TORCH_NCCL_ENABLE_MONITORING",\n          "value": "1"\n         },\n         {\n          "name": "TORCH_NCCL_TRACE_BUFFER_SIZE",\n          "value": "20000"\n         }\n        ],\n        "image": "842413447717.dkr.ecr.us-west-2.amazonaws.com/fsdp:pytorch2.2",\n        "imagePullPolicy": "Always",\n        "name": "pytorch",\n        "resources": {\n         "limits": {\n          "nvidia.com/gpu": 1,\n          "vpc.amazonaws.com/efa": 1\n         },\n         "requests": {\n          "nvidia.com/gpu": 1,\n          "vpc.amazonaws.com/efa": 1\n         }\n        },\n        "securityContext": {\n         "capabilities": {\n          "add": [\n           "IPC_LOCK"\n          ]\n         }\n        },\n        "volumeMounts": [\n         {\n          "mountPath": "/local",\n          "name": "local"\n         },\n         {\n          "mountPath": "/etc/config",\n          "name": "train-script"\n         },\n         {\n          "mountPath": "/dev/shm",\n          "name": "shm"\n         }\n        ]\n       }\n      ],\n      "restartPolicy": "OnFailure",\n      "volumes": [\n       {\n        "hostPath": {\n         "path": "/mnt/k8s-disks/0"\n        },\n        "name": "local"\n       },\n       {\n        "hostPath": {\n         "path": "/dev/shm",\n         "type": "Directory"\n        },\n        "name": "shm"\n       },\n       {\n        "configMap": {\n         "defaultMode": 420,\n         "items": [\n          {\n           "key": "train-script.sh",\n           "mode": 365,\n           "path": "train-script.sh"\n          }\n         ],\n         "name": "train-script-fsdp"\n        },\n        "name": "train-script"\n       }\n      ]\n     }\n    }\n   }\n  }\n },\n "Status": {\n  "conditions": [\n   {\n    "lastTransitionTime": "2024-09-26T01:06:51Z",\n    "lastUpdateTime": "2024-09-26T01:06:51Z",\n    "message": "PyTorchJob fsdp is created.",\n    "reason": "PyTorchJobCreated",\n    "status": "True",\n    "type": "Created"\n   },\n   {\n    "lastTransitionTime": "2024-09-26T01:07:02Z",\n    "lastUpdateTime": "2024-09-26T01:07:02Z",\n    "message": "PyTorchJob kubeflow/fsdp is running.",\n    "reason": "PyTorchJobRunning",\n    "status": "True",\n    "type": "Running"\n   }\n  ],\n  "replicaStatuses": {\n   "Worker": {\n    "active": 8,\n    "selector": "training.kubeflow.org/job-name=fsdp,training.kubeflow.org/operator-name=pytorchjob-controller,training.kubeflow.org/replica-type=worker"\n   }\n  },\n  "startTime": "2024-09-26T01:07:00Z"\n },\n "ConsoleURL": "https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/cluster-management/k8-g5-8x-4"\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:["You can use ",(0,t.jsx)(n.code,{children:"hyperpod list-pods"})," command to list pods."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"hyperpod list-pods --job-name fsdp -n kubeflow\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n "pods": [\n  {\n   "PodName": "fsdp-worker-0",\n   "Namespace": "kubeflow",\n   "Status": "Running",\n   "CreationTime": "2024-09-26 01:07:01+00:00"\n  },\n  {\n   "PodName": "fsdp-worker-1",\n   "Namespace": "kubeflow",\n   "Status": "Running",\n   "CreationTime": "2024-09-26 01:07:01+00:00"\n  },\n  {\n   "PodName": "fsdp-worker-2",\n   "Namespace": "kubeflow",\n   "Status": "Running",\n   "CreationTime": "2024-09-26 01:07:01+00:00"\n  },\n  {\n   "PodName": "fsdp-worker-3",\n   "Namespace": "kubeflow",\n   "Status": "Running",\n   "CreationTime": "2024-09-26 01:07:01+00:00"\n  },\n  {\n   "PodName": "fsdp-worker-4",\n   "Namespace": "kubeflow",\n   "Status": "Running",\n   "CreationTime": "2024-09-26 01:07:01+00:00"\n  },\n  {\n   "PodName": "fsdp-worker-5",\n   "Namespace": "kubeflow",\n   "Status": "Running",\n   "CreationTime": "2024-09-26 01:07:01+00:00"\n  },\n  {\n   "PodName": "fsdp-worker-6",\n   "Namespace": "kubeflow",\n   "Status": "Running",\n   "CreationTime": "2024-09-26 01:07:01+00:00"\n  },\n  {\n   "PodName": "fsdp-worker-7",\n   "Namespace": "kubeflow",\n   "Status": "Running",\n   "CreationTime": "2024-09-26 01:07:01+00:00"\n  }\n ]\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:["You can use ",(0,t.jsx)(n.code,{children:"hyperpod get-log"})," command to print logs from a pod."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"hyperpod get-log --job-name fsdp --pod fsdp-worker-0 -n kubeflow\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"    :\n2024-09-26 01:09:17 I [train.py:102] Batch 0 Loss: 11.67824, Speed: 0.40 samples/sec, lr: 0.000006\n2024-09-26 01:09:34 I [train.py:102] Batch 1 Loss: 11.71413, Speed: 0.47 samples/sec, lr: 0.000013\n2024-09-26 01:09:52 I [train.py:102] Batch 2 Loss: 11.55315, Speed: 0.46 samples/sec, lr: 0.000019\n2024-09-26 01:10:09 I [train.py:102] Batch 3 Loss: 11.21573, Speed: 0.47 samples/sec, lr: 0.000025\n2024-09-26 01:10:26 I [train.py:102] Batch 4 Loss: 10.91101, Speed: 0.46 samples/sec, lr: 0.000031\n    :\n"})}),"\n",(0,t.jsx)(n.h3,{id:"troubleshoot",children:"Troubleshoot"}),"\n",(0,t.jsxs)(n.p,{children:["When you don't see logs from pods, use ",(0,t.jsx)(n.code,{children:"kubectl"})," to check the status of underlying Kubernetes resources."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# List PyTorchJobs\nkubectl get pytorchjobs -n kubeflow\n\n# Get details of a PyTorchJob\nkubectl describe pytorchjob fsdp -n kubeflow\n\n# List Pods\nkubectl get pods -n kubeflow\n\n# Get details of a Pod\nkubectl describe pod fsdp-worker-0 -n kubeflow\n"})}),"\n",(0,t.jsx)(n.h3,{id:"stop",children:"Stop"}),"\n",(0,t.jsx)(n.p,{children:"To stop the current training job, use the following command."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"hyperpod cancel-job --job-name fsdp -n kubeflow\n"})}),"\n",(0,t.jsx)(n.p,{children:"And verify that list of jobs is empty."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"hyperpod list-jobs -n kubeflow\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n "jobs": []\n}\n'})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>o});var a=s(6540);const t={},r=a.createContext(t);function i(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);