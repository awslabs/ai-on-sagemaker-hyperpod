"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[849],{6164:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Introduction","href":"/docs/Introduction","docId":"Introduction","unlisted":false},{"type":"category","label":"Getting Started","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Orchestrated by EKS","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"New cluster creation experience","href":"/docs/getting-started/orchestrated-by-eks/initial-cluster-setup","docId":"getting-started/orchestrated-by-eks/initial-cluster-setup","unlisted":false},{"type":"link","label":"Reviewing the cluster console","href":"/docs/getting-started/orchestrated-by-eks/Reviewing the cluster console","docId":"getting-started/orchestrated-by-eks/Reviewing the cluster console","unlisted":false},{"type":"link","label":"Verifying cluster connection to EKS","href":"/docs/getting-started/orchestrated-by-eks/Verifying cluster connection to EKS","docId":"getting-started/orchestrated-by-eks/Verifying cluster connection to EKS","unlisted":false},{"type":"link","label":"Additional Information","href":"/docs/getting-started/orchestrated-by-eks/additional-information","docId":"getting-started/orchestrated-by-eks/additional-information","unlisted":false},{"type":"link","label":"Set up your shared file system","href":"/docs/getting-started/orchestrated-by-eks/Set up your shared file system","docId":"getting-started/orchestrated-by-eks/Set up your shared file system","unlisted":false},{"type":"link","label":"Adding a Data Repository Association","href":"/docs/getting-started/orchestrated-by-eks/Adding a Data Repository Assocation","docId":"getting-started/orchestrated-by-eks/Adding a Data Repository Assocation","unlisted":false},{"type":"link","label":"Set up an Amazon S3 mountpoint","href":"/docs/getting-started/orchestrated-by-eks/Set up an Amazon S3 mountpoint","docId":"getting-started/orchestrated-by-eks/Set up an Amazon S3 mountpoint","unlisted":false}],"href":"/docs/category/orchestrated-by-eks"},{"type":"category","label":"Orchestrated by SLURM","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"New cluster creation experience","href":"/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup","docId":"getting-started/orchestrated-by-slurm/initial-cluster-setup","unlisted":false},{"type":"link","label":"View the AWS Console","href":"/docs/getting-started/orchestrated-by-slurm/View the AWS Console","docId":"getting-started/orchestrated-by-slurm/View the AWS Console","unlisted":false},{"type":"link","label":"SSH Into Your HyperPod Cluster","href":"/docs/getting-started/orchestrated-by-slurm/ssh-into-hyperpod","docId":"getting-started/orchestrated-by-slurm/ssh-into-hyperpod","unlisted":false},{"type":"link","label":"Basic Slurm Commands","href":"/docs/getting-started/orchestrated-by-slurm/slurm-basics","docId":"getting-started/orchestrated-by-slurm/slurm-basics","unlisted":false}],"href":"/docs/category/orchestrated-by-slurm"},{"type":"link","label":"Installing the required tools","href":"/docs/getting-started/install-pre-requisites","docId":"getting-started/install-pre-requisites","unlisted":false}],"href":"/docs/category/getting-started"},{"type":"category","label":"EKS Blueprints","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Training","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"ddp","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Data Distributed Parallelism (DDP)","href":"/docs/eks-blueprints/training/ddp/distributed-data-parallel","docId":"eks-blueprints/training/ddp/distributed-data-parallel","unlisted":false}]},{"type":"category","label":"fsdp","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Fully Sharded Data Parallelism (FSDP)","href":"/docs/eks-blueprints/training/fsdp/fully-sharded-data-parallel","docId":"eks-blueprints/training/fsdp/fully-sharded-data-parallel","unlisted":false}]},{"type":"category","label":"megatron-lm","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"NVIDIA Megatron-LM","href":"/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme","docId":"eks-blueprints/training/megatron-lm/megatron-lm-readme","unlisted":false}]},{"type":"category","label":"ray-train","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Ray Train","href":"/docs/eks-blueprints/training/ray-train/ray-train-readme","docId":"eks-blueprints/training/ray-train/ray-train-readme","unlisted":false}]},{"type":"category","label":"trainium","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"AWS Trainium","href":"/docs/eks-blueprints/training/trainium/aws-trainium","docId":"eks-blueprints/training/trainium/aws-trainium","unlisted":false}]}],"href":"/docs/category/training"},{"type":"category","label":"Fine Tuning","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"peft","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"LoRA - Trainium","href":"/docs/eks-blueprints/fine-tuning/peft/low-rank-adaptation","docId":"eks-blueprints/fine-tuning/peft/low-rank-adaptation","unlisted":false},{"type":"link","label":"QLoRA (Quantized LoRA)","href":"/docs/eks-blueprints/fine-tuning/peft/quantisized-lora","docId":"eks-blueprints/fine-tuning/peft/quantisized-lora","unlisted":false}]},{"type":"category","label":"preference-alignment","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"dpo","href":"/docs/eks-blueprints/fine-tuning/preference-alignment/dpo","docId":"eks-blueprints/fine-tuning/preference-alignment/dpo","unlisted":false},{"type":"link","label":"ppo","href":"/docs/eks-blueprints/fine-tuning/preference-alignment/ppo","docId":"eks-blueprints/fine-tuning/preference-alignment/ppo","unlisted":false},{"type":"link","label":"grpo","href":"/docs/eks-blueprints/fine-tuning/preference-alignment/grpo","docId":"eks-blueprints/fine-tuning/preference-alignment/grpo","unlisted":false}]}],"href":"/docs/category/fine-tuning"},{"type":"category","label":"Inference","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"inference-operator","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"SageMaker JumpStart","href":"/docs/eks-blueprints/inference/inference-operator/sagemaker-jumpstart","docId":"eks-blueprints/inference/inference-operator/sagemaker-jumpstart","unlisted":false},{"type":"link","label":"Deploying model from S3 or FSX","href":"/docs/eks-blueprints/inference/inference-operator/amazon-s3-and-amazon-fsx","docId":"eks-blueprints/inference/inference-operator/amazon-s3-and-amazon-fsx","unlisted":false}]},{"type":"category","label":"load-balancer-inference","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Mistral 7B with Load Balancer","href":"/docs/eks-blueprints/inference/load-balancer-inference/inference-with-loadbalancer","docId":"eks-blueprints/inference/load-balancer-inference/inference-with-loadbalancer","unlisted":false}]},{"type":"category","label":"ray-service","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Serving Stable Diffusion Model for Inference with Ray Serve","href":"/docs/eks-blueprints/inference/ray-service/ray-service-readme","docId":"eks-blueprints/inference/ray-service/ray-service-readme","unlisted":false}]}],"href":"/docs/category/inference"}],"href":"/docs/category/eks-blueprints"},{"type":"category","label":"SLURM Blueprints","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Training","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"ddp","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"PyTorch DDP on CPU","href":"/docs/slurm-blueprints/training/ddp/distributed-data-parallel","docId":"slurm-blueprints/training/ddp/distributed-data-parallel","unlisted":false}]},{"type":"category","label":"fsdp","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Fully Sharded Data Parallel","href":"/docs/slurm-blueprints/training/fsdp/fully-sharded-data-parallel","docId":"slurm-blueprints/training/fsdp/fully-sharded-data-parallel","unlisted":false}]},{"type":"category","label":"megatron-lm","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"NVIDIA Megatron-LM","href":"/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme","docId":"slurm-blueprints/training/megatron-lm/megatron-lm-readme","unlisted":false}]},{"type":"category","label":"trainium","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Llama-3 70B (trn1.32xlarge) using NxD","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Setting up the software stack","href":"/docs/slurm-blueprints/training/trainium/Llama3-70B/prep","docId":"slurm-blueprints/training/trainium/Llama3-70B/prep","unlisted":false},{"type":"link","label":"Downloading the Llama3-70b model","href":"/docs/slurm-blueprints/training/trainium/Llama3-70B/download-model","docId":"slurm-blueprints/training/trainium/Llama3-70B/download-model","unlisted":false},{"type":"link","label":"Downloading the Wiki-corpus datasets","href":"/docs/slurm-blueprints/training/trainium/Llama3-70B/download-dataset","docId":"slurm-blueprints/training/trainium/Llama3-70B/download-dataset","unlisted":false},{"type":"link","label":"Running Continual Pre-training with NeuronX Distributed","href":"/docs/slurm-blueprints/training/trainium/Llama3-70B/training","docId":"slurm-blueprints/training/trainium/Llama3-70B/training","unlisted":false}],"href":"/docs/slurm-blueprints/training/trainium/Llama3-70B/"}]}],"href":"/docs/category/training-1"},{"type":"category","label":"Fine Tuning","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"peft","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"LoRA - Tranium","href":"/docs/slurm-blueprints/fine-tuning/peft/lora/","docId":"slurm-blueprints/fine-tuning/peft/lora/lora","unlisted":false},{"type":"link","label":"QLoRA (Quantized LoRA) Fine-tuning","href":"/docs/slurm-blueprints/fine-tuning/peft/qlora/","docId":"slurm-blueprints/fine-tuning/peft/qlora/qlora","unlisted":false}]},{"type":"category","label":"preference-aligment","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"dpo","href":"/docs/slurm-blueprints/fine-tuning/preference-aligment/dpo","docId":"slurm-blueprints/fine-tuning/preference-aligment/dpo","unlisted":false},{"type":"link","label":"ppo","href":"/docs/slurm-blueprints/fine-tuning/preference-aligment/ppo","docId":"slurm-blueprints/fine-tuning/preference-aligment/ppo","unlisted":false},{"type":"link","label":"grpo","href":"/docs/slurm-blueprints/fine-tuning/preference-aligment/grpo","docId":"slurm-blueprints/fine-tuning/preference-aligment/grpo","unlisted":false}]}],"href":"/docs/category/fine-tuning-1"},{"type":"category","label":"Inference","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"aws-inferentia","href":"/docs/slurm-blueprints/inference/aws-inferentia","docId":"slurm-blueprints/inference/aws-inferentia","unlisted":false},{"type":"category","label":"iInference-operator","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"amazon-s3-and-amazon-fsx","href":"/docs/slurm-blueprints/inference/iInference-operator/amazon-s3-and-amazon-fsx","docId":"slurm-blueprints/inference/iInference-operator/amazon-s3-and-amazon-fsx","unlisted":false},{"type":"link","label":"sagemaker-jumpstart","href":"/docs/slurm-blueprints/inference/iInference-operator/sagemaker-jumpstart","docId":"slurm-blueprints/inference/iInference-operator/sagemaker-jumpstart","unlisted":false}]}],"href":"/docs/category/inference-1"}],"href":"/docs/category/slurm-blueprints"},{"type":"category","label":"Sagemaker Hyperpod Recipes","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"SageMaker HyperPod Recipes Overview","href":"/docs/hyperpod-recipes/index.en","docId":"hyperpod-recipes/index.en","unlisted":false},{"type":"category","label":"eks-hyperpod-recipes","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Setup and Launch training - EKS","href":"/docs/hyperpod-recipes/eks-hyperpod-recipes/hyperpod-recipes","docId":"hyperpod-recipes/eks-hyperpod-recipes/hyperpod-recipes","unlisted":false}]},{"type":"category","label":"slurm-hyperpod-recipes","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Setup and Launch training - Slurm","href":"/docs/hyperpod-recipes/slurm-hyperpod-recipes/setup-and-train","docId":"hyperpod-recipes/slurm-hyperpod-recipes/setup-and-train","unlisted":false}]}],"href":"/docs/category/sagemaker-hyperpod-recipes"},{"type":"category","label":"Add-Ons","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Installing the Hyperpod CLI","href":"/docs/add-ons/installing-the-hyperpod-cli","docId":"add-ons/installing-the-hyperpod-cli","unlisted":false},{"type":"category","label":"Task Governance","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Task Governance","href":"/docs/add-ons/Task Governance/Task Governance for Training","docId":"add-ons/Task Governance/Task Governance for Training","unlisted":false}],"href":"/docs/category/task-governance"},{"type":"category","label":"Observability","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Observability Overview","href":"/docs/add-ons/Observability/","docId":"add-ons/Observability/Observability","unlisted":false},{"type":"link","label":"One-Click Observability with Amazon Managed Grafana and Amazon Managed Prometheus","href":"/docs/add-ons/Observability/Prometheus and Grafana/","docId":"add-ons/Observability/Prometheus and Grafana/Prometheus and Grafana","unlisted":false},{"type":"link","label":"Amazon CloudWatch Container Insights","href":"/docs/add-ons/Observability/Container Insights/","docId":"add-ons/Observability/Container Insights/Container Insights","unlisted":false},{"type":"link","label":"SageMaker Managed MLflow","href":"/docs/add-ons/Observability/MLFlow/","docId":"add-ons/Observability/MLFlow/MLFlow","unlisted":false},{"type":"link","label":"Weights & Biases","href":"/docs/add-ons/Observability/Weights and Biases/","docId":"add-ons/Observability/Weights and Biases/Weights and Biases","unlisted":false}],"href":"/docs/category/observability"},{"type":"category","label":"HyperPod Training Operator","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"HyperPod Training Operator Overview","href":"/docs/add-ons/hp-training-operator/overview","docId":"add-ons/hp-training-operator/overview","unlisted":false},{"type":"link","label":"Installation and Usage Guide","href":"/docs/add-ons/hp-training-operator/installation-and-usage","docId":"add-ons/hp-training-operator/installation-and-usage","unlisted":false}],"href":"/docs/category/hyperpod-training-operator"},{"type":"category","label":"Integrations","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Ray","href":"/docs/add-ons/integrations/Ray/","docId":"add-ons/integrations/Ray/Ray","unlisted":false},{"type":"link","label":"SkyPilot","href":"/docs/add-ons/integrations/skypilot/","docId":"add-ons/integrations/skypilot/SkyPilot","unlisted":false}],"href":"/docs/category/integrations"},{"type":"category","label":"Scripts","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Cluster Validation","href":"/docs/add-ons/Scripts/Cluster Validation/","docId":"add-ons/Scripts/Cluster Validation/Cluster Validation","unlisted":false},{"type":"link","label":"Monitoring","href":"/docs/add-ons/Scripts/Monitoring/","docId":"add-ons/Scripts/Monitoring/Monitoring","unlisted":false},{"type":"category","label":"NCCL and CUDA validation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Troubleshoot NCCL and CUDA","href":"/docs/add-ons/Scripts/NCCL and CUDA validation/Troubleshoot NCCL and CUDA","docId":"add-ons/Scripts/NCCL and CUDA validation/Troubleshoot NCCL and CUDA","unlisted":false}]},{"type":"link","label":"Performance Testing","href":"/docs/add-ons/Scripts/Performance Testing/","docId":"add-ons/Scripts/Performance Testing/Performance Testing","unlisted":false}]},{"type":"category","label":"Utilities","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"FinOps","href":"/docs/add-ons/Utilities/FinOps/","docId":"add-ons/Utilities/FinOps/FinOps","unlisted":false},{"type":"link","label":"Log Analysis","href":"/docs/add-ons/Utilities/Log Analysis/","docId":"add-ons/Utilities/Log Analysis/Log Analysis","unlisted":false},{"type":"link","label":"Resource Monitoring","href":"/docs/add-ons/Utilities/Resource Monitoring/","docId":"add-ons/Utilities/Resource Monitoring/Resource Monitoring","unlisted":false}]}],"href":"/docs/category/add-ons"},{"type":"category","label":"Dashboards","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Amazon CloudWatch","href":"/docs/dashboards/Amazon CloudWatch/","docId":"dashboards/Amazon CloudWatch/Amazon CloudWatch","unlisted":false},{"type":"link","label":"Managed Grafana","href":"/docs/dashboards/Managed Grafana/","docId":"dashboards/Managed Grafana/Managed Grafana","unlisted":false}],"href":"/docs/category/dashboards"},{"type":"category","label":"Validation and Testing","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Resiliency","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Resiliency Overview","href":"/docs/validation-and-testing/resiliency/overview","docId":"validation-and-testing/resiliency/overview","unlisted":false},{"type":"link","label":"Testing Resiliency with HyperPod EKS","href":"/docs/validation-and-testing/resiliency/eks-resiliency","docId":"validation-and-testing/resiliency/eks-resiliency","unlisted":false},{"type":"link","label":"Testing Resiliency with HyperPod Slurm","href":"/docs/validation-and-testing/resiliency/slurm-resiliency","docId":"validation-and-testing/resiliency/slurm-resiliency","unlisted":false}],"href":"/docs/category/resiliency"},{"type":"link","label":"architecture","href":"/docs/validation-and-testing/architecture","docId":"validation-and-testing/architecture","unlisted":false}],"href":"/docs/category/validation-and-testing"},{"type":"category","label":"Infrastructure as a Code","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Terraform deployment","href":"/docs/infrastructure-as-a-code/terraform/","docId":"infrastructure-as-a-code/terraform/terraform","unlisted":false},{"type":"link","label":"amazon-cloudformation","href":"/docs/infrastructure-as-a-code/amazon-cloudFormation/","docId":"infrastructure-as-a-code/amazon-cloudFormation/amazon-cloudformation","unlisted":false}],"href":"/docs/category/infrastructure-as-a-code"}]},"docs":{"add-ons/hp-training-operator/installation-and-usage":{"id":"add-ons/hp-training-operator/installation-and-usage","title":"Installation and Usage Guide","description":"This guide covers the installation of the HyperPod training operator and provides examples for running distributed training jobs using examples from the awsome-distributed-training repository.","sidebar":"tutorialSidebar"},"add-ons/hp-training-operator/overview":{"id":"add-ons/hp-training-operator/overview","title":"HyperPod Training Operator Overview","description":"The Amazon SageMaker HyperPod training operator helps you accelerate generative AI model development by efficiently managing distributed training across large GPU clusters. It introduces intelligent fault recovery, hang job detection, and process-level management capabilities that minimize training disruptions and reduce costs.","sidebar":"tutorialSidebar"},"add-ons/installing-the-hyperpod-cli":{"id":"add-ons/installing-the-hyperpod-cli","title":"Installing the Hyperpod CLI","description":"This tool is currently under maintenance. Some features may not work as expected. Please check the official repository for the latest updates.","sidebar":"tutorialSidebar"},"add-ons/integrations/Ray/Ray":{"id":"add-ons/integrations/Ray/Ray","title":"Ray","description":"","sidebar":"tutorialSidebar"},"add-ons/integrations/skypilot/SkyPilot":{"id":"add-ons/integrations/skypilot/SkyPilot","title":"SkyPilot","description":"Setup SkyPilot","sidebar":"tutorialSidebar"},"add-ons/Observability/Container Insights/Container Insights":{"id":"add-ons/Observability/Container Insights/Container Insights","title":"Amazon CloudWatch Container Insights","description":"CloudWatch Container Insights can be used to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices.  Container Insights is available for Amazon Elastic Kubernetes Service (Amazon EKS) and helps collect metrics from cluster deployed on EKS.","sidebar":"tutorialSidebar"},"add-ons/Observability/MLFlow/MLFlow":{"id":"add-ons/Observability/MLFlow/MLFlow","title":"SageMaker Managed MLflow","description":"Amazon SageMaker offers a managed MLflow capability for machine learning (ML) and generative AI experimentation. This capability makes it easy for data scientists to use MLflow on SageMaker for model training, registration, and deployment. Admins can quickly set up secure and scalable MLflow environments on AWS. Data scientists and ML developers can efficiently track ML experiments and find the right model for a business problem.","sidebar":"tutorialSidebar"},"add-ons/Observability/Observability":{"id":"add-ons/Observability/Observability","title":"Observability Overview","description":"Overview","sidebar":"tutorialSidebar"},"add-ons/Observability/Prometheus and Grafana/Prometheus and Grafana":{"id":"add-ons/Observability/Prometheus and Grafana/Prometheus and Grafana","title":"One-Click Observability with Amazon Managed Grafana and Amazon Managed Prometheus","description":"Amazon SageMaker HyperPod (SageMaker HyperPod) provides a comprehensive, out-of-the-box dashboard that gives you insights into foundation model (FM) development tasks and cluster resources.","sidebar":"tutorialSidebar"},"add-ons/Observability/Weights and Biases/Weights and Biases":{"id":"add-ons/Observability/Weights and Biases/Weights and Biases","title":"Weights & Biases","description":"This section is under development. W&B for observability documentation will be available soon.","sidebar":"tutorialSidebar"},"add-ons/Scripts/Cluster Validation/Cluster Validation":{"id":"add-ons/Scripts/Cluster Validation/Cluster Validation","title":"Cluster Validation","description":"","sidebar":"tutorialSidebar"},"add-ons/Scripts/Monitoring/Monitoring":{"id":"add-ons/Scripts/Monitoring/Monitoring","title":"Monitoring","description":"","sidebar":"tutorialSidebar"},"add-ons/Scripts/NCCL and CUDA validation/Troubleshoot NCCL and CUDA":{"id":"add-ons/Scripts/NCCL and CUDA validation/Troubleshoot NCCL and CUDA","title":"Troubleshoot NCCL and CUDA","description":"There are moments where you are stuck either because things are not working or the performnace is not what you expected. Most, not always, it will be an issue with libraries and drivers. For GPU-based workloads, those issues can show up more frequently as there are many bits and pieces that need to be working together. A simple mismatch of a library version or not-optimized driver version for that specific librabry version can break things.","sidebar":"tutorialSidebar"},"add-ons/Scripts/Performance Testing/Performance Testing":{"id":"add-ons/Scripts/Performance Testing/Performance Testing","title":"Performance Testing","description":"","sidebar":"tutorialSidebar"},"add-ons/Task Governance/Task Governance for Training":{"id":"add-ons/Task Governance/Task Governance for Training","title":"Task Governance","description":"SageMaker HyperPod task governance is a management system designed to streamline resource allocation and ensure efficient utilization of compute resources across teams and projects for your Amazon EKS clusters. It provides administrators with the capability to set priority levels for various tasks, allocate compute resources for each team, determine how idle compute is borrowed and lent between teams, and configure whether a team can preempt its own tasks.","sidebar":"tutorialSidebar"},"add-ons/Utilities/FinOps/FinOps":{"id":"add-ons/Utilities/FinOps/FinOps","title":"FinOps","description":"","sidebar":"tutorialSidebar"},"add-ons/Utilities/Log Analysis/Log Analysis":{"id":"add-ons/Utilities/Log Analysis/Log Analysis","title":"Log Analysis","description":"","sidebar":"tutorialSidebar"},"add-ons/Utilities/Resource Monitoring/Resource Monitoring":{"id":"add-ons/Utilities/Resource Monitoring/Resource Monitoring","title":"Resource Monitoring","description":"","sidebar":"tutorialSidebar"},"dashboards/Amazon CloudWatch/Amazon CloudWatch":{"id":"dashboards/Amazon CloudWatch/Amazon CloudWatch","title":"Amazon CloudWatch","description":"","sidebar":"tutorialSidebar"},"dashboards/Managed Grafana/Managed Grafana":{"id":"dashboards/Managed Grafana/Managed Grafana","title":"Managed Grafana","description":"","sidebar":"tutorialSidebar"},"eks-blueprints/fine-tuning/peft/low-rank-adaptation":{"id":"eks-blueprints/fine-tuning/peft/low-rank-adaptation","title":"LoRA - Trainium","description":"This example showcases how to train llama 3.1 models using AWS Trainium instances and Huggingface Optimum Neuron. \ud83e\udd17 Optimum Neuron is the interface between the \ud83e\udd17 Transformers library and AWS Accelerators including AWS Trainium and AWS Inferentia. It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks.","sidebar":"tutorialSidebar"},"eks-blueprints/fine-tuning/peft/quantisized-lora":{"id":"eks-blueprints/fine-tuning/peft/quantisized-lora","title":"QLoRA (Quantized LoRA)","description":"This section is under development. QLoRA fine-tuning documentation will be available soon.","sidebar":"tutorialSidebar"},"eks-blueprints/fine-tuning/preference-alignment/dpo":{"id":"eks-blueprints/fine-tuning/preference-alignment/dpo","title":"dpo","description":"This section is under development. DPO fine-tuning documentation will be available soon.","sidebar":"tutorialSidebar"},"eks-blueprints/fine-tuning/preference-alignment/grpo":{"id":"eks-blueprints/fine-tuning/preference-alignment/grpo","title":"grpo","description":"This section is under development. GRPO fine-tuning documentation will be available soon.","sidebar":"tutorialSidebar"},"eks-blueprints/fine-tuning/preference-alignment/ppo":{"id":"eks-blueprints/fine-tuning/preference-alignment/ppo","title":"ppo","description":"This section is under development. PPO fine-tuning documentation will be available soon.","sidebar":"tutorialSidebar"},"eks-blueprints/inference/inference-operator/amazon-s3-and-amazon-fsx":{"id":"eks-blueprints/inference/inference-operator/amazon-s3-and-amazon-fsx","title":"Deploying model from S3 or FSX","description":"You can deploy model artifacts directly from S3 or FSX to your HyperPod cluster using the InferenceEndpointConfig resource. The inference operator will use the S3 CSI driver to provide the model files to the pods in the cluster. Using this configuration, the operator will download the files located under the prefix deepseek15b as set by the modelLocation parameter.","sidebar":"tutorialSidebar"},"eks-blueprints/inference/inference-operator/sagemaker-jumpstart":{"id":"eks-blueprints/inference/inference-operator/sagemaker-jumpstart","title":"SageMaker JumpStart","description":"Amazon SageMaker JumpStart provides pretrained, open-source models for a wide range of problem types to help you get started with machine learning. You can incrementally train and tune these models before deployment. JumpStart also provides solution templates that set up infrastructure for common use cases, and executable example notebooks for machine learning with SageMaker AI.","sidebar":"tutorialSidebar"},"eks-blueprints/inference/load-balancer-inference/inference-with-loadbalancer":{"id":"eks-blueprints/inference/load-balancer-inference/inference-with-loadbalancer","title":"Mistral 7B Inference with Load Balancer","description":"This guide demonstrates how to deploy Mistral 7B for inference using Hugging Face\'s Text Generation Inference (TGI) container and expose it through an AWS Load Balancer on SageMaker HyperPod EKS.","sidebar":"tutorialSidebar"},"eks-blueprints/inference/ray-service/ray-service-readme":{"id":"eks-blueprints/inference/ray-service/ray-service-readme","title":"Serving Stable Diffusion Model for Inference with Ray Serve","description":"Ray Serve is a scalable model serving library for building online inference APIs. Serve is framework-agnostic, so you can use a single toolkit to serve everything from deep learning models built with frameworks like PyTorch, TensorFlow, and Keras, to Scikit-Learn models, to arbitrary Python business logic.","sidebar":"tutorialSidebar"},"eks-blueprints/training/ddp/distributed-data-parallel":{"id":"eks-blueprints/training/ddp/distributed-data-parallel","title":"Data Distributed Parallelism (DDP)","description":"Setup your training job image","sidebar":"tutorialSidebar"},"eks-blueprints/training/fsdp/fully-sharded-data-parallel":{"id":"eks-blueprints/training/fsdp/fully-sharded-data-parallel","title":"Fully Sharded Data Parallelism (FSDP)","description":"This example showcases an easy way to get started with multi node FSDP training on Amazon EKS on SageMaker HyperPod. It is designed to be as simple as possible, requires no data preparation, and uses a docker image.","sidebar":"tutorialSidebar"},"eks-blueprints/training/megatron-lm/megatron-lm-readme":{"id":"eks-blueprints/training/megatron-lm/megatron-lm-readme","title":"NVIDIA Megatron-LM","description":"MegatronLM is a framework from Nvidia designed for training large language models (LLMs). We recommend reading the following papers to understand the various tuning options available:","sidebar":"tutorialSidebar"},"eks-blueprints/training/ray-train/ray-train-readme":{"id":"eks-blueprints/training/ray-train/ray-train-readme","title":"Ray Train","description":"Ray is an open-source distributed computing framework designed to run highly scalable and parallel Python applications. Ray manages, executes, and optimizes compute needs across AI workloads. It unifies infrastructure via a single, flexible framework\u2014enabling any AI workload from data processing to model training to model serving and beyond.","sidebar":"tutorialSidebar"},"eks-blueprints/training/trainium/aws-trainium":{"id":"eks-blueprints/training/trainium/aws-trainium","title":"AWS Trainium","description":"In this section, we showcase how to pre-train Llama3.1-8B, Llama3 8B model using Trn1.32xlarge/Trn1n.32xlarge instances using the Neuron Distributed library. To train the LLama model in this example, we will apply the following optimizations using the Neuron Distributed library:","sidebar":"tutorialSidebar"},"getting-started/install-pre-requisites":{"id":"getting-started/install-pre-requisites","title":"Installing the required tools","description":"Before getting started with SageMaker HyperPod, we will configure our environment with the required tools.","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/Adding a Data Repository Assocation":{"id":"getting-started/orchestrated-by-eks/Adding a Data Repository Assocation","title":"Adding a Data Repository Association","description":"Amazon S3 Data Repository Association","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/additional-information":{"id":"getting-started/orchestrated-by-eks/additional-information","title":"Additional Information","description":"What Environment Variables are Set?","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/initial-cluster-setup":{"id":"getting-started/orchestrated-by-eks/initial-cluster-setup","title":"New cluster creation experience","description":"Initial cluster setup","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/Reviewing the cluster console":{"id":"getting-started/orchestrated-by-eks/Reviewing the cluster console","title":"Reviewing the cluster console","description":":image[SageMaker Logo]{src=\\"/img/01-cluster/sagemaker_logo.png\\" width=128}","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/Set up an Amazon S3 mountpoint":{"id":"getting-started/orchestrated-by-eks/Set up an Amazon S3 mountpoint","title":"Set up an Amazon S3 mountpoint","description":"With the Mountpoint for Amazon S3 Container Storage Interface (CSI) driver, your Kubernetes applications can access Amazon S3 objects through a file system interface, achieving high aggregate throughput without changing any application code. Built on Mountpoint for Amazon S3, the CSI driver presents an Amazon S3 bucket as a volume that can be accessed by containers in Amazon EKS and self-managed Kubernetes clusters. This section shows you how to deploy the Mountpoint for Amazon S3 CSI driver to your Amazon EKS cluster.","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/Set up your shared file system":{"id":"getting-started/orchestrated-by-eks/Set up your shared file system","title":"Set up your shared file system","description":"Install the Amazon FSx for Lustre CSI Driver","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/Verifying cluster connection to EKS":{"id":"getting-started/orchestrated-by-eks/Verifying cluster connection to EKS","title":"Verifying cluster connection to EKS","description":"Source HyperPod Environment Variables","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-slurm/initial-cluster-setup":{"id":"getting-started/orchestrated-by-slurm/initial-cluster-setup","title":"New cluster creation experience","description":"SageMaker HyperPod now provides a new cluster creation experience that sets up all the resources needed for large-scale AI/ML workloads, including, networking, storage, compute, and IAM permissions in just a few clicks. The new cluster creation experience for SageMaker HyperPod introduces dual quick and custom setup paths that simplify getting started for both beginners and advanced AWS customers.","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-slurm/slurm-basics":{"id":"getting-started/orchestrated-by-slurm/slurm-basics","title":"Basic Slurm Commands","description":"Now that you\'ve created and set up the cluster, you will go through some of the commands you\'ll use to run Llama7b on the cluster.","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-slurm/ssh-into-hyperpod":{"id":"getting-started/orchestrated-by-slurm/ssh-into-hyperpod","title":"SSH Into Your HyperPod Cluster","description":"Login to your cluster","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-slurm/View the AWS Console":{"id":"getting-started/orchestrated-by-slurm/View the AWS Console","title":"View the AWS Console","description":"Now that we\'ve created a cluster, we can monitor the status in the SageMaker console, this will show us cluster status, running instances, node groups, and allow us to easy modify the cluster.","sidebar":"tutorialSidebar"},"hyperpod-recipes/eks-hyperpod-recipes/hyperpod-recipes":{"id":"hyperpod-recipes/eks-hyperpod-recipes/hyperpod-recipes","title":"Setup and Launch training - EKS","description":"Prerequisites","sidebar":"tutorialSidebar"},"hyperpod-recipes/index.en":{"id":"hyperpod-recipes/index.en","title":"SageMaker HyperPod Recipes Overview","description":"Amazon SageMaker HyperPod recipes help you get started with training and fine-tuning publicly available foundation models. The recipes provide a pre-packaged set of training stack configurations that enable state-of-art training performance on SageMaker HyperPod. You can also easily switch between GPU-based instances and TRN-based instances with a simple recipe change.","sidebar":"tutorialSidebar"},"hyperpod-recipes/slurm-hyperpod-recipes/setup-and-train":{"id":"hyperpod-recipes/slurm-hyperpod-recipes/setup-and-train","title":"Setup and Launch training - Slurm","description":"Prerequisites","sidebar":"tutorialSidebar"},"infrastructure-as-a-code/amazon-cloudFormation/amazon-cloudformation":{"id":"infrastructure-as-a-code/amazon-cloudFormation/amazon-cloudformation","title":"amazon-cloudformation","description":"","sidebar":"tutorialSidebar"},"infrastructure-as-a-code/terraform/terraform":{"id":"infrastructure-as-a-code/terraform/terraform","title":"Terraform deployment","description":"","sidebar":"tutorialSidebar"},"Introduction":{"id":"Introduction","title":"Introduction","description":"\ud83d\udca1 Optimized Blueprints for deploying high performance clusters to train, fine tune, and host (inference) models on Amazon Sagemaker Hyperpod","sidebar":"tutorialSidebar"},"slurm-blueprints/fine-tuning/peft/lora/lora":{"id":"slurm-blueprints/fine-tuning/peft/lora/lora","title":"LoRA - Tranium","description":"This example showcases how to train Llama 3 models using AWS Trainium instances and \ud83e\udd17 Optimum Neuron. \ud83e\udd17 Optimum Neuron is the interface between the \ud83e\udd17 Transformers library and AWS Accelerators including AWS Trainium and AWS Inferentia. It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks.","sidebar":"tutorialSidebar"},"slurm-blueprints/fine-tuning/peft/qlora/qlora":{"id":"slurm-blueprints/fine-tuning/peft/qlora/qlora","title":"QLoRA (Quantized LoRA) Fine-tuning","description":"This section is under development. QLoRA fine-tuning documentation will be available soon.","sidebar":"tutorialSidebar"},"slurm-blueprints/fine-tuning/preference-aligment/dpo":{"id":"slurm-blueprints/fine-tuning/preference-aligment/dpo","title":"dpo","description":"This section is under development. DPO fine-tuning documentation will be available soon.","sidebar":"tutorialSidebar"},"slurm-blueprints/fine-tuning/preference-aligment/grpo":{"id":"slurm-blueprints/fine-tuning/preference-aligment/grpo","title":"grpo","description":"This section is under development. GRPO fine-tuning documentation will be available soon.","sidebar":"tutorialSidebar"},"slurm-blueprints/fine-tuning/preference-aligment/ppo":{"id":"slurm-blueprints/fine-tuning/preference-aligment/ppo","title":"ppo","description":"This section is under development. PPO fine-tuning documentation will be available soon.","sidebar":"tutorialSidebar"},"slurm-blueprints/inference/aws-inferentia":{"id":"slurm-blueprints/inference/aws-inferentia","title":"aws-inferentia","description":"","sidebar":"tutorialSidebar"},"slurm-blueprints/inference/iInference-operator/amazon-s3-and-amazon-fsx":{"id":"slurm-blueprints/inference/iInference-operator/amazon-s3-and-amazon-fsx","title":"amazon-s3-and-amazon-fsx","description":"","sidebar":"tutorialSidebar"},"slurm-blueprints/inference/iInference-operator/sagemaker-jumpstart":{"id":"slurm-blueprints/inference/iInference-operator/sagemaker-jumpstart","title":"sagemaker-jumpstart","description":"","sidebar":"tutorialSidebar"},"slurm-blueprints/training/ddp/distributed-data-parallel":{"id":"slurm-blueprints/training/ddp/distributed-data-parallel","title":"PyTorch DDP on CPU","description":"This example showcases CPU PyTorch DDP environment setup utilizing two different approaches for managing the software environment, Anaconda and Docker:","sidebar":"tutorialSidebar"},"slurm-blueprints/training/fsdp/fully-sharded-data-parallel":{"id":"slurm-blueprints/training/fsdp/fully-sharded-data-parallel","title":"Fully Sharded Data Parallel","description":"TODO: do we really need that?","sidebar":"tutorialSidebar"},"slurm-blueprints/training/megatron-lm/megatron-lm-readme":{"id":"slurm-blueprints/training/megatron-lm/megatron-lm-readme","title":"NVIDIA Megatron-LM","description":"MegatronLM is a framework from Nvidia that can be used to train LLMs. We recommend that you read papers on the framework to know the different knobs you can tune and in particular these articles:","sidebar":"tutorialSidebar"},"slurm-blueprints/training/trainium/Llama3-70B/download-dataset":{"id":"slurm-blueprints/training/trainium/Llama3-70B/download-dataset","title":"Downloading the Wiki-corpus datasets","description":"In this section, we will download and preprocess the wiki-corpus and tokenize it for training.","sidebar":"tutorialSidebar"},"slurm-blueprints/training/trainium/Llama3-70B/download-model":{"id":"slurm-blueprints/training/trainium/Llama3-70B/download-model","title":"Downloading the Llama3-70b model","description":"In this section, we will download the Llama3 model and the llama tokenizer. We will then also prepare the model for the Neuron runtime by converting the model weights to be pre-sharded based on the parallel processing configuration (i.e., the degrees of the model parallelism axes).","sidebar":"tutorialSidebar"},"slurm-blueprints/training/trainium/Llama3-70B/Llama3-70B":{"id":"slurm-blueprints/training/trainium/Llama3-70B/Llama3-70B","title":"Llama-3 70B (trn1.32xlarge) using NxD","description":"Llama","sidebar":"tutorialSidebar"},"slurm-blueprints/training/trainium/Llama3-70B/prep":{"id":"slurm-blueprints/training/trainium/Llama3-70B/prep","title":"Setting up the software stack","description":"Tranium","sidebar":"tutorialSidebar"},"slurm-blueprints/training/trainium/Llama3-70B/training":{"id":"slurm-blueprints/training/trainium/Llama3-70B/training","title":"Running Continual Pre-training with NeuronX Distributed","description":"Okay, now that we\'ve pre-processed the data and the model checkpoints, we are ready to submit a continual pre-training job. We have two sub-directories under /fsx/ubuntu/llama:","sidebar":"tutorialSidebar"},"validation-and-testing/architecture":{"id":"validation-and-testing/architecture","title":"architecture","description":"","sidebar":"tutorialSidebar"},"validation-and-testing/resiliency/eks-resiliency":{"id":"validation-and-testing/resiliency/eks-resiliency","title":"Testing Resiliency with HyperPod EKS","description":"This guide demonstrates how to test and validate the resiliency features of SageMaker HyperPod when using EKS as the orchestrator. You\'ll learn how to monitor node health, manually trigger node replacement/reboot, simulate failures, and test job auto-resume functionality.","sidebar":"tutorialSidebar"},"validation-and-testing/resiliency/overview":{"id":"validation-and-testing/resiliency/overview","title":"Resiliency Overview","description":"SageMaker HyperPod is built for resilient training with comprehensive health monitoring and automatic recovery capabilities. This section provides an overview of the resiliency features that apply to both HyperPod EKS and HyperPod Slurm orchestrators.","sidebar":"tutorialSidebar"},"validation-and-testing/resiliency/slurm-resiliency":{"id":"validation-and-testing/resiliency/slurm-resiliency","title":"Testing Resiliency with HyperPod Slurm","description":"This guide demonstrates how to test and validate the resiliency features of SageMaker HyperPod when using Slurm as the orchestrator. You\'ll learn how to submit resilient training jobs, inject failures, monitor cluster recovery, and manually replace nodes.","sidebar":"tutorialSidebar"}}}}')}}]);