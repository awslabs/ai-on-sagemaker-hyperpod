"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[7949],{3219:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>p,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"EKS Blueprints/Training/Hyperpod Recipes/Hyperpod Recipes","title":"Amazon Sagemaker Hyperpod Recipes","description":"Prerequisites","source":"@site/docs/1. EKS Blueprints/Training/Hyperpod Recipes/Hyperpod Recipes.md","sourceDirName":"1. EKS Blueprints/Training/Hyperpod Recipes","slug":"/EKS Blueprints/Training/Hyperpod Recipes/","permalink":"/docs/EKS Blueprints/Training/Hyperpod Recipes/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Amazon Sagemaker Hyperpod Recipes","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Fully Sharded Data Parallelism (FSDP)","permalink":"/docs/EKS Blueprints/Training/Fully Sharded Data Parallelism/"},"next":{"title":"Megatron-LM","permalink":"/docs/EKS Blueprints/Training/Megatron-LM/"}}');var a=r(4848),s=r(8453);const t={title:"Amazon Sagemaker Hyperpod Recipes",sidebar_position:1},l="Install and use the Hyperpod CLI",o={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Environment",id:"environment",level:2},{value:"Data",id:"data",level:2},{value:"Launch training",id:"launch-training",level:2},{value:"Using HyperPod Cli (Recommended)",id:"using-hyperpod-cli-recommended",level:3},{value:"Using recipes launcher",id:"using-recipes-launcher",level:3}];function d(e){const n={a:"a",code:"code",div:"div",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"install-and-use-the-hyperpod-cli",children:"Install and use the Hyperpod CLI"})}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["You'll need to create a cluster with Amazon EKS on SageMaker HyperPod. Instructions can be found in ",(0,a.jsx)(n.a,{href:"/docs/category/getting-started",children:"1. Cluster Setup"}),". Please follow them if you haven't done so already."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Dependencies - Make sure that you deployed GPU device plugin, EFA device plugin, and Kubeflow training operator to your cluster. See the ",(0,a.jsx)(n.a,{href:"/docs/category/getting-started",children:"install dependencies"})," page."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Fsx Lustre file system -  please follow the steps ",(0,a.jsx)(n.a,{href:"/docs/category/getting-started",children:"here"})," to create Fsx."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["HyperPod Cli (recommended not mandatory) - Please follow the installation instructions in the ",(0,a.jsx)(n.a,{href:"/docs/category/getting-started",children:"cluster setup"})," section."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"environment",children:"Environment"}),"\n",(0,a.jsx)(n.p,{children:"Set up the virtual environment. Make sure you're using Python 3.9 or greater."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"python3 -m venv ${PWD}/venv\nsource venv/bin/activate\n"})}),"\n",(0,a.jsx)(n.p,{children:"Download and setup HyperPod recipes repo."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"git clone --recursive https://github.com/aws/sagemaker-hyperpod-recipes.git\ncd sagemaker-hyperpod-recipes\npip3 install -r requirements.txt\n"})}),"\n",(0,a.jsx)(n.h2,{id:"data",children:"Data"}),"\n",(0,a.jsx)(n.p,{children:"HyperPod recipes support tokenized data in any of the below formats"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"JSON"}),"\n",(0,a.jsx)(n.li,{children:"JSONGZ (Compressed JSON)"}),"\n",(0,a.jsx)(n.li,{children:"ARROW"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"launch-training",children:"Launch training"}),"\n",(0,a.jsx)(n.h3,{id:"using-hyperpod-cli-recommended",children:"Using HyperPod Cli (Recommended)"}),"\n",(0,a.jsxs)(n.p,{children:["We recommend using the SageMaker HyperPod command-line interface (CLI) tool to submit your training job with your configurations. The following example submits a training job for pretraining llama 3 8b model. You can check the ",(0,a.jsx)(n.a,{href:"https://github.com/aws/sagemaker-hyperpod-recipes/blob/main/recipes_collection/recipes/training/llama/hf_llama3_8b_seq16k_gpu_p5x16_pretrain.yaml",children:"recipe config"})," for more details."]}),"\n",(0,a.jsx)(n.div,{children:"Replace the parameters in override-parameters section with actual values before submitting the job."}),"\n",(0,a.jsxs)(n.div,{children:["If you want to do a dry run without using actual data add the following parameter to the override-parameters section ",(0,a.jsx)(n.code,{children:'"recipes.model.data.use_synthetic_data":"true"'})," ."]}),"\n",(0,a.jsxs)(n.div,{children:["You can provide the HuggingFace token if you need pre-trained weights from HuggingFace by adding the following key-value pair to the override-parameters section ",(0,a.jsx)(n.code,{children:'"recipes.model.hf_access_token": "<your_hf_token>"'})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'hyperpod start-job --recipe training/llama/hf_llama3_8b_seq16k_gpu_p5x16_pretrain \\\n--persistent-volume-claims fsx-claim:data \\\n--override-parameters \\\n\'{\n "recipes.run.name": "hf-llama3-8b",\n "recipes.exp_manager.exp_dir": "/data/<your_exp_dir>",\n "container": "658645717510.dkr.ecr.<region>.amazonaws.com/smdistributed-modelparallel:2.4.1-gpu-py311-cu121",\n "recipes.model.data.train_dir": "<your_train_data_dir>",\n "recipes.model.data.val_dir": "<your_val_data_dir>",\n "cluster": "k8s",\n "cluster_type": "k8s"\n}\'\n'})}),"\n",(0,a.jsxs)(n.div,{children:["You can also use any of the available recipes listed in the ",(0,a.jsx)(n.a,{href:"https://github.com/aws/sagemaker-hyperpod-recipes/tree/main?tab=readme-ov-file#model-support",children:"HyperPod Recipes"})," Git repo by chaning the --recipe parameters"]}),"\n",(0,a.jsx)(n.p,{children:"After you\u2019ve submitted a training job, you can use the following command to verify if you submitted it successfully."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl get pods\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"NAME                             READY   STATUS             RESTARTS        AGE\nhf-llama3-<your-alias>-worker-0   0/1     running         0               36s\n"})}),"\n",(0,a.jsx)(n.p,{children:"After the job STATUS changes to Running, you can examine the log by using the following command."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl logs <name of pod>\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Once the job is completed the ",(0,a.jsx)(n.code,{children:"STATUS"})," of the pds will turn to Completed when you run kubectl get pods."]}),"\n",(0,a.jsx)(n.h3,{id:"using-recipes-launcher",children:"Using recipes launcher"}),"\n",(0,a.jsx)(n.p,{children:"Alternatively, you can use the SageMaker HyperPod recipes to submit your training job. Using the recipes involves updating k8s.yaml, config.yaml and running the launch script."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["In ",(0,a.jsx)(n.code,{children:"recipes_collection/cluster/k8s.yaml"}),", update persistent_volume_claims . It mounts the fsx claim to the /data directory of each computing pod"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-persistent_volume_claims:",children:"  - claimName: fsx-claim\n    mountPath: data\n"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["In ",(0,a.jsx)(n.code,{children:"recipes_collection/config.yaml"})," , update repo_url_or_path under git to use the Hyperpod-recipes git URL"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"git:\n  repo_url_or_path: <training_adapter_repo>\n  branch: null\n  commit: null\n  entry_script: null\n  token: null\n"})}),"\n",(0,a.jsx)(n.p,{children:"HyperPod recipes provides a launch script for each recipe under launcher_scripts directory. In order to pretrain llama 3.1 8b model,  update the launch scripts under launcher_scripts/llama/run_hf_llama3_8b_seq16k_gpu_p5x16_pretrain.sh"}),"\n",(0,a.jsx)(n.p,{children:"The launch script should look like below"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n#Users should setup their cluster type in /recipes_collection/config.yaml\nREGION="<region>"\nIMAGE="658645717510.dkr.ecr.${REGION}.amazonaws.com/smdistributed-modelparallel:2.4.1-gpu-py311-cu121"\nSAGEMAKER_TRAINING_LAUNCHER_DIR=${SAGEMAKER_TRAINING_LAUNCHER_DIR:-"$(pwd)"}\nEXP_DIR="<your_exp_dir>" # Location to save experiment info including logging, checkpoints, ect\nTRAIN_DIR="<your_training_data_dir>" # Location of training dataset\nVAL_DIR="<your_val_data_dir>" # Location of talidation dataset\n\nHYDRA_FULL_ERROR=1 python3 "${SAGEMAKER_TRAINING_LAUNCHER_DIR}/main.py" \\\n    recipes=training/llama/hf_llama3_8b_seq8k_gpu_p5x16_pretrain \\\n    base_results_dir="${SAGEMAKER_TRAINING_LAUNCHER_DIR}/results" \\\n    recipes.run.name="hf-llama3" \\\n    recipes.exp_manager.exp_dir="$EXP_DIR" \\\n    cluster=k8s \\\n    cluster_type=k8s \\\n    container="${IMAGE}" \\\n    recipes.model.data.train_dir=$TRAIN_DIR \\\n    recipes.model.data.val_dir=$VAL_DIR \n'})}),"\n",(0,a.jsx)(n.div,{children:"Replace the env variable in the script with actual values before submitting the job."}),"\n",(0,a.jsxs)(n.div,{children:["If you want to do a dry run without using actual data add the following parameter to the python command ",(0,a.jsx)(n.code,{children:"recipes.model.data.use_synthetic_data=true"})," ."]}),"\n",(0,a.jsxs)(n.div,{children:["You can provide the HuggingFace token if you need pre-trained weights from HuggingFace by adding the following key-value pair to the python command ",(0,a.jsx)(n.code,{children:"recipes.model.hf_access_token=<your_hf_token>"})]}),"\n",(0,a.jsx)(n.p,{children:"Once the script is ready you can launch the training job using below command"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"bash launcher_scripts/llama/run_hf_llama3_8b_seq16k_gpu_p5x16_pretrain.sh\n"})}),"\n",(0,a.jsx)(n.p,{children:"After you\u2019ve submitted a training job, you can use the following command to verify if you submitted it successfully."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl get pods\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"NAME                             READY   STATUS             RESTARTS        AGE\nhf-llama3-<your-alias>-worker-0   0/1     running         0               36s\n"})}),"\n",(0,a.jsx)(n.p,{children:"After the job STATUS changes to Running, you can examine the log by using the following command."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl logs <name of pod>\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Once the job is completed the ",(0,a.jsx)(n.code,{children:"STATUS"})," of the pods will turn to Completed when you run kubectl get pods."]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>l});var i=r(6540);const a={},s=i.createContext(a);function t(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);