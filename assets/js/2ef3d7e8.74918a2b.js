"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[4777],{34:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"slurm-blueprints/training/trainium/Llama3-70B/Llama3-70B","title":"Llama-3 70B (trn1.32xlarge) using NxD","description":"Llama","source":"@site/docs/02-slurm-blueprints/training/trainium/Llama3-70B/Llama3-70B.md","sourceDirName":"02-slurm-blueprints/training/trainium/Llama3-70B","slug":"/slurm-blueprints/training/trainium/Llama3-70B/","permalink":"/docs/slurm-blueprints/training/trainium/Llama3-70B/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Llama-3 70B (trn1.32xlarge) using NxD","sidebar_position":1,"weight":30},"sidebar":"tutorialSidebar","previous":{"title":"NVIDIA Megatron-LM","permalink":"/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme"},"next":{"title":"Setting up the software stack","permalink":"/docs/slurm-blueprints/training/trainium/Llama3-70B/prep"}}');var r=n(4848),s=n(8453);const a={title:"Llama-3 70B (trn1.32xlarge) using NxD",sidebar_position:1,weight:30},l=void 0,o={},c=[{value:"Prerequisutes",id:"prerequisutes",level:2},{value:"Parallelism",id:"parallelism",level:2},{value:"Compilation",id:"compilation",level:2},{value:"Steps",id:"steps",level:2}];function d(e){const i={a:"a",code:"code",em:"em",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.p,{children:(0,r.jsx)(i.img,{alt:"Llama",src:n(9e3).A+"",width:"1400",height:"787"})}),"\n",(0,r.jsx)(i.p,{children:"This tutorial demonstrates launching a Llama 3 70B training job on SageMaker HyperPod (cluster of 16 x ml.trn1.32xlarge instances)."}),"\n",(0,r.jsx)(i.h2,{id:"prerequisutes",children:"Prerequisutes"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["This guide assumes that you have a SMHP SLURM cluster of 16 x ml.trn1.32xlarge instances with a shared parallel filesystem like ",(0,r.jsx)(i.a,{href:"https://docs.aws.amazon.com/fsx/latest/LustreGuide/getting-started.html",children:"Amazon FSx for Lustre"}),". If you don't have this yet, please follow the instructions listed in ",(0,r.jsx)(i.a,{href:"/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup",children:"1. Cluster Setup"}),"."]}),"\n",(0,r.jsxs)(i.li,{children:["WLOG, we assume that you are operating from the home directory of the controller machine as user ",(0,r.jsx)(i.code,{children:"ubuntu"})," (default provisioned power user)."]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"parallelism",children:"Parallelism"}),"\n",(0,r.jsxs)(i.p,{children:["For this sample, we will use the Neuronx Distributed (NxD) package alongside the PyTorch Neuron package. ",(0,r.jsx)(i.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/training-torch-neuronx.html#training-torch-neuronx",children:"NeuronX Distributed"})," is a package used to support different distributed training frameworks and provides a mechanism for those frameworks to run on xla based Neuron cores. NxD supports a bunch of data and model parallelism strategies that we will look into below. ",(0,r.jsx)(i.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/training/pytorch-neuron-programming-guide.html#pytorch-neuronx",children:"PyTorch Neuron"})," is a software package that enables PyTorch training, evaluation, and inference on Neuron devices."]}),"\n",(0,r.jsx)(i.p,{children:'We will be utilizing 3D parallelism for this sample. 3D parallelism combines data parallelism with model (tensor + pipeline) parallelism into a cohesive framework, creating a 3-D mesh of devices. Each axis of this "mesh" corresponds to one of:'}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Data Parallelism Axis: Distributes training data across devices"}),"\n",(0,r.jsx)(i.li,{children:"Tensor Parallelism Axis: Parallelizes tensor layers' computations/calculations across devices"}),"\n",(0,r.jsx)(i.li,{children:"Pipeline Parallelism Axis: Distributes the model's layers across devices"}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:["This combination of data + model parallelism allows for efficient scaling and utilization of hardware resources. For instance, tensor parallelism requires the highest communication bandwidth and is best suited for Trainium chips within the same Trn1 node with strong NeuronLink interconnect. Pipeline parallelism, which has lower communication requirements, can be used across nodes. Data parallelism, which requires the least communication, can span across multiple nodes. To learn more about the Trainium Architecture, check the ",(0,r.jsx)(i.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/trainium.html",children:"Neuron Docs"}),"."]}),"\n",(0,r.jsx)(i.h2,{id:"compilation",children:"Compilation"}),"\n",(0,r.jsxs)(i.p,{children:["To get the best performance on Trainium, it's a good idea to compile the model before training. This can be done by running ",(0,r.jsx)(i.code,{children:"neuron_parallel_compile torchrun your_model.py"})," and setting the model to train for a few steps (5-10). This will build the graphs of the model, and store them in a cache so the next time you run ",(0,r.jsx)(i.code,{children:"torchrun your_model.py"})," training will start much faster."]}),"\n",(0,r.jsx)(i.h2,{id:"steps",children:"Steps"}),"\n",(0,r.jsx)(i.p,{children:"We will be doing the following:"}),"\n",(0,r.jsxs)(i.ol,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.em,{children:"Download the llama3 model and tokenizer:"})," Download the model weights (checkpoints) and the tokenizer. We will also convert the checkpoints based on the distributed training configuration"]}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.em,{children:"Download and preprocess the wiki-corpus dataset"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.em,{children:"Train with NxD!"})}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,s.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>l});var t=n(6540);const r={},s=t.createContext(r);function a(e){const i=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),t.createElement(s.Provider,{value:i},e.children)}},9e3:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/llama3-93c369217f9d09df49dc3719c31899fc.jpg"}}]);