"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[849],{6164:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Introduction","href":"/docs/Introduction","docId":"Introduction","unlisted":false},{"type":"category","label":"Getting Started","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Orchestrated by EKS","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"New cluster creation experience","href":"/docs/getting-started/orchestrated-by-eks/initial-cluster-setup","docId":"getting-started/orchestrated-by-eks/initial-cluster-setup","unlisted":false},{"type":"link","label":"Reviewing the cluster console","href":"/docs/getting-started/orchestrated-by-eks/Reviewing the cluster console","docId":"getting-started/orchestrated-by-eks/Reviewing the cluster console","unlisted":false},{"type":"link","label":"Verifying cluster connection to EKS","href":"/docs/getting-started/orchestrated-by-eks/Verifying cluster connection to EKS","docId":"getting-started/orchestrated-by-eks/Verifying cluster connection to EKS","unlisted":false},{"type":"link","label":"Set up your shared file system","href":"/docs/getting-started/orchestrated-by-eks/Set up your shared file system","docId":"getting-started/orchestrated-by-eks/Set up your shared file system","unlisted":false},{"type":"link","label":"Adding a Data Repository Association","href":"/docs/getting-started/orchestrated-by-eks/Adding a Data Repository Assocation","docId":"getting-started/orchestrated-by-eks/Adding a Data Repository Assocation","unlisted":false},{"type":"link","label":"Set up an Amazon S3 mountpoint","href":"/docs/getting-started/orchestrated-by-eks/Set up an Amazon S3 mountpoint","docId":"getting-started/orchestrated-by-eks/Set up an Amazon S3 mountpoint","unlisted":false}],"href":"/docs/category/orchestrated-by-eks"},{"type":"category","label":"Orchestrated by SLURM","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"New cluster creation experience","href":"/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup","docId":"getting-started/orchestrated-by-slurm/initial-cluster-setup","unlisted":false},{"type":"link","label":"View the AWS Console","href":"/docs/getting-started/orchestrated-by-slurm/View the AWS Console","docId":"getting-started/orchestrated-by-slurm/View the AWS Console","unlisted":false},{"type":"link","label":"SSH Into Your HyperPod Cluster","href":"/docs/getting-started/orchestrated-by-slurm/ssh-into-hyperpod","docId":"getting-started/orchestrated-by-slurm/ssh-into-hyperpod","unlisted":false},{"type":"link","label":"Basic Slurm Commands","href":"/docs/getting-started/orchestrated-by-slurm/slurm-basics","docId":"getting-started/orchestrated-by-slurm/slurm-basics","unlisted":false}],"href":"/docs/category/orchestrated-by-slurm"},{"type":"link","label":"Installing the required tools","href":"/docs/getting-started/install-pre-requisites","docId":"getting-started/install-pre-requisites","unlisted":false}],"href":"/docs/category/getting-started"},{"type":"category","label":"EKS Blueprints","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Training","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"ddp","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Data Distributed Parallelism (DDP)","href":"/docs/eks-blueprints/training/ddp/distributed-data-parallel","docId":"eks-blueprints/training/ddp/distributed-data-parallel","unlisted":false}]},{"type":"category","label":"fsdp","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Fully Sharded Data Parallelism (FSDP)","href":"/docs/eks-blueprints/training/fsdp/fully-sharded-data-parallel","docId":"eks-blueprints/training/fsdp/fully-sharded-data-parallel","unlisted":false}]},{"type":"category","label":"megatron-lm","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"NVIDIA Megatron-LM","href":"/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme","docId":"eks-blueprints/training/megatron-lm/megatron-lm-readme","unlisted":false}]},{"type":"category","label":"ray-train","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Ray Train","href":"/docs/eks-blueprints/training/ray-train/ray-train-readme","docId":"eks-blueprints/training/ray-train/ray-train-readme","unlisted":false}]},{"type":"category","label":"trainium","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"AWS Trainium","href":"/docs/eks-blueprints/training/trainium/aws-trainium","docId":"eks-blueprints/training/trainium/aws-trainium","unlisted":false}]}],"href":"/docs/category/training"},{"type":"category","label":"Fine Tuning","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"peft","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"lora","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"low-rank-adaptation","href":"/docs/eks-blueprints/fine-tuning/peft/lora/low-rank-adaptation","docId":"eks-blueprints/fine-tuning/peft/lora/low-rank-adaptation","unlisted":false}]},{"type":"category","label":"qlora","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"quantisized-lora","href":"/docs/eks-blueprints/fine-tuning/peft/qlora/quantisized-lora","docId":"eks-blueprints/fine-tuning/peft/qlora/quantisized-lora","unlisted":false}]}]},{"type":"category","label":"preference-alignment","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"dpo","href":"/docs/eks-blueprints/fine-tuning/preference-alignment/dpo","docId":"eks-blueprints/fine-tuning/preference-alignment/dpo","unlisted":false},{"type":"link","label":"ppo","href":"/docs/eks-blueprints/fine-tuning/preference-alignment/ppo","docId":"eks-blueprints/fine-tuning/preference-alignment/ppo","unlisted":false},{"type":"link","label":"grpo","href":"/docs/eks-blueprints/fine-tuning/preference-alignment/grpo","docId":"eks-blueprints/fine-tuning/preference-alignment/grpo","unlisted":false}]}],"href":"/docs/category/fine-tuning"},{"type":"category","label":"Inference","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"inference-operator","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"SageMaker JumpStart","href":"/docs/eks-blueprints/inference/inference-operator/sagemaker-jumpstart","docId":"eks-blueprints/inference/inference-operator/sagemaker-jumpstart","unlisted":false},{"type":"link","label":"Deploying model from S3 or FSX","href":"/docs/eks-blueprints/inference/inference-operator/amazon-s3-and-amazon-fsx","docId":"eks-blueprints/inference/inference-operator/amazon-s3-and-amazon-fsx","unlisted":false}]},{"type":"category","label":"load-balancer-inference","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Mistral 7B with Load Balancer","href":"/docs/eks-blueprints/inference/load-balancer-inference/inference-with-loadbalancer","docId":"eks-blueprints/inference/load-balancer-inference/inference-with-loadbalancer","unlisted":false}]}],"href":"/docs/category/inference"}],"href":"/docs/category/eks-blueprints"},{"type":"category","label":"SLURM Blueprints","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Training","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"WIP","href":"/docs/slurm-blueprints/training/ray-train/","docId":"slurm-blueprints/training/ray-train/ray-train","unlisted":false},{"type":"category","label":"ddp","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"PyTorch DDP on CPU","href":"/docs/slurm-blueprints/training/ddp/distributed-data-parallel","docId":"slurm-blueprints/training/ddp/distributed-data-parallel","unlisted":false}]},{"type":"category","label":"fsdp","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Fully Sharded Data Parallel","href":"/docs/slurm-blueprints/training/fsdp/fully-sharded-data-parallel","docId":"slurm-blueprints/training/fsdp/fully-sharded-data-parallel","unlisted":false}]},{"type":"category","label":"megatron-lm","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"NVIDIA Megatron-LM","href":"/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme","docId":"slurm-blueprints/training/megatron-lm/megatron-lm-readme","unlisted":false}]},{"type":"category","label":"trainium","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"aws-trainium","href":"/docs/slurm-blueprints/training/trainium/aws-trainium","docId":"slurm-blueprints/training/trainium/aws-trainium","unlisted":false}]}],"href":"/docs/category/training-1"},{"type":"category","label":"Fine Tuning","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"peft","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"lora","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"low-rank-adaptation","href":"/docs/slurm-blueprints/fine-tuning/peft/lora/low-rank-adaptation","docId":"slurm-blueprints/fine-tuning/peft/lora/low-rank-adaptation","unlisted":false}]},{"type":"category","label":"qlora","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"quantisized-lora","href":"/docs/slurm-blueprints/fine-tuning/peft/qlora/quantisized-lora","docId":"slurm-blueprints/fine-tuning/peft/qlora/quantisized-lora","unlisted":false}]}]},{"type":"category","label":"preference-aligment","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"dpo","href":"/docs/slurm-blueprints/fine-tuning/preference-aligment/dpo","docId":"slurm-blueprints/fine-tuning/preference-aligment/dpo","unlisted":false},{"type":"link","label":"ppo","href":"/docs/slurm-blueprints/fine-tuning/preference-aligment/ppo","docId":"slurm-blueprints/fine-tuning/preference-aligment/ppo","unlisted":false},{"type":"link","label":"grpo","href":"/docs/slurm-blueprints/fine-tuning/preference-aligment/grpo","docId":"slurm-blueprints/fine-tuning/preference-aligment/grpo","unlisted":false}]}],"href":"/docs/category/fine-tuning-1"},{"type":"category","label":"Inference","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"aws-inferentia","href":"/docs/slurm-blueprints/inference/aws-inferentia","docId":"slurm-blueprints/inference/aws-inferentia","unlisted":false},{"type":"category","label":"iInference-operator","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"amazon-s3-and-amazon-fsx","href":"/docs/slurm-blueprints/inference/iInference-operator/amazon-s3-and-amazon-fsx","docId":"slurm-blueprints/inference/iInference-operator/amazon-s3-and-amazon-fsx","unlisted":false},{"type":"link","label":"sagemaker-jumpstart","href":"/docs/slurm-blueprints/inference/iInference-operator/sagemaker-jumpstart","docId":"slurm-blueprints/inference/iInference-operator/sagemaker-jumpstart","unlisted":false}]}],"href":"/docs/category/inference-1"}],"href":"/docs/category/slurm-blueprints"},{"type":"category","label":"Sagemaker Hyperpod Recipes","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"SageMaker HyperPod Recipes Overview","href":"/docs/hyperpod-recipes/index.en","docId":"hyperpod-recipes/index.en","unlisted":false},{"type":"category","label":"eks-hyperpod-recipes","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Setup and Launch training - EKS","href":"/docs/hyperpod-recipes/eks-hyperpod-recipes/hyperpod-recipes","docId":"hyperpod-recipes/eks-hyperpod-recipes/hyperpod-recipes","unlisted":false}]},{"type":"category","label":"slurm-hyperpod-recipes","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Setup and Launch training - Slurm","href":"/docs/hyperpod-recipes/slurm-hyperpod-recipes/setup-and-train","docId":"hyperpod-recipes/slurm-hyperpod-recipes/setup-and-train","unlisted":false}]}],"href":"/docs/category/sagemaker-hyperpod-recipes"},{"type":"category","label":"Add-Ons","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Installing the Hyperpod CLI","href":"/docs/add-ons/installing-the-hyperpod-cli","docId":"add-ons/installing-the-hyperpod-cli","unlisted":false},{"type":"category","label":"Task Governance","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Task Governance","href":"/docs/add-ons/Task Governance/Task Governance for Training","docId":"add-ons/Task Governance/Task Governance for Training","unlisted":false}],"href":"/docs/category/task-governance"},{"type":"category","label":"Observability","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Observability Overview","href":"/docs/add-ons/Observability/","docId":"add-ons/Observability/Observability","unlisted":false},{"type":"link","label":"One-Click Observability with Amazon Managed Grafana and Amazon Managed Prometheus","href":"/docs/add-ons/Observability/Prometheus and Grafana/","docId":"add-ons/Observability/Prometheus and Grafana/Prometheus and Grafana","unlisted":false},{"type":"link","label":"Amazon CloudWatch Container Insights","href":"/docs/add-ons/Observability/Container Insights/","docId":"add-ons/Observability/Container Insights/Container Insights","unlisted":false},{"type":"link","label":"SageMaker Managed MLflow","href":"/docs/add-ons/Observability/MLFlow/","docId":"add-ons/Observability/MLFlow/MLFlow","unlisted":false},{"type":"link","label":"Weights and Biases","href":"/docs/add-ons/Observability/Weights and Biases/","docId":"add-ons/Observability/Weights and Biases/Weights and Biases","unlisted":false}],"href":"/docs/category/observability"},{"type":"category","label":"Integrations","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Ray","href":"/docs/add-ons/integrations/Ray/","docId":"add-ons/integrations/Ray/Ray","unlisted":false},{"type":"link","label":"SkyPilot","href":"/docs/add-ons/integrations/skypilot/","docId":"add-ons/integrations/skypilot/SkyPilot","unlisted":false}],"href":"/docs/category/integrations"},{"type":"category","label":"Scripts","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Cluster Validation","href":"/docs/add-ons/Scripts/Cluster Validation/","docId":"add-ons/Scripts/Cluster Validation/Cluster Validation","unlisted":false},{"type":"link","label":"Monitoring","href":"/docs/add-ons/Scripts/Monitoring/","docId":"add-ons/Scripts/Monitoring/Monitoring","unlisted":false},{"type":"category","label":"NCCL and CUDA validation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Troubleshoot NCCL and CUDA","href":"/docs/add-ons/Scripts/NCCL and CUDA validation/Troubleshoot NCCL and CUDA","docId":"add-ons/Scripts/NCCL and CUDA validation/Troubleshoot NCCL and CUDA","unlisted":false}]},{"type":"link","label":"Performance Testing","href":"/docs/add-ons/Scripts/Performance Testing/","docId":"add-ons/Scripts/Performance Testing/Performance Testing","unlisted":false}]},{"type":"category","label":"Utilities","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"FinOps","href":"/docs/add-ons/Utilities/FinOps/","docId":"add-ons/Utilities/FinOps/FinOps","unlisted":false},{"type":"link","label":"Log Analysis","href":"/docs/add-ons/Utilities/Log Analysis/","docId":"add-ons/Utilities/Log Analysis/Log Analysis","unlisted":false},{"type":"link","label":"Resource Monitoring","href":"/docs/add-ons/Utilities/Resource Monitoring/","docId":"add-ons/Utilities/Resource Monitoring/Resource Monitoring","unlisted":false}]}],"href":"/docs/category/add-ons"},{"type":"category","label":"Dashboards","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Amazon CloudWatch","href":"/docs/dashboards/Amazon CloudWatch/","docId":"dashboards/Amazon CloudWatch/Amazon CloudWatch","unlisted":false},{"type":"link","label":"Managed Grafana","href":"/docs/dashboards/Managed Grafana/","docId":"dashboards/Managed Grafana/Managed Grafana","unlisted":false}],"href":"/docs/category/dashboards"},{"type":"category","label":"Validation and Testing","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Resiliency","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"resiliency","href":"/docs/validation-and-testing/resiliency/","docId":"validation-and-testing/resiliency/resiliency","unlisted":false}],"href":"/docs/category/resiliency"},{"type":"link","label":"architecture","href":"/docs/validation-and-testing/architecture","docId":"validation-and-testing/architecture","unlisted":false}],"href":"/docs/category/validation-and-testing"},{"type":"category","label":"Infrastructure as a Code","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Terraform deployment","href":"/docs/infrastructure-as-a-code/terraform/","docId":"infrastructure-as-a-code/terraform/terraform","unlisted":false},{"type":"link","label":"amazon-cloudformation","href":"/docs/infrastructure-as-a-code/amazon-cloudFormation/","docId":"infrastructure-as-a-code/amazon-cloudFormation/amazon-cloudformation","unlisted":false}],"href":"/docs/category/infrastructure-as-a-code"}]},"docs":{"add-ons/installing-the-hyperpod-cli":{"id":"add-ons/installing-the-hyperpod-cli","title":"Installing the Hyperpod CLI","description":"The Amazon SageMaker HyperPod command-line interface (HyperPod CLI) is a tool that helps manage training jobs on the SageMaker HyperPod clusters orchestrated by Amazon EKS. With the HyperPod CLI, scientists can submit training jobs by providing a .yaml file and manage jobs (list, describe, view, cancel) without needing to use kubectl. It is essentially a wrapper on top of kubectl and the AWS CLI.","sidebar":"tutorialSidebar"},"add-ons/integrations/Ray/Ray":{"id":"add-ons/integrations/Ray/Ray","title":"Ray","description":"","sidebar":"tutorialSidebar"},"add-ons/integrations/skypilot/SkyPilot":{"id":"add-ons/integrations/skypilot/SkyPilot","title":"SkyPilot","description":"Setup SkyPilot","sidebar":"tutorialSidebar"},"add-ons/Observability/Container Insights/Container Insights":{"id":"add-ons/Observability/Container Insights/Container Insights","title":"Amazon CloudWatch Container Insights","description":"CloudWatch Container Insights can be used to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices.  Container Insights is available for Amazon Elastic Kubernetes Service (Amazon EKS) and helps collect metrics from cluster deployed on EKS.","sidebar":"tutorialSidebar"},"add-ons/Observability/MLFlow/MLFlow":{"id":"add-ons/Observability/MLFlow/MLFlow","title":"SageMaker Managed MLflow","description":"Amazon SageMaker offers a managed MLflow capability for machine learning (ML) and generative AI experimentation. This capability makes it easy for data scientists to use MLflow on SageMaker for model training, registration, and deployment. Admins can quickly set up secure and scalable MLflow environments on AWS. Data scientists and ML developers can efficiently track ML experiments and find the right model for a business problem.","sidebar":"tutorialSidebar"},"add-ons/Observability/Observability":{"id":"add-ons/Observability/Observability","title":"Observability Overview","description":"Overview","sidebar":"tutorialSidebar"},"add-ons/Observability/Prometheus and Grafana/Prometheus and Grafana":{"id":"add-ons/Observability/Prometheus and Grafana/Prometheus and Grafana","title":"One-Click Observability with Amazon Managed Grafana and Amazon Managed Prometheus","description":"Amazon SageMaker HyperPod (SageMaker HyperPod) provides a comprehensive, out-of-the-box dashboard that gives you insights into foundation model (FM) development tasks and cluster resources.","sidebar":"tutorialSidebar"},"add-ons/Observability/Weights and Biases/Weights and Biases":{"id":"add-ons/Observability/Weights and Biases/Weights and Biases","title":"Weights and Biases","description":"","sidebar":"tutorialSidebar"},"add-ons/Scripts/Cluster Validation/Cluster Validation":{"id":"add-ons/Scripts/Cluster Validation/Cluster Validation","title":"Cluster Validation","description":"","sidebar":"tutorialSidebar"},"add-ons/Scripts/Monitoring/Monitoring":{"id":"add-ons/Scripts/Monitoring/Monitoring","title":"Monitoring","description":"","sidebar":"tutorialSidebar"},"add-ons/Scripts/NCCL and CUDA validation/Troubleshoot NCCL and CUDA":{"id":"add-ons/Scripts/NCCL and CUDA validation/Troubleshoot NCCL and CUDA","title":"Troubleshoot NCCL and CUDA","description":"There are moments where you are stuck either because things are not working or the performnace is not what you expected. Most, not always, it will be an issue with libraries and drivers. For GPU-based workloads, those issues can show up more frequently as there are many bits and pieces that need to be working together. A simple mismatch of a library version or not-optimized driver version for that specific librabry version can break things.","sidebar":"tutorialSidebar"},"add-ons/Scripts/Performance Testing/Performance Testing":{"id":"add-ons/Scripts/Performance Testing/Performance Testing","title":"Performance Testing","description":"","sidebar":"tutorialSidebar"},"add-ons/Task Governance/Task Governance for Training":{"id":"add-ons/Task Governance/Task Governance for Training","title":"Task Governance","description":"SageMaker HyperPod task governance is a management system designed to streamline resource allocation and ensure efficient utilization of compute resources across teams and projects for your Amazon EKS clusters. It provides administrators with the capability to set priority levels for various tasks, allocate compute resources for each team, determine how idle compute is borrowed and lent between teams, and configure whether a team can preempt its own tasks.","sidebar":"tutorialSidebar"},"add-ons/Utilities/FinOps/FinOps":{"id":"add-ons/Utilities/FinOps/FinOps","title":"FinOps","description":"","sidebar":"tutorialSidebar"},"add-ons/Utilities/Log Analysis/Log Analysis":{"id":"add-ons/Utilities/Log Analysis/Log Analysis","title":"Log Analysis","description":"","sidebar":"tutorialSidebar"},"add-ons/Utilities/Resource Monitoring/Resource Monitoring":{"id":"add-ons/Utilities/Resource Monitoring/Resource Monitoring","title":"Resource Monitoring","description":"","sidebar":"tutorialSidebar"},"dashboards/Amazon CloudWatch/Amazon CloudWatch":{"id":"dashboards/Amazon CloudWatch/Amazon CloudWatch","title":"Amazon CloudWatch","description":"","sidebar":"tutorialSidebar"},"dashboards/Managed Grafana/Managed Grafana":{"id":"dashboards/Managed Grafana/Managed Grafana","title":"Managed Grafana","description":"","sidebar":"tutorialSidebar"},"eks-blueprints/fine-tuning/peft/lora/low-rank-adaptation":{"id":"eks-blueprints/fine-tuning/peft/lora/low-rank-adaptation","title":"low-rank-adaptation","description":"","sidebar":"tutorialSidebar"},"eks-blueprints/fine-tuning/peft/qlora/quantisized-lora":{"id":"eks-blueprints/fine-tuning/peft/qlora/quantisized-lora","title":"quantisized-lora","description":"","sidebar":"tutorialSidebar"},"eks-blueprints/fine-tuning/preference-alignment/dpo":{"id":"eks-blueprints/fine-tuning/preference-alignment/dpo","title":"dpo","description":"","sidebar":"tutorialSidebar"},"eks-blueprints/fine-tuning/preference-alignment/grpo":{"id":"eks-blueprints/fine-tuning/preference-alignment/grpo","title":"grpo","description":"","sidebar":"tutorialSidebar"},"eks-blueprints/fine-tuning/preference-alignment/ppo":{"id":"eks-blueprints/fine-tuning/preference-alignment/ppo","title":"ppo","description":"","sidebar":"tutorialSidebar"},"eks-blueprints/inference/inference-operator/amazon-s3-and-amazon-fsx":{"id":"eks-blueprints/inference/inference-operator/amazon-s3-and-amazon-fsx","title":"Deploying model from S3 or FSX","description":"You can deploy model artifacts directly from S3 or FSX to your HyperPod cluster using the InferenceEndpointConfig resource. The inference operator will use the S3 CSI driver to provide the model files to the pods in the cluster. Using this configuration, the operator will download the files located under the prefix deepseek15b as set by the modelLocation parameter.","sidebar":"tutorialSidebar"},"eks-blueprints/inference/inference-operator/sagemaker-jumpstart":{"id":"eks-blueprints/inference/inference-operator/sagemaker-jumpstart","title":"SageMaker JumpStart","description":"Amazon SageMaker JumpStart provides pretrained, open-source models for a wide range of problem types to help you get started with machine learning. You can incrementally train and tune these models before deployment. JumpStart also provides solution templates that set up infrastructure for common use cases, and executable example notebooks for machine learning with SageMaker AI.","sidebar":"tutorialSidebar"},"eks-blueprints/inference/load-balancer-inference/inference-with-loadbalancer":{"id":"eks-blueprints/inference/load-balancer-inference/inference-with-loadbalancer","title":"Mistral 7B Inference with Load Balancer","description":"This guide demonstrates how to deploy Mistral 7B for inference using Hugging Face\'s Text Generation Inference (TGI) container and expose it through an AWS Load Balancer on SageMaker HyperPod EKS.","sidebar":"tutorialSidebar"},"eks-blueprints/training/ddp/distributed-data-parallel":{"id":"eks-blueprints/training/ddp/distributed-data-parallel","title":"Data Distributed Parallelism (DDP)","description":"Setup your training job image","sidebar":"tutorialSidebar"},"eks-blueprints/training/fsdp/fully-sharded-data-parallel":{"id":"eks-blueprints/training/fsdp/fully-sharded-data-parallel","title":"Fully Sharded Data Parallelism (FSDP)","description":"This example showcases an easy way to get started with multi node FSDP training on Amazon EKS on SageMaker HyperPod. It is designed to be as simple as possible, requires no data preparation, and uses a docker image.","sidebar":"tutorialSidebar"},"eks-blueprints/training/megatron-lm/megatron-lm-readme":{"id":"eks-blueprints/training/megatron-lm/megatron-lm-readme","title":"NVIDIA Megatron-LM","description":"MegatronLM is a framework from Nvidia designed for training large language models (LLMs). We recommend reading the following papers to understand the various tuning options available:","sidebar":"tutorialSidebar"},"eks-blueprints/training/ray-train/ray-train-readme":{"id":"eks-blueprints/training/ray-train/ray-train-readme","title":"Ray Train","description":"Ray is an open-source distributed computing framework designed to run highly scalable and parallel Python applications. Ray manages, executes, and optimizes compute needs across AI workloads. It unifies infrastructure via a single, flexible framework\u2014enabling any AI workload from data processing to model training to model serving and beyond.","sidebar":"tutorialSidebar"},"eks-blueprints/training/trainium/aws-trainium":{"id":"eks-blueprints/training/trainium/aws-trainium","title":"AWS Trainium","description":"In this section, we showcase how to pre-train Llama3.1-8B, Llama3 8B model using Trn1.32xlarge/Trn1n.32xlarge instances using the Neuron Distributed library. To train the LLama model in this example, we will apply the following optimizations using the Neuron Distributed library:","sidebar":"tutorialSidebar"},"getting-started/install-pre-requisites":{"id":"getting-started/install-pre-requisites","title":"Installing the required tools","description":"Before getting started with SageMaker HyperPod, we will configure our environment with the required tools.","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/Adding a Data Repository Assocation":{"id":"getting-started/orchestrated-by-eks/Adding a Data Repository Assocation","title":"Adding a Data Repository Association","description":"Amazon S3 Data Repository Association","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/initial-cluster-setup":{"id":"getting-started/orchestrated-by-eks/initial-cluster-setup","title":"New cluster creation experience","description":"Initial cluster setup","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/Reviewing the cluster console":{"id":"getting-started/orchestrated-by-eks/Reviewing the cluster console","title":"Reviewing the cluster console","description":":image[SageMaker Logo]{src=\\"/img/01-cluster/sagemaker_logo.png\\" width=128}","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/Set up an Amazon S3 mountpoint":{"id":"getting-started/orchestrated-by-eks/Set up an Amazon S3 mountpoint","title":"Set up an Amazon S3 mountpoint","description":"With the Mountpoint for Amazon S3 Container Storage Interface (CSI) driver, your Kubernetes applications can access Amazon S3 objects through a file system interface, achieving high aggregate throughput without changing any application code. Built on Mountpoint for Amazon S3, the CSI driver presents an Amazon S3 bucket as a volume that can be accessed by containers in Amazon EKS and self-managed Kubernetes clusters. This section shows you how to deploy the Mountpoint for Amazon S3 CSI driver to your Amazon EKS cluster.","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/Set up your shared file system":{"id":"getting-started/orchestrated-by-eks/Set up your shared file system","title":"Set up your shared file system","description":"Install the Amazon FSx for Lustre CSI Driver","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-eks/Verifying cluster connection to EKS":{"id":"getting-started/orchestrated-by-eks/Verifying cluster connection to EKS","title":"Verifying cluster connection to EKS","description":"Verify kubectl Access","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-slurm/initial-cluster-setup":{"id":"getting-started/orchestrated-by-slurm/initial-cluster-setup","title":"New cluster creation experience","description":"SageMaker HyperPod now provides a new cluster creation experience that sets up all the resources needed for large-scale AI/ML workloads, including, networking, storage, compute, and IAM permissions in just a few clicks. The new cluster creation experience for SageMaker HyperPod introduces dual quick and custom setup paths that simplify getting started for both beginners and advanced AWS customers.","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-slurm/slurm-basics":{"id":"getting-started/orchestrated-by-slurm/slurm-basics","title":"Basic Slurm Commands","description":"Now that you\'ve created and set up the cluster, you will go through some of the commands you\'ll use to run Llama7b on the cluster.","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-slurm/ssh-into-hyperpod":{"id":"getting-started/orchestrated-by-slurm/ssh-into-hyperpod","title":"SSH Into Your HyperPod Cluster","description":"Login to your cluster","sidebar":"tutorialSidebar"},"getting-started/orchestrated-by-slurm/View the AWS Console":{"id":"getting-started/orchestrated-by-slurm/View the AWS Console","title":"View the AWS Console","description":"Now that we\'ve created a cluster, we can monitor the status in the SageMaker console, this will show us cluster status, running instances, node groups, and allow us to easy modify the cluster.","sidebar":"tutorialSidebar"},"hyperpod-recipes/eks-hyperpod-recipes/hyperpod-recipes":{"id":"hyperpod-recipes/eks-hyperpod-recipes/hyperpod-recipes","title":"Setup and Launch training - EKS","description":"Prerequisites","sidebar":"tutorialSidebar"},"hyperpod-recipes/index.en":{"id":"hyperpod-recipes/index.en","title":"SageMaker HyperPod Recipes Overview","description":"Amazon SageMaker HyperPod recipes help you get started with training and fine-tuning publicly available foundation models. The recipes provide a pre-packaged set of training stack configurations that enable state-of-art training performance on SageMaker HyperPod. You can also easily switch between GPU-based instances and TRN-based instances with a simple recipe change.","sidebar":"tutorialSidebar"},"hyperpod-recipes/slurm-hyperpod-recipes/setup-and-train":{"id":"hyperpod-recipes/slurm-hyperpod-recipes/setup-and-train","title":"Setup and Launch training - Slurm","description":"Prerequisites","sidebar":"tutorialSidebar"},"infrastructure-as-a-code/amazon-cloudFormation/amazon-cloudformation":{"id":"infrastructure-as-a-code/amazon-cloudFormation/amazon-cloudformation","title":"amazon-cloudformation","description":"","sidebar":"tutorialSidebar"},"infrastructure-as-a-code/terraform/terraform":{"id":"infrastructure-as-a-code/terraform/terraform","title":"Terraform deployment","description":"","sidebar":"tutorialSidebar"},"Introduction":{"id":"Introduction","title":"Introduction","description":"\ud83d\udca1 Optimized Blueprints for deploying high performance clusters to train, fine tune, and host (inference) models on Amazon Sagemaker Hyperpod","sidebar":"tutorialSidebar"},"slurm-blueprints/fine-tuning/peft/lora/low-rank-adaptation":{"id":"slurm-blueprints/fine-tuning/peft/lora/low-rank-adaptation","title":"low-rank-adaptation","description":"","sidebar":"tutorialSidebar"},"slurm-blueprints/fine-tuning/peft/qlora/quantisized-lora":{"id":"slurm-blueprints/fine-tuning/peft/qlora/quantisized-lora","title":"quantisized-lora","description":"","sidebar":"tutorialSidebar"},"slurm-blueprints/fine-tuning/preference-aligment/dpo":{"id":"slurm-blueprints/fine-tuning/preference-aligment/dpo","title":"dpo","description":"","sidebar":"tutorialSidebar"},"slurm-blueprints/fine-tuning/preference-aligment/grpo":{"id":"slurm-blueprints/fine-tuning/preference-aligment/grpo","title":"grpo","description":"","sidebar":"tutorialSidebar"},"slurm-blueprints/fine-tuning/preference-aligment/ppo":{"id":"slurm-blueprints/fine-tuning/preference-aligment/ppo","title":"ppo","description":"","sidebar":"tutorialSidebar"},"slurm-blueprints/inference/aws-inferentia":{"id":"slurm-blueprints/inference/aws-inferentia","title":"aws-inferentia","description":"","sidebar":"tutorialSidebar"},"slurm-blueprints/inference/iInference-operator/amazon-s3-and-amazon-fsx":{"id":"slurm-blueprints/inference/iInference-operator/amazon-s3-and-amazon-fsx","title":"amazon-s3-and-amazon-fsx","description":"","sidebar":"tutorialSidebar"},"slurm-blueprints/inference/iInference-operator/sagemaker-jumpstart":{"id":"slurm-blueprints/inference/iInference-operator/sagemaker-jumpstart","title":"sagemaker-jumpstart","description":"","sidebar":"tutorialSidebar"},"slurm-blueprints/training/ddp/distributed-data-parallel":{"id":"slurm-blueprints/training/ddp/distributed-data-parallel","title":"PyTorch DDP on CPU","description":"This example showcases CPU PyTorch DDP environment setup utilizing two different approaches for managing the software environment, Anaconda and Docker:","sidebar":"tutorialSidebar"},"slurm-blueprints/training/fsdp/fully-sharded-data-parallel":{"id":"slurm-blueprints/training/fsdp/fully-sharded-data-parallel","title":"Fully Sharded Data Parallel","description":"TODO: do we really need that?","sidebar":"tutorialSidebar"},"slurm-blueprints/training/megatron-lm/megatron-lm-readme":{"id":"slurm-blueprints/training/megatron-lm/megatron-lm-readme","title":"NVIDIA Megatron-LM","description":"MegatronLM is a framework from Nvidia that can be used to train LLMs. We recommend that you read papers on the framework to know the different knobs you can tune and in particular these articles:","sidebar":"tutorialSidebar"},"slurm-blueprints/training/ray-train/ray-train":{"id":"slurm-blueprints/training/ray-train/ray-train","title":"WIP","description":"","sidebar":"tutorialSidebar"},"slurm-blueprints/training/trainium/aws-trainium":{"id":"slurm-blueprints/training/trainium/aws-trainium","title":"aws-trainium","description":"","sidebar":"tutorialSidebar"},"validation-and-testing/architecture":{"id":"validation-and-testing/architecture","title":"architecture","description":"","sidebar":"tutorialSidebar"},"validation-and-testing/resiliency/resiliency":{"id":"validation-and-testing/resiliency/resiliency","title":"resiliency","description":"","sidebar":"tutorialSidebar"}}}}')}}]);