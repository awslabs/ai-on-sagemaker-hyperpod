"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[4536],{5585:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"eks-blueprints/fine-tuning/peft/low-rank-adaptation","title":"LoRA - Trainium","description":"This example showcases how to train llama 3.1 models using AWS Trainium instances and Huggingface Optimum Neuron. \ud83e\udd17 Optimum Neuron is the interface between the \ud83e\udd17 Transformers library and AWS Accelerators including AWS Trainium and AWS Inferentia. It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks.","source":"@site/docs/01-eks-blueprints/fine-tuning/peft/low-rank-adaptation.md","sourceDirName":"01-eks-blueprints/fine-tuning/peft","slug":"/eks-blueprints/fine-tuning/peft/low-rank-adaptation","permalink":"/ai-on-sagemaker-hyperpod/docs/eks-blueprints/fine-tuning/peft/low-rank-adaptation","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"LoRA - Trainium","sidebar_position":2,"weight":3},"sidebar":"tutorialSidebar","previous":{"title":"Fine Tuning","permalink":"/ai-on-sagemaker-hyperpod/docs/category/fine-tuning"},"next":{"title":"QLoRA (Quantized LoRA)","permalink":"/ai-on-sagemaker-hyperpod/docs/eks-blueprints/fine-tuning/peft/quantisized-lora"}}');var s=i(4848),a=i(8453);const r={title:"LoRA - Trainium",sidebar_position:2,weight:3},o=void 0,l={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Verified instance types, instance counts",id:"verified-instance-types-instance-counts",level:2},{value:"Validate the cluster configuration",id:"validate-the-cluster-configuration",level:2},{value:"Create and mount the FSx Lustre File System to the SageMaker HyperPod",id:"create-and-mount-the-fsx-lustre-file-system-to-the-sagemaker-hyperpod",level:2},{value:"Apply Low-Rank Adaptation (LoRA) Finetune Llama 3.1 8B model with Optimum Neuron using SageMaker HyperPod",id:"apply-low-rank-adaptation-lora-finetune-llama-31-8b-model-with-optimum-neuron-using-sagemaker-hyperpod",level:2},{value:"Generate Job Spec Files for tokenization and training",id:"generate-job-spec-files-for-tokenization-and-training",level:2},{value:"Tokenize Data",id:"tokenize-data",level:2},{value:"Compile the model",id:"compile-the-model",level:2},{value:"Train Model",id:"train-model",level:2},{value:"Consolidation the trained weights",id:"consolidation-the-trained-weights",level:2},{value:"Merge LoRA weights",id:"merge-lora-weights",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",div:"div",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"This example showcases how to train llama 3.1 models using AWS Trainium instances and Huggingface Optimum Neuron. \ud83e\udd17 Optimum Neuron is the interface between the \ud83e\udd17 Transformers library and AWS Accelerators including AWS Trainium and AWS Inferentia. It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks."}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before running this training, you'll need to create a SageMaker HyperPod cluster with at least 1 trn1.32xlarge/ trn1n.32xlarge instance group."}),"\n",(0,s.jsx)(n.p,{children:"Please  make sure that you deploy Neuron device plugin, EFA device plugin, and Kubeflow training operator to your cluster."}),"\n",(0,s.jsxs)(n.p,{children:["See ",(0,s.jsx)(n.a,{href:"/docs/getting-started/install-pre-requisites",children:"What Dependencies are Installed on Your EKS Cluster"})," for details."]}),"\n",(0,s.jsx)(n.p,{children:"To build a container image, you need a x86-64 based development environment with Docker installed. If you use recent Mac with Apple Silicon, they are not x86-64 based but ARM based. You can use SageMaker Code Editor for this purpose."}),"\n",(0,s.jsxs)(n.p,{children:["Since ",(0,s.jsx)(n.a,{href:"https://huggingface.co/meta-llama/Meta-Llama-3.1-8B",children:"llama 3.1"})," is a gated model users have to register in Huggingface and obtain an HF_Access_Token before running this example."]}),"\n",(0,s.jsxs)(n.p,{children:["We need to setup an PVC for FSx to store the tokenized data and training checkpoints. Please follow the link ",(0,s.jsx)(n.a,{href:"/docs/getting-started/orchestrated-by-eks/Set%20up%20your%20shared%20file%20system",children:"here"})," to setup FSx CSI Driver and PVC."]}),"\n",(0,s.jsx)(n.h2,{id:"verified-instance-types-instance-counts",children:"Verified instance types, instance counts"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"ml.trn1.32xlarge x (1,2)"}),"\n",(0,s.jsx)(n.li,{children:"ml.trn1n.32xlarge x (1,2)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"validate-the-cluster-configuration",children:"Validate the cluster configuration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["View the AWS Console following this ",(0,s.jsx)(n.a,{href:"/docs/getting-started/orchestrated-by-eks/Reviewing%20the%20cluster%20console",children:"instruction"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Set environment variables. This is done in ",(0,s.jsx)(n.a,{href:"/docs/getting-started/orchestrated-by-eks/Verifying%20cluster%20connection%20to%20EKS",children:"Verifying cluster connection to EKS"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"create-and-mount-the-fsx-lustre-file-system-to-the-sagemaker-hyperpod",children:"Create and mount the FSx Lustre File System to the SageMaker HyperPod"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["First install the FSx for Lustre CSI driver following this ",(0,s.jsx)(n.a,{href:"/docs/getting-started/orchestrated-by-eks/Set%20up%20your%20shared%20file%20system",children:"instruction"}),", and we will use dynamic provisioning"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Create a persistent volume claim that uses the ",(0,s.jsx)(n.code,{children:"fsx-claim"})," storage claim with namespace ",(0,s.jsx)(n.code,{children:"kubeflow"}),":"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"cat <<EOF> pvc.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: fsx-claim\n  namespace: kubeflow\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: fsx-sc\n  resources:\n    requests:\n      storage: 1200Gi\nEOF\n\nkubectl apply -f pvc.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:"This persistent volume claim will kick off the dynamic provisioning of an FSx for Lustre file system based on the specifications provided in the storage class."}),"\n",(0,s.jsx)(n.p,{children:"View the status of the persistent volume claim:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"kubectl describe pvc fsx-claim -n kubeflow\n"})}),"\n",(0,s.jsx)(n.p,{children:"Mount the volume to container"}),"\n",(0,s.jsxs)(n.p,{children:["After the pvc status is set to ",(0,s.jsx)(n.code,{children:"Bound"})," before deploying any pods that reference the persistent volume claim. The status may remain in a Pending state ( ~10 mins) while the file system is being provisioned.\nUse the following command to mount the volume."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:'cat <<EOF> pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: fsx-app\n  namespace: kubeflow\nspec:\n  containers:\n  - name: app\n    image: ubuntu\n    command: ["/bin/sh"]\n    args: ["-c", "while true; do echo $(date -u) >> /data/out.txt; sleep 5; done"]\n    volumeMounts:\n    - name: persistent-storage\n      mountPath: /data\n  volumes:\n  - name: persistent-storage\n    persistentVolumeClaim:\n      claimName: fsx-claim\nEOF\n\nkubectl apply -f pod.yaml\n'})}),"\n",(0,s.jsx)(n.h2,{id:"apply-low-rank-adaptation-lora-finetune-llama-31-8b-model-with-optimum-neuron-using-sagemaker-hyperpod",children:"Apply Low-Rank Adaptation (LoRA) Finetune Llama 3.1 8B model with Optimum Neuron using SageMaker HyperPod"}),"\n",(0,s.jsx)(n.p,{children:"In this section, we showcase how to finetune Llama3.1-8B, Llama3 8B model using Trn1.32xlarge/Trn1n.32xlarge instances using the Optimum Neuron library. To finetune the LLama model in this example, we will apply the following optimization techniques:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/tensor_parallelism_overview.html#tensor-parallelism-overview",children:"Tensor Parallelism"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/activation_memory_reduction.html#sequence-parallelism",children:"Sequence Parallel"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/activation_memory_reduction.html#activation-memory-reduction",children:"Selective checkpointing"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/lora_finetune_developer_guide.html",children:"Lora Finetuning"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Login to ECR and pull the ",(0,s.jsx)(n.code,{children:"huggingface-pytorch-training-neuronx"})," image"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"region=us-east-1\ndlc_account_id=763104351884\naws ecr get-login-password --region $region | docker login --username AWS --password-stdin $dlc_account_id.dkr.ecr.$region.amazonaws.com\n\ndocker pull ${dlc_account_id}.dkr.ecr.${region}.amazonaws.com/huggingface-pytorch-training-neuronx:2.1.2-transformers4.43.2-neuronx-py310-sdk2.20.0-ubuntu20.04-v1.0\n"})}),"\n",(0,s.jsx)(n.p,{children:"On your x86-64 based development environment:"}),"\n",(0,s.jsx)(n.p,{children:"Navigate to your home directory or your preferred project directory, clone the repo."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~\ngit clone https://github.com/Captainia/awsome-distributed-training.git\ngit checkout optimum-neuron-eks\ncd 3.test_cases/pytorch/optimum-neuron/llama3/kubernetes/fine-tuning\n"})}),"\n",(0,s.jsx)(n.p,{children:"Build Docker Image and push to ECR"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"export AWS_REGION=$(aws ec2 describe-availability-zones --output text --query 'AvailabilityZones[0].[RegionName]')\nexport ACCOUNT=$(aws sts get-caller-identity --query Account --output text)\nexport REGISTRY=${ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com/\nexport IMAGE=peft-optimum-neuron\nexport TAG=:latest\ndocker build --network=sagemaker -t ${REGISTRY}${IMAGE}${TAG} .\n"})}),"\n",(0,s.jsxs)(n.p,{children:["We will build docker image using the ",(0,s.jsx)(n.a,{href:"https://github.com/Captainia/awsome-distributed-training/blob/optimum-neuron-eks/3.test_cases/pytorch/optimum-neuron/llama3/kubernetes/fine-tuning/Dockerfile",children:"Dockerfile"})," in this directory."]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.div,{children:(0,s.jsxs)(n.p,{children:["The environment variable",(0,s.jsx)(n.code,{children:"$DOCKER_NETWORK"})," is set to ",(0,s.jsx)(n.code,{children:"--network=sagemaker"})," only if you deployed the SageMaker Studio Code Editor. This is necessary because SageMaker Studio uses a specific network configuration for its containers. Otherwise, it remains unset."]})})}),"\n",(0,s.jsx)(n.p,{children:"Then push the image to your private registry"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:'# Create registry if needed\nexport REGISTRY_COUNT=$(aws ecr describe-repositories | grep \\"${IMAGE}\\" | wc -l)\nif [ "${REGISTRY_COUNT//[!0-9]/}" == "0" ]; then\n    echo "Creating repository ${REGISTRY}${IMAGE} ..."\n    aws ecr create-repository --repository-name ${IMAGE}\nelse\n    echo "Repository ${REGISTRY}${IMAGE} already exists"\nfi\n\n# Login to registry\necho "Logging in to $REGISTRY ..."\naws ecr get-login-password | docker login --username AWS --password-stdin $REGISTRY\n\n# Push image to registry\ndocker image push ${REGISTRY}${IMAGE}${TAG}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"generate-job-spec-files-for-tokenization-and-training",children:"Generate Job Spec Files for tokenization and training"}),"\n",(0,s.jsx)(n.p,{children:"The default config in the script launches a 8B Llama 3.1 model. When you run the generate-jobspec.sh script it creates 2 yaml files tokenize_data.yaml and llama3_train.yaml"}),"\n",(0,s.jsx)(n.p,{children:"You will have to update the HF_ACCESS_TOKEN in order for the tokenization to work."}),"\n",(0,s.jsxs)(n.p,{children:["Please edit the ",(0,s.jsx)(n.code,{children:"./generate-jobspec.sh"})," script with your desired environment settings."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"./generate-jobspec.sh\n"})}),"\n",(0,s.jsx)(n.h2,{id:"tokenize-data",children:"Tokenize Data"}),"\n",(0,s.jsxs)(n.p,{children:["The example uses ",(0,s.jsx)(n.a,{href:"https://huggingface.co/datasets/gboleda/wikicorpus",children:"wikicorpus"})," dataset from Hugginface Hub. The tokenize_data.yaml job downloads the dataset and tokenizes it. Finally store the dataset in Fsx Lustre which can be used for training the model."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f ./tokenize_data.yaml\n"})}),"\n",(0,s.jsx)(n.h2,{id:"compile-the-model",children:"Compile the model"}),"\n",(0,s.jsx)(n.p,{children:"Training on Trainium requires model compilation using the neuron_parallel_compile utility."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"kubectl apply -f ./compile_peft.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:"This step does the following:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Extracts computation graphs from a trial run (~10 training steps)"}),"\n",(0,s.jsx)(n.li,{children:"Performs parallel pre-compilation of these graphs"}),"\n",(0,s.jsx)(n.li,{children:"Uses identical scripts to actual training but with reduced max_steps"}),"\n",(0,s.jsx)(n.li,{children:"Prepares the model for efficient execution on Trainium hardware"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The compilation process is essential for optimizing model performance on the specialized Trainium architecture."}),"\n",(0,s.jsx)(n.h2,{id:"train-model",children:"Train Model"}),"\n",(0,s.jsx)(n.p,{children:"The launch_peft_train.yaml job spec file finetunes llama 3.1 8B model with the tokenized data from previous step. By default the code uses 1 trn1.32xlarge but can be changed to any number of nodes."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f ./launch_peft_train.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:"The training process uses tensor parallelism with degree 8 and leverages all 32 NeuronCores in the ml.trn1.32xlarge instance. Key features include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Data parallel degree of 4"}),"\n",(0,s.jsx)(n.li,{children:"BFloat16 precision (XLA_USE_BF16=1) for reduced memory footprint"}),"\n",(0,s.jsx)(n.li,{children:"Gradient accumulation steps of 3 for larger effective batch size"}),"\n",(0,s.jsx)(n.li,{children:"LoRA configuration with:\nr=16 (rank)\nlora_alpha=16\nlora_dropout=0.05\nTarget modules: q_proj and v_proj"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"consolidation-the-trained-weights",children:"Consolidation the trained weights"}),"\n",(0,s.jsx)(n.p,{children:"During distributed training, model checkpoints are split across multiple devices. The consolidation process:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Combines distributed checkpoints into a unified model"}),"\n",(0,s.jsx)(n.li,{children:"Processes tensors in memory-efficient chunks"}),"\n",(0,s.jsx)(n.li,{children:"Creates sharded outputs with an index file"}),"\n",(0,s.jsx)(n.li,{children:"Saves the consolidated weights in safetensor format"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This step is crucial for bringing together the distributed training results into a usable format."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"kubectl apply -f ./consolidation.yaml\n"})}),"\n",(0,s.jsx)(n.h2,{id:"merge-lora-weights",children:"Merge LoRA weights"}),"\n",(0,s.jsx)(n.p,{children:"The final step merges the LoRA adapters with the base model."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"kubectl apply -f ./merge_lora.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:"This process does the following:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Loads the base model and LoRA configuration"}),"\n",(0,s.jsx)(n.li,{children:"Transforms LoRA weight names to match base model structure"}),"\n",(0,s.jsx)(n.li,{children:"Merges the adapters with the original model weights"}),"\n",(0,s.jsx)(n.li,{children:"Saves the final model in a sharded format"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The resulting merged model combines the base model's knowledge with the task-specific adaptations learned during fine-tuning, while maintaining the efficiency benefits of LoRA training."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var t=i(6540);const s={},a=t.createContext(s);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);