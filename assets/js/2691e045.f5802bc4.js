"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[7403],{6164:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>c,contentTitle:()=>d,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>a});const o=JSON.parse('{"id":"Tips/Slurm/gres","title":"Gres (--gpus)","description":"This section describes how to setup Slurm Gres which allows scheduling jobs based on the number of gpu\'s needed i.e. --gpus=4. Please see the below note before proceeding with the setup:","source":"@site/docs/08-Tips/Slurm/08-gres.md","sourceDirName":"08-Tips/Slurm","slug":"/Tips/Slurm/gres","permalink":"/ai-on-sagemaker-hyperpod/docs/Tips/Slurm/gres","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"title":"Gres (--gpus)","weight":48},"sidebar":"tutorialSidebar","previous":{"title":"Login Node","permalink":"/ai-on-sagemaker-hyperpod/docs/Tips/Slurm/login-node"},"next":{"title":"Diagnose GPU Failures","permalink":"/ai-on-sagemaker-hyperpod/docs/Tips/Slurm/diagnose-gpus"}}');var r=n(4848),t=n(8453);const i={title:"Gres (--gpus)",weight:48},d=void 0,c={},a=[];function l(e){const s={a:"a",admonition:"admonition",code:"code",p:"p",pre:"pre",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(s.p,{children:["This section describes how to setup ",(0,r.jsx)(s.a,{href:"https://slurm.schedmd.com/gres.html",children:"Slurm Gres"})," which allows scheduling jobs based on the number of gpu's needed i.e. ",(0,r.jsx)(s.code,{children:"--gpus=4"}),". Please see the below note before proceeding with the setup:"]}),"\n",(0,r.jsx)(s.admonition,{type:"warning",children:(0,r.jsxs)(s.p,{children:["If you enable Gres support and your cluster uses an AMI older than the ",(0,r.jsx)(s.a,{href:"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-release-notes.html#sagemaker-hyperpod-release-notes-20240820",children:"August 20, 2024 release"}),", the auto-resume feature doesn't work. This is because instances need to be scheduled with ",(0,r.jsx)(s.code,{children:"--nodes"})," and ",(0,r.jsx)(s.code,{children:"--exclusive"})," flags for automatic instance replacement to function. To enable Gres and auto-resume, update your cluster to the latest AMI."]})}),"\n",(0,r.jsxs)(s.p,{children:["If you see an error ",(0,r.jsx)(s.code,{children:"sbatch: error: Invalid generic resource (gres) specification"}),", you need to set ",(0,r.jsx)(s.code,{children:"GresType"})," in slurm.conf so it recognizes the parameter ",(0,r.jsx)(s.code,{children:"--gres=gpu:8"}),". To do that run the following:"]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:'printf "\\nGresTypes=gpu" | sudo tee -a /opt/slurm/etc/slurm.conf\n'})}),"\n",(0,r.jsxs)(s.p,{children:["Next remove the ",(0,r.jsx)(s.code,{children:"OverSubscribe=EXCLUSIVE"})," section on the line:"]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{children:"PartitionName=dev Nodes=ALL Default=YES MaxTime=INFINITE State=UP ~~OverSubscribe=EXCLUSIVE~~\n"})}),"\n",(0,r.jsxs)(s.p,{children:["Next edit the file ",(0,r.jsx)(s.code,{children:"/opt/slurm/etc/slurm.conf"})," and remove the ",(0,r.jsx)(s.code,{children:"file="})," part after ",(0,r.jsx)(s.code,{children:"Gres"})," (if it exists). Make sure the number of gpus corresponds to the number on the instance, i.e. 8 for p4d/p5."]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{children:"# change\nNodeName=ip-10-1-57-141 NodeHostname=ip-10-1-57-141 NodeAddr=10.1.57.141 CPUs=48 Gres=gpu:4,file=/dev/nvidia-[0-3] State=CLOUD\n# to\nNodeName=ip-10-1-57-141 NodeHostname=ip-10-1-57-141 NodeAddr=10.1.57.141 CPUs=48 Gres=gpu:4 State=CLOUD\n"})}),"\n",(0,r.jsxs)(s.p,{children:["Reconfigure and restart ",(0,r.jsx)(s.code,{children:"slurmctld"}),":"]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:"sudo systemctl restart slurmctld\nsudo scontrol reconfigure\nsudo systemctl restart slurmctld\n"})}),"\n",(0,r.jsxs)(s.p,{children:["Restart ",(0,r.jsx)(s.code,{children:"slurmd"})," daemons on the compute node, you will need to ssh in if the nodes show ",(0,r.jsx)(s.code,{children:"inval"})," or ",(0,r.jsx)(s.code,{children:"drain"}),". We've included a script that can be used to automate this below:"]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:"ssh ip-10-0-0-XX sudo systemctl restart slurmd\n"})}),"\n",(0,r.jsxs)(s.admonition,{type:"warning",children:[(0,r.jsxs)(s.p,{children:["You may have to SSH into each node if the nodes are in the ",(0,r.jsx)(s.code,{children:"invalid"}),", ",(0,r.jsx)(s.code,{children:"down"})," or ",(0,r.jsx)(s.code,{children:"drain"})," state instead of using the above ",(0,r.jsx)(s.code,{children:"srun"})," command. In order to do that please use the following script where ",(0,r.jsx)(s.code,{children:'nodes="ip-10-0-0-[1,2]"'})," is the nodelist from ",(0,r.jsx)(s.code,{children:"sinfo"}),"."]}),(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:'#!/bin/bash\n\nnodes="ip-10-0-0-[1,2]"\nNODES=$( scontrol show hostnames $nodes  | sed \'N;s/\\n/ /\' )\necho $NODES\ncommand="sudo systemctl restart slurmd"\n\nfor node in $NODES\ndo\n        echo -e "SSH into $node"\n        ssh $node $command\ndone\n'})}),(0,r.jsxs)(s.p,{children:["Run this script like, where ",(0,r.jsx)(s.code,{children:"restart_slurmd.sh"})," is the name of the script above:"]}),(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:"bash restart_slurmd.sh\n"})})]}),"\n",(0,r.jsxs)(s.p,{children:["Finally set the node states to ",(0,r.jsx)(s.code,{children:"idle"}),", where the ",(0,r.jsx)(s.code,{children:"nodename"})," points to all the nodes in your cluster:"]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:"sudo scontrol update nodename=ip-26-0-148-28,ip-x-x-x-x state=idle\n"})}),"\n",(0,r.jsx)(s.p,{children:"You can check to make sure gres is setup by running the following command:"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:'sinfo -o "%P %G %D %N"\n'})}),"\n",(0,r.jsx)(s.p,{children:"This command will show you the Gres resources for your instances:"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{children:"PARTITION GRES NODES NODELIST\nml.p5.48xlarge* gpu:8 1 ip-10-1-10-42\n"})}),"\n",(0,r.jsx)(s.p,{children:"Next you can test by submitting two jobs that each requires 4 gpus:"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:'cat > gpus.sh << EOF\n#!/bin/bash\n#SBATCH --gpus=4\n#SBATCH --array=0-1\n#SBATCH --mem=4G\n\necho -e "GPUs\'s assigned to this job: \\$CUDA_VISIBLE_DEVICES"\nEOF\n'})}),"\n",(0,r.jsx)(s.p,{children:"Submit the job and take note of the job id:"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:"sbatch gpus.sh\n"})}),"\n",(0,r.jsxs)(s.p,{children:["You'll see the following output, assuming each instance has 8 gpus and the job id is ",(0,r.jsx)(s.code,{children:"1"}),":"]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:"$ cat slurm-1_*\nGPUs's assigned to this job: 0,1,2,3\nGPUs's assigned to this job: 4,5,6,7\n"})})]})}function h(e={}){const{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453:(e,s,n)=>{n.d(s,{R:()=>i,x:()=>d});var o=n(6540);const r={},t=o.createContext(r);function i(e){const s=o.useContext(t);return o.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function d(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),o.createElement(t.Provider,{value:s},e.children)}}}]);