"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[3144],{8453:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>o});var r=a(6540);const i={},t=r.createContext(i);function s(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(t.Provider,{value:n},e.children)}},9980:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"eks-blueprints/training/ray-train/ray-train-readme","title":"Ray Train","description":"Ray is an open-source distributed computing framework designed to run highly scalable and parallel Python applications. Ray manages, executes, and optimizes compute needs across AI workloads. It unifies infrastructure via a single, flexible framework\u2014enabling any AI workload from data processing to model training to model serving and beyond.","source":"@site/docs/01-eks-blueprints/training/ray-train/ray-train-readme.md","sourceDirName":"01-eks-blueprints/training/ray-train","slug":"/eks-blueprints/training/ray-train/ray-train-readme","permalink":"/ai-on-sagemaker-hyperpod/docs/eks-blueprints/training/ray-train/ray-train-readme","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Ray Train","sidebar_position":2,"sidebar_label":"Ray Train"},"sidebar":"tutorialSidebar","previous":{"title":"NVIDIA Megatron-LM","permalink":"/ai-on-sagemaker-hyperpod/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme"},"next":{"title":"AWS Trainium","permalink":"/ai-on-sagemaker-hyperpod/docs/eks-blueprints/training/trainium/aws-trainium"}}');var i=a(4848),t=a(8453);const s={title:"Ray Train",sidebar_position:2,sidebar_label:"Ray Train"},o="Ray Train on HyperPod EKS",l={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Verified Instance Types",id:"verified-instance-types",level:3},{value:"1. Setup",id:"1-setup",level:2},{value:"1.1 Verify Connection to HyperPod Cluster",id:"11-verify-connection-to-hyperpod-cluster",level:3},{value:"1.2 Install Ray",id:"12-install-ray",level:3},{value:"1.3 Setup Dependencies",id:"13-setup-dependencies",level:3},{value:"1.4 Verify Persistent Volume Claims",id:"14-verify-persistent-volume-claims",level:3},{value:"2. Create Ray Container Image",id:"2-create-ray-container-image",level:2},{value:"2.1 Create Dockerfile",id:"21-create-dockerfile",level:3},{value:"2.2 Build and Push Container Image",id:"22-build-and-push-container-image",level:3},{value:"3. Create and Deploy RayCluster",id:"3-create-and-deploy-raycluster",level:2},{value:"3.1 Create RayCluster Manifest",id:"31-create-raycluster-manifest",level:3},{value:"3.2 Deploy the RayCluster",id:"32-deploy-the-raycluster",level:3},{value:"3.3 Verify RayCluster Deployment",id:"33-verify-raycluster-deployment",level:3},{value:"3.4 Expose Ray Dashboard",id:"34-expose-ray-dashboard",level:3},{value:"4. Submit Training Job",id:"4-submit-training-job",level:2},{value:"4.1 Download Training Code",id:"41-download-training-code",level:3},{value:"4.2 Configure Training Parameters",id:"42-configure-training-parameters",level:3},{value:"4.3 Submit Training Job",id:"43-submit-training-job",level:3},{value:"Method 1: Ray Jobs Submission SDK",id:"method-1-ray-jobs-submission-sdk",level:4},{value:"Method 2: Execute in Head Pod",id:"method-2-execute-in-head-pod",level:4},{value:"4.4 Monitor Training",id:"44-monitor-training",level:3},{value:"5. What&#39;s Next?",id:"5-whats-next",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"ray-train-on-hyperpod-eks",children:"Ray Train on HyperPod EKS"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://www.ray.io/",children:"Ray"})," is an open-source distributed computing framework designed to run highly scalable and parallel Python applications. Ray manages, executes, and optimizes compute needs across AI workloads. It unifies infrastructure via a single, flexible framework\u2014enabling any AI workload from data processing to model training to model serving and beyond."]}),"\n",(0,i.jsxs)(n.p,{children:["This example showcases how to get started with deploying a RayCluster for model training on SageMaker HyperPod. It demonstrates multi-node ",(0,i.jsx)(n.a,{href:"https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html",children:"FSDP"})," training using ",(0,i.jsx)(n.a,{href:"https://lightning.ai/docs/pytorch/stable/",children:"PyTorch Lightning"})," with Ray on Amazon EKS."]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.p,{children:"Before running this training, ensure you have:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A functional HyperPod EKS cluster on AWS with GPU device plugin deployed"}),"\n",(0,i.jsx)(n.li,{children:"An FSx for Lustre filesystem with a persistent volume claim (PVC)"}),"\n",(0,i.jsx)(n.li,{children:"Docker installed for building container images"}),"\n",(0,i.jsx)(n.li,{children:"A x86-64 based development environment (use SageMaker Code Editor if on ARM-based systems)"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"verified-instance-types",children:"Verified Instance Types"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ml.g5.8xlarge x 8"})," - used for training section"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"1-setup",children:"1. Setup"}),"\n",(0,i.jsx)(n.h3,{id:"11-verify-connection-to-hyperpod-cluster",children:"1.1 Verify Connection to HyperPod Cluster"}),"\n",(0,i.jsx)(n.p,{children:"Verify your connection to the HyperPod cluster:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes -L node.kubernetes.io/instance-type -L sagemaker.amazonaws.com/node-health-status -L sagemaker.amazonaws.com/deep-health-check-status\n"})}),"\n",(0,i.jsx)(n.p,{children:"Expected output:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"NAME                           STATUS   ROLES    AGE   VERSION               INSTANCE-TYPE   NODE-HEALTH-STATUS   DEEP-HEALTH-CHECK-STATUS\nhyperpod-i-01f3dccc49d2f1292   Ready    <none>   15d   v1.31.6-eks-1552ad0   ml.g5.8xlarge   Schedulable          Passed\nhyperpod-i-0499da6bcd94f240b   Ready    <none>   15d   v1.31.6-eks-1552ad0   ml.g5.8xlarge   Schedulable          Passed\n...\n"})}),"\n",(0,i.jsx)(n.h3,{id:"12-install-ray",children:"1.2 Install Ray"}),"\n",(0,i.jsx)(n.p,{children:"Install Ray on your local environment:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'pip install -U "ray[default]"\n'})}),"\n",(0,i.jsx)(n.h3,{id:"13-setup-dependencies",children:"1.3 Setup Dependencies"}),"\n",(0,i.jsx)(n.p,{children:"Set up the KubeRay operator to manage Ray clusters:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create KubeRay namespace\nkubectl create namespace kuberay\n\n# Deploy the KubeRay operator with the Helm chart repository\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm repo update\n\n# Install both CRDs and KubeRay operator v1.1.0\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.1.0 --namespace kuberay\n\n# Verify KubeRay operator deployment\nkubectl get pods --namespace kuberay\n"})}),"\n",(0,i.jsx)(n.p,{children:"Verify the KubeRay operator is running:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n kuberay\n"})}),"\n",(0,i.jsx)(n.p,{children:"Expected output:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"NAME                               READY   STATUS    RESTARTS   AGE\nkuberay-operator-cdc889475-dfmsc   1/1     Running   0          15d\n"})}),"\n",(0,i.jsx)(n.h3,{id:"14-verify-persistent-volume-claims",children:"1.4 Verify Persistent Volume Claims"}),"\n",(0,i.jsx)(n.p,{children:'Ensure you have an active PVC with status "Bound":'}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get pvc\n"})}),"\n",(0,i.jsx)(n.p,{children:"Expected output:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nfsx-claim   Bound    pvc-8a696249-051d-4d4e-8908-bdb6c66ddbca   1200Gi     RWX            fsx-sc         18d\n"})}),"\n",(0,i.jsx)(n.h2,{id:"2-create-ray-container-image",children:"2. Create Ray Container Image"}),"\n",(0,i.jsx)(n.h3,{id:"21-create-dockerfile",children:"2.1 Create Dockerfile"}),"\n",(0,i.jsxs)(n.p,{children:["With the recent ",(0,i.jsx)(n.a,{href:"https://github.com/ray-project/ray/issues/46378",children:"deprecation"})," of the ",(0,i.jsx)(n.code,{children:"rayproject/ray-ml"})," images starting from Ray version 2.31.0, create a custom container image using our ",(0,i.jsx)(n.a,{href:"https://gallery.ecr.aws/hpc-cloud/nccl-tests",children:"nccl-tests public container image"})," as a base image:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'cat <<EOF > Dockerfile\nFROM public.ecr.aws/hpc-cloud/nccl-tests:latest\n\n# Create ray user and home directory\nRUN useradd -m -d /home/ray ray && \\\n    chown -R ray:ray /home/ray\n\nCOPY --from=rayproject/ray:2.42.1-py310-gpu /home/ray/requirements_compiled.txt /tmp/\n\n# Install anaconda if it\'s not in base image\nRUN if [ ! -d "/opt/anaconda3" ]; then \\\n    wget https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh -O /tmp/anaconda.sh && \\\n    bash /tmp/anaconda.sh -b -p /opt/anaconda3 && \\\n    rm /tmp/anaconda.sh; \\\n    fi\n\n# Add anaconda to system-wide PATH\nENV PATH=/opt/anaconda3/bin:$PATH\n\n# Install Ray and dependencies\nRUN pip --no-cache-dir install -c /tmp/requirements_compiled.txt \\\n    "ray[all]==2.42.1"\n\n# Install Python dependencies for PyTorch, Ray, Hugging Face, and more\nRUN pip install --no-cache-dir \\\n    torch torchvision torchaudio \\\n    numpy \\\n    pytorch-lightning \\\n    transformers datasets evaluate tqdm click \\\n    ray[train] ray[air] \\\n    ray[train-torch] ray[train-lightning] \\\n    torchdata \\\n    torchmetrics \\\n    torch_optimizer \\\n    accelerate \\\n    scikit-learn \\\n    Pillow==9.5.0 \\\n    protobuf==3.20.3\n\nRUN pip install --upgrade datasets transformers\n\n# Save pip freeze output\nRUN pip freeze > /home/ray/pip-freeze.txt && \\\n    chown ray:ray /home/ray/pip-freeze.txt\n\n# Cleanup\nRUN rm -rf /tmp/requirements_compiled.txt\n\n# Set the user\nUSER ray\nWORKDIR /home/ray\n\n# Verify ray installation\nRUN which ray && \\\n    ray --version\n\n# Default command\nCMD [ "/bin/bash" ]\nEOF\n'})}),"\n",(0,i.jsx)(n.h3,{id:"22-build-and-push-container-image",children:"2.2 Build and Push Container Image"}),"\n",(0,i.jsx)(n.p,{children:"Build and push the image to Amazon ECR:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'export AWS_REGION=$(aws configure get region)\nexport ACCOUNT=$(aws sts get-caller-identity --query Account --output text)\nexport REGISTRY=${ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com/\n\necho "This process may take 10-15 minutes to complete..."\necho "Building image..."\n\ndocker build --platform linux/amd64 -t ${REGISTRY}aws-ray-custom:latest .\n\n# Create registry if needed\nREGISTRY_COUNT=$(aws ecr describe-repositories | grep \\"aws-ray-custom\\" | wc -l)\nif [ "$REGISTRY_COUNT" == "0" ]; then\n    aws ecr create-repository --repository-name aws-ray-custom\nfi\n\n# Login to registry\necho "Logging in to $REGISTRY ..."\naws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $REGISTRY\n\necho "Pushing image to $REGISTRY ..."\n\n# Push image to registry\ndocker image push ${REGISTRY}aws-ray-custom:latest\n'})}),"\n",(0,i.jsx)(n.h2,{id:"3-create-and-deploy-raycluster",children:"3. Create and Deploy RayCluster"}),"\n",(0,i.jsx)(n.h3,{id:"31-create-raycluster-manifest",children:"3.1 Create RayCluster Manifest"}),"\n",(0,i.jsx)(n.p,{children:"Create a Ray cluster manifest that defines the head node and worker nodes:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'cat <<\'EOF\' > raycluster.yaml\napiVersion: ray.io/v1alpha1\nkind: RayCluster\nmetadata:\n  name: rayml\n  labels:\n    controller-tools.k8s.io: "1.0"\nspec:\n  # Ray head pod template\n  headGroupSpec:\n    rayStartParams:\n      dashboard-host: \'0.0.0.0\'\n    template:\n      spec:\n        securityContext:\n          runAsUser: 0\n          runAsGroup: 0\n          fsGroup: 0\n        containers:\n        - name: ray-head\n          image: ${REGISTRY}aws-ray-custom:latest\n          env:\n            - name: RAY_GRAFANA_IFRAME_HOST\n              value: http://localhost:3000\n            - name: RAY_GRAFANA_HOST\n              value: http://prometheus-grafana.prometheus-system.svc:80\n            - name: RAY_PROMETHEUS_HOST\n              value: http://prometheus-kube-prometheus-prometheus.prometheus-system.svc:9090\n            - name: FI_PROVIDER\n              value: "efa"\n            - name: NCCL_DEBUG\n              value: "INFO"\n            - name: FI_LOG_LEVEL\n              value: "warn"\n            - name: NCCL_SOCKET_IFNAME\n              value: "eth0"\n          lifecycle:\n            preStop:\n              exec:\n                command: ["/bin/sh","-c","ray stop"]\n          resources:\n            limits:\n              cpu: 1\n              memory: 8Gi\n            requests:\n              cpu: 1\n              memory: 8Gi\n          ports:\n          - containerPort: 6379\n            name: gcs-server\n          - containerPort: 8265 # Ray dashboard\n            name: dashboard\n          - containerPort: 10001\n            name: client\n          - containerPort: 8000\n            name: serve\n          volumeMounts:\n          - name: fsx-storage\n            mountPath: /fsx\n          - name: ray-logs\n            mountPath: /tmp/ray\n        volumes:\n          - name: ray-logs\n            emptyDir: {}\n          - name: fsx-storage\n            persistentVolumeClaim:\n              claimName: fsx-claim\n  workerGroupSpecs:\n  - replicas: 8\n    minReplicas: 1\n    maxReplicas: 10\n    groupName: gpu-group\n    rayStartParams:\n      num-gpus: "1"\n    template:\n      spec:\n        securityContext:\n          runAsUser: 0\n          runAsGroup: 0\n          fsGroup: 0\n        containers:\n        - name: ray-worker\n          image: ${REGISTRY}aws-ray-custom:latest\n          env:\n            - name: FI_PROVIDER\n              value: "efa"\n            - name: NCCL_DEBUG\n              value: "INFO"\n            - name: FI_LOG_LEVEL\n              value: "warn"\n            - name: NCCL_SOCKET_IFNAME\n              value: "eth0"\n          lifecycle:\n            preStop:\n              exec:\n                command: ["/bin/sh","-c","ray stop"]\n          resources:\n            limits:\n              nvidia.com/gpu: 1\n              vpc.amazonaws.com/efa: 1\n            requests:\n              nvidia.com/gpu: 1\n              vpc.amazonaws.com/efa: 1\n          volumeMounts:\n          - name: ray-logs\n            mountPath: /tmp/ray\n          - name: fsx-storage\n            mountPath: /fsx\n        volumes:\n        - name: fsx-storage\n          persistentVolumeClaim:\n            claimName: fsx-claim\n        - name: ray-logs\n          emptyDir: {}\nEOF\n'})}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["If you want to use a different file system, change ",(0,i.jsx)(n.code,{children:"claimName"})," to your desired PVC. Check available PVCs with ",(0,i.jsx)(n.code,{children:"kubectl get pvc"}),"."]})}),"\n",(0,i.jsx)(n.h3,{id:"32-deploy-the-raycluster",children:"3.2 Deploy the RayCluster"}),"\n",(0,i.jsx)(n.p,{children:"Deploy the RayCluster using environment variable substitution:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"envsubst < raycluster.yaml | kubectl apply -f -\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["We use ",(0,i.jsx)(n.code,{children:"envsubst"})," to substitute the ",(0,i.jsx)(n.code,{children:"${REGISTRY}"})," variable. If you don't have envsubst, manually replace the variable in the YAML or install it following ",(0,i.jsx)(n.a,{href:"https://github.com/a8m/envsubst",children:"this documentation"}),"."]})}),"\n",(0,i.jsx)(n.h3,{id:"33-verify-raycluster-deployment",children:"3.3 Verify RayCluster Deployment"}),"\n",(0,i.jsx)(n.p,{children:"Check the status of your RayCluster:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get pods\n"})}),"\n",(0,i.jsx)(n.p,{children:"Expected output:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"NAME                           READY   STATUS    RESTARTS   AGE\nrayml-gpu-group-worker-xxxx   1/1     Running   0          3d3h\nrayml-gpu-group-worker-xxxx   1/1     Running   0          3d3h\nrayml-gpu-group-worker-xxxx   1/1     Running   0          3d3h\nrayml-gpu-group-worker-xxxx   1/1     Running   0          3d3h\nrayml-gpu-group-worker-xxxx   1/1     Running   0          3d3h\nrayml-gpu-group-worker-xxxx   1/1     Running   0          3d3h\nrayml-gpu-group-worker-xxxx   1/1     Running   0          3d3h\nrayml-gpu-group-worker-xxxx   1/1     Running   0          3d3h\nrayml-head-xxxx               1/1     Running   0          3d3h\n"})}),"\n",(0,i.jsx)(n.h3,{id:"34-expose-ray-dashboard",children:"3.4 Expose Ray Dashboard"}),"\n",(0,i.jsx)(n.p,{children:"To access the Ray dashboard for monitoring cluster status, job submissions, and resource utilization:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Get the name of the head service\nexport SERVICEHEAD=$(kubectl get service | grep head-svc | awk '{print $1}' | head -n 1)\n\n# Port forward the dashboard\nkubectl port-forward --address 0.0.0.0 service/${SERVICEHEAD} 8265:8265 > /dev/null 2>&1 &\n"})}),"\n",(0,i.jsxs)(n.p,{children:["You can now access the Ray dashboard at ",(0,i.jsx)(n.code,{children:"http://localhost:8265"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"4-submit-training-job",children:"4. Submit Training Job"}),"\n",(0,i.jsx)(n.h3,{id:"41-download-training-code",children:"4.1 Download Training Code"}),"\n",(0,i.jsx)(n.p,{children:"Download the aws-do-ray repository which contains the training code:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/aws-samples/aws-do-ray.git\ncd aws-do-ray/Container-Root/ray/raycluster/jobs/\n"})}),"\n",(0,i.jsx)(n.h3,{id:"42-configure-training-parameters",children:"4.2 Configure Training Parameters"}),"\n",(0,i.jsx)(n.p,{children:"The training job uses FSDP (Fully Sharded Data Parallel) with PyTorch Lightning. To utilize all GPU nodes, modify the scaling configuration:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"vim fsdp-ray/fsdp-ray.py\n"})}),"\n",(0,i.jsxs)(n.p,{children:["In the main function, edit the ",(0,i.jsx)(n.code,{children:"scaling_config"})," to use 8 workers (one for each GPU instance):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Schedule workers for FSDP training (1 GPU/worker by default)\nscaling_config = ScalingConfig(\n    num_workers=8,  # Change from 2 to 8\n    use_gpu=True,\n    resources_per_worker={"GPU": 1, "CPU": 3}\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"43-submit-training-job",children:"4.3 Submit Training Job"}),"\n",(0,i.jsx)(n.p,{children:"You can submit the training job using two methods:"}),"\n",(0,i.jsx)(n.h4,{id:"method-1-ray-jobs-submission-sdk",children:"Method 1: Ray Jobs Submission SDK"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Within jobs/ folder\nray job submit --address http://localhost:8265 --working-dir "fsdp-ray" -- python3 fsdp-ray.py\n'})}),"\n",(0,i.jsx)(n.h4,{id:"method-2-execute-in-head-pod",children:"Method 2: Execute in Head Pod"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Get head pod name\nhead_pod=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\n\n# Copy the script to the head pod\nkubectl cp "fsdp-ray/fsdp-ray.py" "$head_pod:/tmp/fsdp-ray.py"\n\n# Run the Python script on the head pod\nkubectl exec -it "$head_pod" -- python /tmp/fsdp-ray.py\n'})}),"\n",(0,i.jsx)(n.h3,{id:"44-monitor-training",children:"4.4 Monitor Training"}),"\n",(0,i.jsx)(n.p,{children:"The training job will:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Preprocess the CoLA dataset with Ray Data"}),"\n",(0,i.jsx)(n.li,{children:"Define a training function with PyTorch Lightning"}),"\n",(0,i.jsx)(n.li,{children:"Launch distributed training with Ray Train's TorchTrainer"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Expected output:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Training started with configuration:\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Training config                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 train_loop_config/batch_size      16 \u2502\n\u2502 train_loop_config/eps          1e-08 \u2502\n\u2502 train_loop_config/lr           1e-05 \u2502\n\u2502 train_loop_config/max_epochs       5 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n"})}),"\n",(0,i.jsx)(n.p,{children:"You can monitor the training progress through:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Ray Dashboard"}),": View job status, resource utilization, and logs at ",(0,i.jsx)(n.code,{children:"http://localhost:8265"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Terminal logs"}),": Follow the training progress in your terminal"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Shared filesystem"}),": Trained models will be saved to your FSx filesystem"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"5-whats-next",children:"5. What's Next?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model Output"}),": Your trained model will be saved to the shared file system (",(0,i.jsx)(n.code,{children:"/fsx"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Experiment with other models"}),": Try training other example models in the repository"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Explore Ray features"}),": Check out the Ray decorators and distributed computing capabilities"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scale your workloads"}),": Modify the RayCluster configuration for different instance types and counts"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Inference"}),": We will deploy RayService for inference in the inference section of this repository."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["For more information on RayCluster configurations, see the ",(0,i.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/config.html",children:"Ray documentation"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["For additional examples and configurations, refer to the ",(0,i.jsx)(n.a,{href:"https://github.com/aws-samples/aws-do-ray",children:"aws-do-ray repository"}),"."]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);