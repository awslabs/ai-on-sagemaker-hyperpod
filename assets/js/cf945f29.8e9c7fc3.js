"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[981],{8453:(e,s,n)=>{n.d(s,{R:()=>i,x:()=>l});var t=n(6540);const o={},r=t.createContext(o);function i(e){const s=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(r.Provider,{value:s},e.children)}},8602:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>i,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"getting-started/orchestrated-by-slurm/slurm-basics","title":"Basic Slurm Commands","description":"Now that you\'ve created and set up the cluster, you will go through some of the commands you\'ll use to run Llama7b on the cluster.","source":"@site/docs/00-getting-started/orchestrated-by-slurm/slurm-basics.md","sourceDirName":"00-getting-started/orchestrated-by-slurm","slug":"/getting-started/orchestrated-by-slurm/slurm-basics","permalink":"/docs/getting-started/orchestrated-by-slurm/slurm-basics","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Basic Slurm Commands","sidebar_position":5,"weight":4},"sidebar":"tutorialSidebar","previous":{"title":"SSH Into Your HyperPod Cluster","permalink":"/docs/getting-started/orchestrated-by-slurm/ssh-into-hyperpod"},"next":{"title":"Installing the required tools","permalink":"/docs/getting-started/install-pre-requisites"}}');var o=n(4848),r=n(8453);const i={title:"Basic Slurm Commands",sidebar_position:5,weight:4},l=void 0,c={},d=[{value:"SLURM",id:"slurm",level:2},{value:"Shared Filesystems",id:"shared-filesystems",level:2},{value:"SSH to compute nodes",id:"ssh-to-compute-nodes",level:2}];function a(e){const s={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(s.p,{children:"Now that you've created and set up the cluster, you will go through some of the commands you'll use to run Llama7b on the cluster."}),"\n",(0,o.jsx)(s.h2,{id:"slurm",children:"SLURM"}),"\n",(0,o.jsx)("p",{align:"center",children:(0,o.jsx)("img",{src:"/img/01-cluster/slurm.png",alt:"Cloud Shell",width:"128"})}),"\n",(0,o.jsxs)(s.p,{children:[(0,o.jsx)(s.a,{href:"https://slurm.schedmd.com",children:"SLURM"})," from SchedMD is one of the batch schedulers that you can use in SageMaker HyperPod. For an overview of the SLURM commands, see the ",(0,o.jsx)(s.a,{href:"https://slurm.schedmd.com/quickstart.html",children:"SLURM Quick Start User Guide"}),"."]}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsxs)(s.li,{children:[(0,o.jsx)(s.strong,{children:"List existing partitions and nodes per partition"}),". Running ",(0,o.jsx)(s.code,{children:"sinfo"})," shows the partition we created. Initially you will see the node in state ",(0,o.jsx)(s.code,{children:"idle"}),", this means no jobs are running. When you submit a job, the instance will go into state ",(0,o.jsx)(s.code,{children:"alloc"})," meaning it's currently completely allocated, or ",(0,o.jsx)(s.code,{children:"mix"})," meaning some but not all cores are allocated."]}),"\n"]}),"\n",(0,o.jsxs)(s.table,{children:[(0,o.jsx)(s.thead,{children:(0,o.jsxs)(s.tr,{children:[(0,o.jsx)(s.th,{children:(0,o.jsx)(s.strong,{children:"State"})}),(0,o.jsx)(s.th,{children:(0,o.jsx)(s.strong,{children:"Description"})})]})}),(0,o.jsxs)(s.tbody,{children:[(0,o.jsxs)(s.tr,{children:[(0,o.jsx)(s.td,{children:(0,o.jsx)(s.code,{children:"idle"})}),(0,o.jsx)(s.td,{children:"Instance is not running any jobs but is available."})]}),(0,o.jsxs)(s.tr,{children:[(0,o.jsx)(s.td,{children:(0,o.jsx)(s.code,{children:"mix"})}),(0,o.jsx)(s.td,{children:"Instance is partially allocated."})]}),(0,o.jsxs)(s.tr,{children:[(0,o.jsx)(s.td,{children:(0,o.jsx)(s.code,{children:"alloc"})}),(0,o.jsx)(s.td,{children:"Instance is completely allocated."})]})]})]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"sinfo\n"})}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsxs)(s.li,{children:[(0,o.jsx)(s.strong,{children:"List jobs in the queues or running"}),". Obviously, there won't be any since you did not submit anything...yet!"]}),"\n"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"squeue\n"})}),"\n",(0,o.jsx)(s.h2,{id:"shared-filesystems",children:"Shared Filesystems"}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsxs)(s.li,{children:[(0,o.jsx)(s.strong,{children:"List mounted NFS volumes"}),". A few volumes are shared by the head-node and will be mounted on compute instances when they boot up."]}),"\n"]}),"\n",(0,o.jsxs)(s.p,{children:["You can see ",(0,o.jsx)(s.em,{children:"network mount filesystems"}),", such as the ",(0,o.jsx)(s.em,{children:"/fsx"})," FSx Lustre filesystem that was mounted on the cluster by running ",(0,o.jsx)(s.code,{children:"df -h"}),":"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"df -h\n"})}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{children:"Filesystem                 Size  Used Avail Use% Mounted on\n/dev/root                  146G   59G   87G  41% /\ndevtmpfs                   3.8G     0  3.8G   0% /dev\ntmpfs                      3.8G     0  3.8G   0% /dev/shm\ntmpfs                      765M  1.2M  763M   1% /run\ntmpfs                      5.0M     0  5.0M   0% /run/lock\ntmpfs                      3.8G     0  3.8G   0% /sys/fs/cgroup\n/dev/loop0                  56M   56M     0 100% /snap/core18/2066\n/dev/loop1                  56M   56M     0 100% /snap/core18/2796\n/dev/loop4                  68M   68M     0 100% /snap/lxd/20326\n/dev/loop5                  92M   92M     0 100% /snap/lxd/24061\n/dev/loop3                  64M   64M     0 100% /snap/core20/2015\n/dev/loop2                  41M   41M     0 100% /snap/snapd/20290\ntmpfs                      765M   24K  765M   1% /run/user/128\n/dev/nvme1n1               500G  711M  500G   1% /opt/sagemaker\n10.1.71.197@tcp:/oyuutbev  1.2T  5.5G  1.2T   1% /fsx\ntmpfs                      765M  4.0K  765M   1% /run/user/1000\n"})}),"\n",(0,o.jsx)(s.h2,{id:"ssh-to-compute-nodes",children:"SSH to compute nodes"}),"\n",(0,o.jsx)(s.p,{children:"Now let's SSH to the compute nodes, this allows you to test code on GPU instances quickly without submitting a bash job."}),"\n",(0,o.jsxs)(s.ol,{children:["\n",(0,o.jsxs)(s.li,{children:["First make sure you're logged into the cluster as ",(0,o.jsx)(s.code,{children:"ubuntu"}),":"]}),"\n"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"ssh ml-cluster\n"})}),"\n",(0,o.jsxs)(s.ol,{start:"2",children:["\n",(0,o.jsx)(s.li,{children:"Now we can ssh into one of the compute nodes!"}),"\n"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"salloc -N 1\nssh $(srun hostname)\n"})}),"\n",(0,o.jsxs)(s.p,{children:["This allocates an interactive node with ",(0,o.jsx)(s.code,{children:"salloc"}),", then uses ",(0,o.jsx)(s.code,{children:"srun"})," to grab the hostname and ssh in."]}),"\n",(0,o.jsx)(s.p,{children:"Now that we're familiar with the cluster, we're ready to submit our first job. Before proceeding, make sure you exit to the Head Node:"}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"exit\n"})}),"\n",(0,o.jsxs)(s.p,{children:["Run ",(0,o.jsx)(s.code,{children:"exit"})," one more time to cancel the ",(0,o.jsx)(s.code,{children:"srun"})," job:"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"exit\n"})}),"\n",(0,o.jsxs)(s.admonition,{title:"Pro-tip",type:"info",children:[(0,o.jsxs)(s.p,{children:[(0,o.jsx)(s.strong,{children:"Note: This is an optional step. If you're confused about which node you're on, this is a great way to check"}),"\nIf you have a huge cluster it can be confusing to figure out which node you're on. To make it easier, you can update your bash prompt to reflect if you're on a CONTROLLER or WORKER node."]}),(0,o.jsxs)(s.p,{children:["You can execute the following commands to set the type of the instance ",(0,o.jsx)(s.code,{children:"CONTROLLER"})," or ",(0,o.jsx)(s.code,{children:"WORKER"})," in the bash prompt:"]}),(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"echo -e \"\\n# Show (CONTROLLER) or (WORKER) on the CLI prompt\" >> ~/.bashrc\necho 'head_node_ip=$(sudo cat /opt/ml/config/resource_config.json | jq '\"'\"'.InstanceGroups[] | select(.Name == \"controller-machine\") | .Instances[0].CustomerIpAddress'\"'\"' | tr -d '\"'\"'\"'\"'\"')' >> ~/.bashrc\necho 'if [ $(hostname -I | awk '\"'\"'{print $1}'\"'\"') = $head_node_ip ]; then PS1=\"(CONTROLLER) ${PS1}\"; else PS1=\"(WORKER) ${PS1}\"; fi' >> ~/.bashrc\n"})}),(0,o.jsx)(s.p,{children:"Next source ~/.bashrc:"}),(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"source ~/.bashrc\n"})}),(0,o.jsxs)(s.p,{children:["Voila, you should now see ",(0,o.jsx)(s.code,{children:"(CONTROLLER)"})," or ",(0,o.jsx)(s.code,{children:"(WORKER)"})," on the cli prompt."]})]})]})}function h(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,o.jsx)(s,{...e,children:(0,o.jsx)(a,{...e})}):a(e)}}}]);