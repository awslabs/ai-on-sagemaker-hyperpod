"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1254],{4385:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"Tips/Slurm/slurm-epilogue","title":"Enable Slurm epilog Script","description":"Slurm epilog scripts can be used to perform tasks automatically after a job completes on a cluster. Implementing Slurm epilog scripts allows users and administrators to automate essential post-job tasks, such as resource cleanup, logging and monitoring, notifcations, and data management:","source":"@site/docs/08-Tips/Slurm/18-slurm-epilogue.md","sourceDirName":"08-Tips/Slurm","slug":"/Tips/Slurm/slurm-epilogue","permalink":"/ai-on-sagemaker-hyperpod/docs/Tips/Slurm/slurm-epilogue","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":18,"frontMatter":{"title":"Enable Slurm epilog Script","weight":58},"sidebar":"tutorialSidebar","previous":{"title":"Configure Cgroups for Slurm","permalink":"/ai-on-sagemaker-hyperpod/docs/Tips/Slurm/enable-cgroups"},"next":{"title":"Delete Cluster Nodes","permalink":"/ai-on-sagemaker-hyperpod/docs/Tips/Slurm/delete-cluster-nodes"}}');var t=s(4848),i=s(8453);const r={title:"Enable Slurm epilog Script",weight:58},l=void 0,a={},c=[];function d(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",em:"em",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://slurm.schedmd.com/prolog_epilog.html",children:"Slurm epilog scripts"})," can be used to perform tasks automatically after a job completes on a cluster. Implementing Slurm epilog scripts allows users and administrators to automate essential post-job tasks, such as resource cleanup, logging and monitoring, notifcations, and data management:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Resource Cleanup"}),": Automatically clean up allocated resources, terminate orphaned processes, and reset system states."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Logging and Monitoring"}),": Capture job-related information, performance metrics, and potential issues for auditing, analysis, and troubleshooting.\nResource Accounting: Update resource usage databases for accurate tracking, billing, or quota management."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Notifications"}),": Send automated notifications to users or administrators about job statuses, errors, or other relevant events."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data Management"}),": Archive, transfer, or process data generated by jobs, ensuring efficient data handling and storage.\nSecurity: Enforce security policies and scan for unauthorized access or changes during job execution."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"The following explains steps to add a sample (and benign!) epilog script to your hyperpod cluster, which will run on a compute node at the competition of any slurm job."})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Create a directory on your shared file system to house your epilog script and epilog log files."}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["In this example, we will assume there is a shared file system with path ",(0,t.jsx)(n.code,{children:"/fsx"}),", however if you have a different path for your shared file system, you will need to substitute the path for your shared file system path into the example provided below."]})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"On controller/head node:"})}),"\n",(0,t.jsx)(n.blockquote,{children:"\n"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# assume root privileges\nsudo su\n\n# cd into root FSxL directory, /fsx\ncd /fsx\n\n# confirm you are on the shared file system home directory\npwd\n# should show /fsx\n\n# make a directory to house epilog artifacts \nmkdir epilog\n\n# move into the newly created epilog directory\ncd epilog \n\n# confirm you are in the epilog directory\npwd\n# should show /fsx/epilog\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"The following example epilog script it will echo a timestamp, list the node-name, get a list of the top 10 running processes, and and list any active user sessions. The epilog script will write logs to the log directory on the shared file-system defined in step 1. in this case we use the same directory for our logs as we do the epilog script itself."}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'sudo bash -c \'cat > /fsx/epilog/epilog-script.sh <<EOF\n#!/bin/bash\n\n# Get the node name (hostname)\nnode_name=\\$(hostname)\n\n# Get the current timestamp\ntimestamp=\\$(date)\n\n# Get the list of top 10 running processes\ntop_processes=\\$(ps -e --sort=-%mem | head -n 11)\n\n# Get the Slurm job ID and job name\njob_id=\\${SLURM_JOB_ID:-"undefined_job_id"}\njob_name=\\${SLURM_JOB_NAME:-"undefined_job_name"}\n\n# Get the list of active user sessions\nuser_sessions=\\$(who)\n\n# Define the log directory and create it if it doesn\'\\\'\'t exist\nlog_dir="/fsx/epilog/logs"\nmkdir -p "\\$log_dir"\n\n# Name the log file using the Slurm job name and job ID\nlogfile="\\$log_dir/epilog_\\${job_name}_\\${job_id}.log"\n\n# Log the information to the file\n{\n    echo "Node Name: \\$node_name"\n    echo "Timestamp: \\$timestamp"\n    echo "Slurm Job ID: \\$job_id"\n    echo "Slurm Job Name: \\$job_name"\n    echo "Top 10 Running Processes:"\n    echo "\\$top_processes"\n    echo ""\n    echo "Active User Sessions:"\n    echo "\\$user_sessions"\n} >> "\\$logfile" 2>&1\nEOF\'\n\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsxs)(n.li,{children:["Add execute permissions on ",(0,t.jsx)(n.code,{children:"epilog-script.sh"})," and modify permissions to allow slurm to write to log directory ",(0,t.jsx)(n.code,{children:"/fsx/epilog/log"}),":"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo chmod +x epilog-script.sh\n\nsudo chown -R slurm:slurm /fsx/epilog\nsudo chmod -R 755 /fsx/epilog\n\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"4",children:["\n",(0,t.jsxs)(n.li,{children:["With the epilog script written to ",(0,t.jsx)(n.code,{children:"/fsx/epilog/epilog-script.sh"}),", next step is to modify slurm.conf. To do so, we will add a line to specify the path to your epilog script under the section. Lets start by grepping our ",(0,t.jsx)(n.code,{children:"slurm.conf"})," file to see where we will add a refence path for slurm to execute the epilog script created in step 2."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'grep -A 3 "# Slurmctld settings" /opt/slurm/etc/slurm.conf\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Now lets add a line in ",(0,t.jsx)(n.code,{children:"slurm.conf"})," to tell slurm where to find our epilog script:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo sed -i '/# Slurmctld settings/a Epilog=/fsx/epilog/epilog-script.sh' /opt/slurm/etc/slurm.conf\n"})}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["Optionally, as an alternative to standard Epilog which runs on each individual cluster node, you can specify a EpilogSlurmctld which will only run on Slurmctl node (Controller Node):\n",(0,t.jsx)(n.code,{children:"sudo sed -i '/# Slurmctld settings/a EpilogSlurmctld=/fsx/epilog/epilog-script.sh' /opt/slurm/etc/slurm.conf "})]})}),"\n",(0,t.jsx)(n.p,{children:"Once added, we can verify the line has been added with:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'cat /opt/slurm/etc/slurm.conf | grep "epilog"\n\n# should show "Epilog=/fsx/epilog/epilog-script.sh"\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"5",children:["\n",(0,t.jsxs)(n.li,{children:["With the epilog defined in ",(0,t.jsx)(n.code,{children:"slurm.conf"}),", lets apply the new configuration to the cluster."]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["When we make modifications to ",(0,t.jsx)(n.code,{children:"slurm.conf"}),", it is required to restart slurmctld (",(0,t.jsx)(n.a,{href:"https://slurm.schedmd.com/slurmctld.html",children:"Slurm Controller Daemon"}),") to apply configuration changes. This process should not effect any running jobs if you are just adding an epilog script to ",(0,t.jsx)(n.code,{children:"slurm.conf"}),". You will, however, see a breif downtime (1-2min) when trying to run slurm commands like",(0,t.jsx)(n.code,{children:"sinfo"})," that communicate with the Slurm Controller Daemon. It is best practice to notify those using the cluster that you are restarting slurmctld before doing so."]})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# restart slurm ctld (takes 1-2 minutes)\nsudo systemctl restart slurmctld \n\n"})}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.p,{children:"After restarting slurmctld, it is normal to observe temporary downtime when running slurm commands. Dont worry, your running jobs wont be effected. This is normal and expected behavior when restarting slurmctld, wait another minute or so and try again."})}),"\n",(0,t.jsx)(n.p,{children:"Once sinfo is responding, it is safe to run scontrol reconfigure to propogate the changes to the cluster nodes (this process will also take 1-2 minutes):"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"#pass the updated configuration to the cluster nodes (takes 1-2 mintues)\nsudo scontrol reconfigure \n# note slurm commands may be temporarily unavailable after executing this command. They should return after 1-2 minutes\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"5",children:["\n",(0,t.jsxs)(n.li,{children:["Congratulations! After following these steps, you can verify your epilog script is configured with slurm by running a job. Upon job completetion, you can view the output of the epilog script in the directory ",(0,t.jsx)(n.code,{children:"/fsx/epilog"}),". You can now modify and adapt the epilog script to accomplish more useful tasks than just logging!"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>l});var o=s(6540);const t={},i=o.createContext(t);function r(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);