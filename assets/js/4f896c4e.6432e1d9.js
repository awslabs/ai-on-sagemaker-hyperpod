"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[8628],{1036:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"slurm-blueprints/training/trainium/Llama3-70B/download-model","title":"Downloading the Llama3-70b model","description":"In this section, we will download the Llama3 model and the llama tokenizer. We will then also prepare the model for the Neuron runtime by converting the model weights to be pre-sharded based on the parallel processing configuration (i.e., the degrees of the model parallelism axes).","source":"@site/docs/02-slurm-blueprints/training/trainium/Llama3-70B/02-download-model.md","sourceDirName":"02-slurm-blueprints/training/trainium/Llama3-70B","slug":"/slurm-blueprints/training/trainium/Llama3-70B/download-model","permalink":"/ai-on-sagemaker-hyperpod/docs/slurm-blueprints/training/trainium/Llama3-70B/download-model","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Downloading the Llama3-70b model","sidebar_position":3,"weight":32},"sidebar":"tutorialSidebar","previous":{"title":"Setting up the software stack","permalink":"/ai-on-sagemaker-hyperpod/docs/slurm-blueprints/training/trainium/Llama3-70B/prep"},"next":{"title":"Downloading the Wiki-corpus datasets","permalink":"/ai-on-sagemaker-hyperpod/docs/slurm-blueprints/training/trainium/Llama3-70B/download-dataset"}}');var s=a(4848),i=a(8453);const r={title:"Downloading the Llama3-70b model",sidebar_position:3,weight:32},o=void 0,l={},d=[{value:"Download the Llama3-70b model and tokenizer",id:"download-the-llama3-70b-model-and-tokenizer",level:2},{value:"Convert the Llama3 model weights",id:"convert-the-llama3-model-weights",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"In this section, we will download the Llama3 model and the llama tokenizer. We will then also prepare the model for the Neuron runtime by converting the model weights to be pre-sharded based on the parallel processing configuration (i.e., the degrees of the model parallelism axes)."}),"\n",(0,s.jsx)(n.h2,{id:"download-the-llama3-70b-model-and-tokenizer",children:"Download the Llama3-70b model and tokenizer"}),"\n",(0,s.jsxs)(n.p,{children:["First, make sure that you have a Hugging Face account with a valid ",(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"User Access Token"}),". Also, since the Llama3 herd of families are hosted on gated repos on Hugging Face, please make sure that your Hugging Face account has access to the ",(0,s.jsx)(n.a,{href:"https://huggingface.co/meta-llama/Meta-Llama-3-70B",children:"Meta-Llama-3-70B"})," model repository."]}),"\n",(0,s.jsx)(n.p,{children:"On your head node, run"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"huggingface-cli login\n"})}),"\n",(0,s.jsxs)(n.p,{children:["You will be prompted to enter your token. Paste in the token and answer ",(0,s.jsx)(n.code,{children:"n"})," when prompted to add the token as a git credential."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\nEnter your token (input will not be visible): \nAdd token as git credential? (Y/n) n\nToken is valid (permission: read).\nYour token has been saved to /fsx/ubuntu/.cache/huggingface/token\nLogin successful\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Now that you're logged in, let's grab the model weights (you may choose to use ",(0,s.jsx)(n.code,{children:"git clone"})," if you wish too):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"huggingface-cli download meta-llama/Meta-Llama-3-70B --local-dir /fsx/ubuntu/Meta-Llama-3-70B\n"})}),"\n",(0,s.jsx)(n.p,{children:"Once the download is completed (~30 min), you will see the following directory structure:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"/fsx/ubuntu/Meta-Llama-3-70B/\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 USE_POLICY.md\n\u251c\u2500\u2500 config.json\n\u251c\u2500\u2500 generation_config.json\n\u251c\u2500\u2500 model-00001-of-00030.safetensors\n...\n\u251c\u2500\u2500 model-00030-of-00030.safetensors\n\u251c\u2500\u2500 model.safetensors.index.json\n\u251c\u2500\u2500 original\n\u2502   \u251c\u2500\u2500 consolidated.00.pth\n....\n\u2502   \u251c\u2500\u2500 consolidated.07.pth\n\u2502   \u251c\u2500\u2500 params.json\n\u2502   \u2514\u2500\u2500 tokenizer.model\n\u251c\u2500\u2500 special_tokens_map.json\n\u251c\u2500\u2500 tokenizer.json\n\u2514\u2500\u2500 tokenizer_config.json\n"})}),"\n",(0,s.jsx)(n.p,{children:"Copy over the tokenizer configs under the test case repository"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cp /fsx/ubuntu/Meta-Llama-3-70B/*token* /fsx/ubuntu/llama\n"})}),"\n",(0,s.jsx)(n.h2,{id:"convert-the-llama3-model-weights",children:"Convert the Llama3 model weights"}),"\n",(0,s.jsx)(n.p,{children:"As mentioned, NxD requires that the model checkpoints be pre-sharded based on the chosen parallel configurations (tensor, pipeline parallelism degrees). The preprocessing entails:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Saving the original checkpoint into a single binary file."}),"\n",(0,s.jsxs)(n.li,{children:["Sharding the checkpoints (binary file) using the provided ",(0,s.jsx)(n.code,{children:"convert_checkpoints.py"})," ",(0,s.jsx)(n.a,{href:"https://github.com/aws-neuron/neuronx-distributed/blob/main/examples/training/llama/convert_checkpoints.py",children:"utility script"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"First, let's save the original checkpoints into a single binary file."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cat > save-llama3-70B-model.py << EOF\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\"/fsx/ubuntu/Meta-Llama-3-70B\")\ntorch.save(model.state_dict(), '/fsx/ubuntu/llama-3-70b.pt')\nEOF\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Let's then run this created script using ",(0,s.jsx)(n.code,{children:"sbatch"})," on a cluster compute node (ml.trn1.32xlarge), which has enough HBM memory to be able to load the model and run the script:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'sbatch --job-name=save-checkpoints --output=logs/save-checkpoints.out \\\n       --wrap "srun python save-llama3-70B-model.py"\n'})}),"\n",(0,s.jsx)(n.p,{children:"Once this job completes, let's convert the checkpoints (i.e., shard the checkpoints):"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'mkdir -p /fsx/ubuntu/llama3_70B/pretrained_weight\nsbatch --job-name=convert-checkpoint --output=logs/convert-checkpoint.out \\\n       --wrap "\\ \n              srun python convert_checkpoints.py \\\n              --hw_backend trn1 \\\n              --tp_size 32 --pp_size 8 --n_layers 80 \\\n              --save_xser 1 \\\n              --kv_size_multiplier 4 \\\n              --qkv_linear 1 \\\n              --fuse_qkv True \\\n              --input_dir /fsx/ubuntu/llama-3-70b.pt \\\n              --output_dir /fsx/ubuntu/llama3_70B/pretrained_weight \\\n              --config /fsx/ubuntu/Meta-Llama-3-70B/config.json \\\n              --convert_from_full_state"\n'})}),"\n",(0,s.jsx)(n.p,{children:"You can track the progress by tailing your defined log file:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"tail -f logs/convert-checkpoint.out\n"})}),"\n",(0,s.jsx)(n.p,{children:"Your logs will look like"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Saving to /fsx/ubuntu/llama3_70B/pretrained_weight/model/dp_rank_00_tp_rank_00_pp_rank_00.pt\nSaving to /fsx/ubuntu/llama3_70B/pretrained_weight/model/dp_rank_00_tp_rank_00_pp_rank_01.pt\nSaving to /fsx/ubuntu/llama3_70B/pretrained_weight/model/dp_rank_00_tp_rank_00_pp_rank_02.pt\nSaving to /fsx/ubuntu/llama3_70B/pretrained_weight/model/dp_rank_00_tp_rank_00_pp_rank_03.pt\nSaving to /fsx/ubuntu/llama3_70B/pretrained_weight/model/dp_rank_00_tp_rank_00_pp_rank_04.pt\nSaving to /fsx/ubuntu/llama3_70B/pretrained_weight/model/dp_rank_00_tp_rank_00_pp_rank_05.pt\n...\n"})}),"\n",(0,s.jsxs)(n.p,{children:["At the end of this process, we will end up with 32 x 8 = 256 checkpoints. This is because ",(0,s.jsx)(n.code,{children:"convert_checkpoints.py"})," shards the model per tensor parallel and pipeline parallel dimensions."]}),"\n",(0,s.jsx)(n.p,{children:"As a sanity check:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ls /fsx/ubuntu/llama3_70B/pretrained_weight/model/dp_rank_*_tp_rank_*_pp_rank_*.pt | wc -l\n# Output should be 256\n"})}),"\n",(0,s.jsxs)(n.admonition,{title:"Parallelism Strategy:",type:"info",children:[(0,s.jsx)(n.p,{children:"Note: The sharding is done based on the hardware setup. In our case, we are running on a cluster of 16 x ml.trn1.32xlarge instances (SageMaker HyperPod SLURM cluster)."}),(0,s.jsx)(n.p,{children:"Each ml.trn1.32xlarge instance has 16 Trainium Neuron Chips (Neuron Devices). Each of these Neuron Chips has 2 NeuronCore-v2 (i.e., 2 Neuron Cores), totalling to 32 Neuron Cores per ml.trn1.32large instance, and thus 512 Neuron Cores in the entire cluster."}),(0,s.jsx)(n.hr,{}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Pipeline Parallelism"}),": Given that we have 512 Neuron Cores, and that Llama3-70b has 80 layers, we can do:"]}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"First 10 layers: Instance 1"}),"\n",(0,s.jsx)(n.li,{children:"Second 10 layers: Instance 2"}),"\n",(0,s.jsx)(n.li,{children:"..."}),"\n",(0,s.jsx)(n.li,{children:"Eighth 10 layers: Instance 8"}),"\n"]}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"=> Pipeline Parallelism = 8 (i.e., across 8 ml.trn1.32xlarge instances)"})}),(0,s.jsx)(n.hr,{}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Tensor Parallelism"}),": We split the model by layers (10) across 8 instances via Pipeline Parallelism. Within each of these instances, we can further split the layers via Tensor Parallelism, dividing the stage's parameters across 32 Neuron Cores."]}),(0,s.jsx)(n.hr,{}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Data Parallelism"}),": Since we are maintaining two replicas of the sharded models, we employ data parallelism with a degree of 2 to speed up the training process. The resultant checkpoints will be used in the next continual pre-training stage."]})]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>o});var t=a(6540);const s={},i=t.createContext(s);function r(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);