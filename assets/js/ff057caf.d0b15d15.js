"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[3581],{6219:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"eks-blueprints/training/megatron-lm/megatron-lm-readme","title":"NVIDIA Megatron-LM","description":"MegatronLM is a framework from Nvidia designed for training large language models (LLMs). We recommend reading the following papers to understand the various tuning options available:","source":"@site/docs/01-eks-blueprints/training/megatron-lm/megatron-lm-readme.md","sourceDirName":"01-eks-blueprints/training/megatron-lm","slug":"/eks-blueprints/training/megatron-lm/megatron-lm-readme","permalink":"/ai-on-sagemaker-hyperpod/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"NVIDIA Megatron-LM","sidebar_position":1,"sidebar_label":"NVIDIA Megatron-LM"},"sidebar":"tutorialSidebar","previous":{"title":"Fully Sharded Data Parallelism (FSDP)","permalink":"/ai-on-sagemaker-hyperpod/docs/eks-blueprints/training/fsdp/fully-sharded-data-parallel"},"next":{"title":"Ray Train","permalink":"/ai-on-sagemaker-hyperpod/docs/eks-blueprints/training/ray-train/ray-train-readme"}}');var s=t(4848),a=t(8453);const i={title:"NVIDIA Megatron-LM",sidebar_position:1,sidebar_label:"NVIDIA Megatron-LM"},o="Running Megatron-LM on HyperPod EKS",l={},d=[{value:"1. Preparation",id:"1-preparation",level:2},{value:"1.1 Clone the Repository",id:"11-clone-the-repository",level:3},{value:"2. Building the Container",id:"2-building-the-container",level:2},{value:"3. GPT Model Training on HyperPod EKS with Megatron-LM",id:"3-gpt-model-training-on-hyperpod-eks-with-megatron-lm",level:2},{value:"3.1 Determine Compute Resources",id:"31-determine-compute-resources",level:3},{value:"3.2 Data Preprocessing",id:"32-data-preprocessing",level:3},{value:"Step 1: Create and Apply the Data Download Job",id:"step-1-create-and-apply-the-data-download-job",level:4},{value:"Step 2: Verify Job Creation",id:"step-2-verify-job-creation",level:4},{value:"Step 3: Monitor Job Progress",id:"step-3-monitor-job-progress",level:4},{value:"Step 4: Cleanup",id:"step-4-cleanup",level:4},{value:"3.3 Distributed Training",id:"33-distributed-training",level:3},{value:"4. What&#39;s Next?",id:"4-whats-next",level:2},{value:"5. Appendix",id:"5-appendix",level:2},{value:"5.1 Benchmark Mode",id:"51-benchmark-mode",level:3},{value:"5.2 Adjust Training Steps",id:"52-adjust-training-steps",level:3},{value:"6. Kubernetes Manifests",id:"6-kubernetes-manifests",level:2},{value:"7. Directory Structure",id:"7-directory-structure",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"running-megatron-lm-on-hyperpod-eks",children:"Running Megatron-LM on HyperPod EKS"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://github.com/NVIDIA/Megatron-LM",children:"MegatronLM"})," is a framework from Nvidia designed for training large language models (LLMs). We recommend reading the following papers to understand the various tuning options available:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1909.08053",children:"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2104.04473",children:"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2205.05198",children:"Reducing Activation Recomputatio in Large Transformer Models"})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"To run a test case, follow these steps:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Prepare your environment."}),"\n",(0,s.jsx)(n.li,{children:"Build a container, download, and preprocess the data."}),"\n",(0,s.jsx)(n.li,{children:"Train the model."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"1-preparation",children:"1. Preparation"}),"\n",(0,s.jsx)(n.p,{children:"Ensure you have the following prerequisites:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A functional HyperPod EKS cluster on AWS with EFA device plugin and NVIDIA device plugin deployed. This should be installed automatically."}),"\n",(0,s.jsx)(n.li,{children:"Docker installed for building the container image."}),"\n",(0,s.jsxs)(n.li,{children:["An FSx for Lustre filesystem mounted via a persistent volume claim on ",(0,s.jsx)(n.code,{children:"/fsx"})," in EKS pods. An example of setting up FSx on EKS is available ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/fsx-csi-create.html",children:"here"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["To run distributed training jobs as described in this guide, you must also have the ",(0,s.jsx)(n.a,{href:"https://www.kubeflow.org/docs/components/training/",children:"Kubeflow Training Operator"})," installed and configured on your HyperPod EKS cluster. This should be installed automatically via our ",(0,s.jsx)(n.a,{href:"https://github.com/aws/sagemaker-hyperpod-cli/tree/main/helm_chart",children:"helm charts"}),". If not installed, please install helm charts."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"11-clone-the-repository",children:"1.1 Clone the Repository"}),"\n",(0,s.jsx)(n.p,{children:"First, clone the awesome-distributed-training repository to get access to the Dockerfile and Kubernetes manifests:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/aws-samples/awsome-distributed-training.git\ncd awsome-distributed-training/3.test_cases/megatron/megatron-lm\n"})}),"\n",(0,s.jsx)(n.h2,{id:"2-building-the-container",children:"2. Building the Container"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["You should now be in the ",(0,s.jsx)(n.code,{children:"awsome-distributed-training/3.test_cases/megatron/megatron-lm"})," directory which contains the ",(0,s.jsx)(n.code,{children:"aws-megatron-lm.Dockerfile"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Build the container image:"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"docker build -t aws-megatron-lm -f aws-megatron-lm.Dockerfile .\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsx)(n.li,{children:"Tag and push the image to your container registry:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"export AWS_REGION=us-east-1  # Set to the AWS region where your EKS cluster and ECR repository are located\nexport ACCOUNT=$(aws sts get-caller-identity --query Account --output text)\nexport REGISTRY=${ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com\nexport ECR_REPOSITORY_NAME=aws-megatron-lm\nexport REPO_URI=${REGISTRY}/${ECR_REPOSITORY_NAME}:latest\n\n# Create ECR repository if it doesn't exist\naws ecr describe-repositories --repository-names ${ECR_REPOSITORY_NAME} --region ${AWS_REGION} 2>/dev/null || \\\naws ecr create-repository --repository-name ${ECR_REPOSITORY_NAME} --region ${AWS_REGION}\n\n# Login to ECR\naws ecr get-login-password --region ${AWS_REGION} | docker login --username AWS --password-stdin ${REGISTRY}\n\ndocker tag ${ECR_REPOSITORY_NAME}:latest ${REGISTRY}/${ECR_REPOSITORY_NAME}:latest\ndocker push ${REGISTRY}/${ECR_REPOSITORY_NAME}:latest\n"})}),"\n",(0,s.jsx)(n.p,{children:"Now you are all set for distributed training with Megatron-LM on EKS! Proceed to the GPT3 training section for detailed instructions."}),"\n",(0,s.jsx)(n.h2,{id:"3-gpt-model-training-on-hyperpod-eks-with-megatron-lm",children:"3. GPT Model Training on HyperPod EKS with Megatron-LM"}),"\n",(0,s.jsxs)(n.p,{children:["Before proceeding with GPT training setup, please follow the steps described above to prepare your environment.\nThe following example assumes that you have a PVC named ",(0,s.jsx)(n.code,{children:"fsx-claim"})," and the ",(0,s.jsx)(n.code,{children:"REPO_URI"})," environment variable is exported."]}),"\n",(0,s.jsx)(n.h3,{id:"31-determine-compute-resources",children:"3.1 Determine Compute Resources"}),"\n",(0,s.jsx)(n.p,{children:"Before running the training, you need to determine the compute resources available on your EKS cluster nodes. This will help you set the correct resource limits for GPUs and EFA (Elastic Fabric Adapter) network interfaces."}),"\n",(0,s.jsx)(n.p,{children:"Export the following environment variables based on your instance type:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Example for p5.48xlarge\nexport INSTANCE_TYPE=ml.p5.48xlarge\nexport GPU_PER_NODE=8\nexport EFA_PER_NODE=32\nexport NUM_NODES=2\n"})}),"\n",(0,s.jsx)(n.p,{children:"You can refer to the following table to find the correct values for your instance type:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Instance Type"}),(0,s.jsx)(n.th,{children:"GPUs"}),(0,s.jsx)(n.th,{children:"EFA Interfaces"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"ml.p5.48xlarge"}),(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"32"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"ml.p5e.48xlarge"}),(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"32"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"ml.p5en.48xlarge"}),(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"16"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"ml.p6-b200.48xlarge"}),(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"8"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"32-data-preprocessing",children:"3.2 Data Preprocessing"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Navigate to the GPT3 manifests directory and run the following snippet to create a job container that mounts the fsx volume and downloads the input datasets and vocabulary on it:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd kubernetes/gpt3\n"})}),"\n",(0,s.jsx)(n.h4,{id:"step-1-create-and-apply-the-data-download-job",children:"Step 1: Create and Apply the Data Download Job"}),"\n",(0,s.jsxs)(n.p,{children:["Generate the ",(0,s.jsx)(n.code,{children:"getdata-job.yaml"})," manifest from the template and apply it:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"envsubst < manifests/getdata-job.yaml-template > manifests/getdata-job.yaml\nkubectl apply -f manifests/getdata-job.yaml\n"})}),"\n",(0,s.jsx)(n.h4,{id:"step-2-verify-job-creation",children:"Step 2: Verify Job Creation"}),"\n",(0,s.jsx)(n.p,{children:"List jobs to confirm creation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get jobs\n"})}),"\n",(0,s.jsxs)(n.p,{children:["You should see an entry for ",(0,s.jsx)(n.code,{children:"getdata-job"})," with information about its status, completions, and age. To get more details about the pods created by the job, run:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -l job-name=getdata-job\n"})}),"\n",(0,s.jsx)(n.p,{children:"This will show the pod(s) managed by the job. If you want to describe the job and see events or issues, use:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl describe job getdata-job\n"})}),"\n",(0,s.jsx)(n.h4,{id:"step-3-monitor-job-progress",children:"Step 3: Monitor Job Progress"}),"\n",(0,s.jsx)(n.p,{children:"Stream the logs to monitor download progress:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl logs -f job/getdata-job\n"})}),"\n",(0,s.jsx)(n.p,{children:"You should be able to see output similar to the following once the downloads have completed successfully:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"...\nSaving to: 'gpt2-merges.txt'\n\n 0K .......... .......... .......... .......... .......... 11% 19.2M 0s\n50K .......... .......... .......... .......... .......... 22% 55.9M 0s\n100K .......... .......... .......... .......... .......... 33% 57.3M 0s\n150K .......... .......... .......... .......... .......... 44% 66.1M 0s\n200K .......... .......... .......... .......... .......... 56%  106M 0s\n250K .......... .......... .......... .......... .......... 67%  132M 0s\n300K .......... .......... .......... .......... .......... 78%  139M 0s\n350K .......... .......... .......... .......... .......... 89%  133M 0s\n400K .......... .......... .......... .......... .....     100%  122M=0.007s\n\n2025-06-20 08:59:58 (62.9 MB/s) - 'gpt2-merges.txt' saved [456318/456318]\n\ntotal 940M\ndrwxr-xr-x 2 root root   33K Jun 20 09:00 .\ndrwxr-xr-x 5 root root   33K Jun 20 08:59 ..\n-rw-r--r-- 1 root root  446K Feb 18  2019 gpt2-merges.txt\n-rw-r--r-- 1 root root 1018K Feb 18  2019 gpt2-vocab.json\n-rw-r--r-- 1 root root  1.1G Jul 24  2021 oscar-1GB.jsonl\nDownload completed.\n"})}),"\n",(0,s.jsx)(n.h4,{id:"step-4-cleanup",children:"Step 4: Cleanup"}),"\n",(0,s.jsxs)(n.p,{children:["Once the job status is ",(0,s.jsx)(n.code,{children:"Completed"}),", delete the job and its pod:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl delete -f manifests/getdata-job.yaml\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Preprocess the Data"})}),"\n",(0,s.jsx)(n.p,{children:"Launch the preprocessing job to convert the downloaded data for training."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cat manifests/prepdata-job.yaml-template | envsubst > manifests/prepdata-job.yaml\nkubectl apply -f ./manifests/prepdata-job.yaml\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Check pods for ",(0,s.jsx)(n.code,{children:"prepdata-job"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -l job-name=prepdata-job\n"})}),"\n",(0,s.jsx)(n.p,{children:"Monitor the job's progress by streaming its logs:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl logs -f job/prepdata-job\n"})}),"\n",(0,s.jsx)(n.p,{children:"The expected log output from the above command should look similar to the following when preprocessing completes successfully:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"...\n-rw-r--r--  1 root root 3.4K Jun 14 02:55 pretrain_vision_classify.py\n-rw-r--r--  1 root root 3.5K Jun 14 02:55 pretrain_vision_dino.py\n-rw-r--r--  1 root root 4.8K Jun 14 02:55 pretrain_vision_inpaint.py\n-rw-r--r--  1 root root 8.2K Jun 14 02:55 pretrain_vlm.py\n-rw-r--r--  1 root root  824 Jun 14 02:55 pyproject.toml\n-rw-r--r--  1 root root 4.0K Jun 14 02:55 setup.py\ndrwxr-xr-x  8 root root  200 Jun 14 02:55 tasks\ndrwxr-xr-x  4 root root   67 Jun 14 02:55 tests\ndrwxr-xr-x  6 root root 4.0K Jun 14 02:55 tools\nData preprocessing completed.\n"})}),"\n",(0,s.jsxs)(n.p,{children:["After the job status is ",(0,s.jsx)(n.code,{children:"Completed"}),", clean up the job and its pod:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl delete -f prepdata-job.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:"Voil\xe0! The preprocessing job has finished. You are now ready to proceed to the training step."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"33-distributed-training",children:"3.3 Distributed Training"}),"\n",(0,s.jsx)(n.p,{children:"Now that the data is preprocessed, we will pretrain a GPT3 model MegatronLM.  Launch a PyTorchJob with the environment variables:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"export TENSOR_PARALLEL=8\nexport PIPELINE_PARALLEL=1\nexport NUM_LAYERS=36\nexport HIDDEN_SIZE=4096\nexport NUM_ATTENTION_HEADS=32\nexport SEQ_LENGTH=2048\nexport MAX_POSITION_EMBEDDINGS=2048\nexport MICRO_BATCH_SIZE=1\nexport GLOBAL_BATCH_SIZE=288\ncat manifests/pytorchjob.yaml-template | envsubst > manifests/pytorchjob.yaml\nkubectl apply -f ./manifests/pytorchjob.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:"The training starts running:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get pods\n"})}),"\n",(0,s.jsx)(n.p,{children:"You should see one etcd and one worker pod."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"NAME                    READY   STATUS      RESTARTS   AGE\netcd-7787559c74-wpcb9   1/1     Running     0          3m10s\nmegatron-worker-0       1/1     Running     0          3m10s\n"})}),"\n",(0,s.jsx)(n.p,{children:"Log lines describing the iterations show that the training is working properly."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl logs -f megatron-worker-0\n"})}),"\n",(0,s.jsx)(n.p,{children:"An abbreviated sample log is shown below:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"...\nusing torch.float16 for parameters ...\n------------------------uments ------------------------\naccumulate_allreduce_grads_in_fp32 .............. False\nadam_beta1 ...................................... 0.9\nadam_beta2 ...................................... 0.95\n...\n-------------------- end of arguments ---------------------\nsetting number of micro-batches to constant 288\n> building GPT2BPETokenizer tokenizer ...\n> padded vocab (size: 50257) with 943 dummy tokens (new size: 51200)\n> initializing torch distributed ...\n> initialized tensor model parallel with size 8\n> initialized pipeline model parallel with size 1\n> setting random seeds to 1234 ...\n> compiling dataset index builder ...\nmake: Entering directory '/workspace/Megatron-LM/megatron/core/datasets'\n...\ntime to initialize megatron (seconds): 15.424\n[after megatron is initialized] datetime: 2024-07-16 22:14:01\nbuilding GPT model ...\n> number of parameters on (tensor, pipeline) model parallel rank (4, 0): 941594624\n...\n> building train, validation, and test datasets ...\n> datasets target sizes (minimum size):\n    train:      146484375\n    validation: 5863680\n    test:       11520\n...\niteration        1/  508626 | consumed samples:          288 | elapsed time per iteration (ms): 255940.5 | learning rate: 0.000E+00 | global batch size:   288 | loss scale: 4294967296.0 | number of skipped iterations:   1 | number of nan iterations:   0 |\niteration        2/  508626 | consumed samples:          576 | elapsed time per iteration (ms): 243438.3 | learning rate: 0.000E+00 | global batch size:   288 | loss scale: 2147483648.0 | number of skipped iterations:   1 | number of nan iterations:   0 |\niteration        3/  508626 | consumed samples:          864 | elapsed time per iteration (ms): 243344.4 | learning rate: 0.000E+00 | global batch size:   288 | loss scale: 1073741824.0 | number of skipped iterations:   1 | number of nan iterations:   0 |\n...\n"})}),"\n",(0,s.jsx)(n.p,{children:"You can stop the training job by executing:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl delete -f ./pytorchjob.yaml\n"})}),"\n",(0,s.jsx)(n.h2,{id:"4-whats-next",children:"4. What's Next?"}),"\n",(0,s.jsxs)(n.p,{children:["The example is based on the GPT3 example from MegatronLM's ",(0,s.jsx)(n.a,{href:"https://github.com/NVIDIA/Megatron-LM/blob/main/examples/pretrain_gpt.sh",children:"repository"}),". You can modify ",(0,s.jsx)(n.code,{children:"NUM_ATTENTION_HEADS"}),", ",(0,s.jsx)(n.code,{children:"NUM_LAYERS"}),", and ",(0,s.jsx)(n.code,{children:"HIDDEN_SIZE"})," based on the Table 1 (Page 8) of the document ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2104.04473",children:"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"})," to change the model size. You can launch training for different model sizes by setting the environment variables before applying the PyTorchJob, for example: ",(0,s.jsx)(n.code,{children:"NUM_LAYERS=64 HIDDEN_SIZE=8192 NUM_ATTENTION_HEADS=48"})]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model size"}),(0,s.jsx)(n.th,{children:"Parameters"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1.7B"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"NUM_ATTENTION_HEADS=24 HIDDEN_SIZE=2304 NUM_LAYERS=24"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"3.6B"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"NUM_ATTENTION_HEADS=32 HIDDEN_SIZE=3072 NUM_LAYERS=30"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"7.5B"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"NUM_ATTENTION_HEADS=32 HIDDEN_SIZE=4096 NUM_LAYERS=36"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"18.4B"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"NUM_ATTENTION_HEADS=48 HIDDEN_SIZE=6144 NUM_LAYERS=40"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"39.1B"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"NUM_ATTENTION_HEADS=64 HIDDEN_SIZE=8192 NUM_LAYERS=48"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"76.1B"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"NUM_ATTENTION_HEADS=80 HIDDEN_SIZE=10240 NUM_LAYERS=60"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"145.6B"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"NUM_ATTENTION_HEADS=96 HIDDEN_SIZE=12288 NUM_LAYERS=80"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"310.1B"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"NUM_ATTENTION_HEADS=128 HIDDEN_SIZE=16384 NUM_LAYERS=96"})})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"5-appendix",children:"5. Appendix"}),"\n",(0,s.jsx)(n.h3,{id:"51-benchmark-mode",children:"5.1 Benchmark Mode"}),"\n",(0,s.jsxs)(n.p,{children:["To run in benchmark mode (i.e., train only, no validation and test), modify the PyTorchJob arguments in the ",(0,s.jsx)(n.code,{children:"pytorchjob.yaml-template"})," file:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-diff",children:"-        --eval-iters 40 \\\n-        --eval-interval 1000 \\\n-        --split 98,2,0 \\\n+        --eval-iters 0 \\\n+        --split 100,0,0 \\\n"})}),"\n",(0,s.jsx)(n.p,{children:"Incorrect settings will cause this error message to appear in the training logs:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'Traceback (most recent call last):\n  File "/workspace/Megatron-LM/pretrain_gpt.py", line 198, in <module>\n    pretrain(train_valid_test_datasets_provider,\n  File "/workspace/Megatron-LM/megatron/training.py", line 227, in pretrain\n    = build_train_valid_test_data_iterators(\n  File "/workspace/Megatron-LM/megatron/training.py", line 1283, in build_train_valid_test_data_iterators\n    build_train_valid_test_data_loaders(\n  File "/workspace/Megatron-LM/megatron/training.py", line 1244, in build_train_valid_test_data_loaders\n    train_ds, valid_ds, test_ds = build_train_valid_test_datasets(\n  File "/workspace/Megatron-LM/megatron/training.py", line 1214, in build_train_valid_test_datasets\n    return build_train_valid_test_datasets_provider(train_val_test_num_samples)\n  File "/workspace/Megatron-LM/pretrain_gpt.py", line 186, in train_valid_test_datasets_provider\n    ).build()\n  File "/workspace/Megatron-LM/megatron/core/datasets/blended_megatron_dataset_builder.py", line 56, in build\n    return self._build_blended_dataset_splits()\n  File "/workspace/Megatron-LM/megatron/core/datasets/blended_megatron_dataset_builder.py", line 76, in _build_blended_dataset_splits\n    return self._build_megatron_dataset_splits(blend[0], split, self.sizes)\n  File "/workspace/Megatron-LM/megatron/core/datasets/blended_megatron_dataset_builder.py", line 216, in _build_megatron_dataset_splits\n    self.build_generic_dataset(\n  File "/workspace/Megatron-LM/megatron/core/datasets/blended_megatron_dataset_builder.py", line 258, in build_generic_dataset\n    dataset = cls(*args)\n  File "/workspace/Megatron-LM/megatron/core/datasets/gpt_dataset.py", line 68, in __init__\n    super().__init__(indexed_dataset, indexed_indices, num_samples, index_split, config)\n  File "/workspace/Megatron-LM/megatron/core/datasets/megatron_dataset.py", line 42, in __init__\n    assert num_samples > 0\nAssertionError\n'})}),"\n",(0,s.jsx)(n.h3,{id:"52-adjust-training-steps",children:"5.2 Adjust Training Steps"}),"\n",(0,s.jsxs)(n.p,{children:["By default, the PyTorchJob specifies the number of samples, then the number of training steps equals to ",(0,s.jsx)(n.code,{children:"--train_samples"})," / ",(0,s.jsx)(n.code,{children:"--global-batch-size"}),". To directly specify the number of steps, modify the arguments in the ",(0,s.jsx)(n.code,{children:"pytorchjob.yaml-template"})," file. Note that ",(0,s.jsx)(n.code,{children:"samples"})," and ",(0,s.jsx)(n.code,{children:"iters"})," are mutually exclusive."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-diff",children:"-        --train-samples 146484375 \\\n-        --lr-decay-samples 126953125 \\\n-        --lr-warmup-samples 183105 \\\n+        --train-iters 50 \\\n+        --lr-decay-iters 45 \\\n+        --lr-warmup-iters 2 \\\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Following the same pattern, you can train other models. Pretraining scripts for models like\nBert, ICT, and T5 are already included in the Megatron-LM container under ",(0,s.jsx)(n.code,{children:"/workspace/Megatron-LM"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"6-kubernetes-manifests",children:"6. Kubernetes Manifests"}),"\n",(0,s.jsxs)(n.p,{children:["The training setup uses three main Kubernetes manifest templates located in the ",(0,s.jsx)(n.code,{children:"kubernetes/gpt3/manifests/"})," directory of the cloned repository:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"getdata-job.yaml-template"})})," - Downloads training data and vocabulary files"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"prepdata-job.yaml-template"})})," - Preprocesses data for training"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"pytorchjob.yaml-template"})})," - Runs distributed training using PyTorchJob"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:["The manifest templates use environment variable substitution with ",(0,s.jsx)(n.code,{children:"envsubst"}),". Make sure all required environment variables are exported before generating the final manifests."]})}),"\n",(0,s.jsx)(n.h2,{id:"7-directory-structure",children:"7. Directory Structure"}),"\n",(0,s.jsx)(n.p,{children:"After cloning the repository, your directory structure should look like this:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"awsome-distributed-training/\n\u2514\u2500\u2500 3.test_cases/\n    \u2514\u2500\u2500 megatron/\n        \u2514\u2500\u2500 megatron-lm/\n            \u251c\u2500\u2500 aws-megatron-lm.Dockerfile\n            \u251c\u2500\u2500 README.md\n            \u251c\u2500\u2500 kubernetes/\n            \u2502   \u251c\u2500\u2500 README.md\n            \u2502   \u2514\u2500\u2500 gpt3/\n            \u2502       \u251c\u2500\u2500 README.md\n            \u2502       \u2514\u2500\u2500 manifests/\n            \u2502           \u251c\u2500\u2500 getdata-job.yaml-template\n            \u2502           \u251c\u2500\u2500 prepdata-job.yaml-template\n            \u2502           \u2514\u2500\u2500 pytorchjob.yaml-template\n            \u2514\u2500\u2500 slurm/ \n"})}),"\n",(0,s.jsxs)(n.p,{children:["For additional examples and configurations, refer to the ",(0,s.jsx)(n.a,{href:"https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/megatron/megatron-lm",children:"awesome-distributed-training repository"})," and the ",(0,s.jsx)(n.a,{href:"https://github.com/NVIDIA/Megatron-LM",children:"Megatron-LM GitHub repository"}),"."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var r=t(6540);const s={},a=r.createContext(s);function i(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);