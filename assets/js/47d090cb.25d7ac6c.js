"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[2236],{2209:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/NVIDIA-GPU-Docker-cc5ed9aa97c55c99efcec9376d9be7c4.png"},3673:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"Tips/Slurm/containers","title":"Containers","description":"Containers","source":"@site/docs/08-Tips/Slurm/02-containers.md","sourceDirName":"08-Tips/Slurm","slug":"/Tips/Slurm/containers","permalink":"/docs/Tips/Slurm/containers","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Containers","weight":42},"sidebar":"tutorialSidebar","previous":{"title":"Receive cluster status/health events","permalink":"/docs/Tips/EKS/event-bridge"},"next":{"title":"Troubleshooting","permalink":"/docs/Tips/Slurm/troubleshooting"}}');var o=i(4848),r=i(8453);const s={title:"Containers",weight:42},a=void 0,c={},l=[{value:"What are containers?",id:"what-are-containers",level:2},{value:"Containers in Slurm",id:"containers-in-slurm",level:2},{value:"Containers on SageMaker HyperPod",id:"containers-on-sagemaker-hyperpod",level:2}];function d(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",em:"em",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Containers",src:i(2209).A+"",width:"522",height:"369"})}),"\n",(0,o.jsx)(n.h2,{id:"what-are-containers",children:"What are containers?"}),"\n",(0,o.jsxs)(n.p,{children:["Containers are a great way to package software, they wrap the runtime of the software up with the application's code. This allows you to pull down optimized software containers and run them out of the box without all the complications of compiling them for a new system. In this blog we'll focus on the ",(0,o.jsx)(n.a,{href:"https://catalog.ngc.nvidia.com/containers",children:"nvidia container repository (ngc)"})," since they have optimized containers for applications like ",(0,o.jsx)(n.a,{href:"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch",children:"pytorch"}),", ",(0,o.jsx)(n.a,{href:"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo",children:"nemo"}),", and ",(0,o.jsx)(n.a,{href:"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo_bert_text_classification",children:"BERT"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"containers-in-slurm",children:"Containers in Slurm"}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsx)(n.p,{children:"What are the issues with using docker containers in traditional Slurm clusters?"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Containers typically require a privileged runtime, i.e. the person invoking the container needs sudo access. This is a problem for Slurm clusters which are typically multi-user environments with POSIX file permissions used to give access to certain files and directories based on ",(0,o.jsx)(n.em,{children:"users and groups"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["To solve this Nvidia published ",(0,o.jsx)(n.a,{href:"https://github.com/NVIDIA/enroot",children:"enroot"})," - this uses the linux kernel feature ",(0,o.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Chroot",children:"chroot(1)"})," to create an isolated runtime environment for the container. Think of this like creating a mount point ",(0,o.jsx)(n.code,{children:"/tmp/container"})," in which the container can only see it's local directory i.e. ",(0,o.jsx)(n.code,{children:"container/"}),". This serves to separate the outside OS from the container's runtime. Here's an example of enroot in action:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Import and start an Amazon Linux image from DockerHub\nenroot import docker://amazonlinux:latest\nenroot create amazonlinux+latest.sqsh\nenroot start amazonlinux+latest\n"})}),"\n",(0,o.jsxs)(n.p,{children:["In the above example we imported a container from dockerhub, converted it to an enroot container with ",(0,o.jsx)(n.code,{children:"enroot create"})," and then ran it with ",(0,o.jsx)(n.code,{children:"enroot start"}),"."]}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsx)(n.p,{children:"So how do you schedule and run containers with Slurm?"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Slurm provides a ",(0,o.jsx)(n.a,{href:"https://slurm.schedmd.com/containers.html",children:"container capability"})," for OCI containers that's half baked. It requires users to pull down their container images, convert them to an OCI runtime, then point Slurm at that OCI image. To solve this, Nvidia introduced ",(0,o.jsx)(n.a,{href:"https://github.com/NVIDIA/pyxis",children:"Pyxis"})," which is a plugin for Slurm that allows you to run containers using the native OCI runtime capabilities and only specifying the container uri, i.e. ",(0,o.jsx)(n.code,{children:"amazonlinux/latest"}),". An example of this is like so:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"#!/bin/bash\n#SBATCH --container-image nvcr.io\\#nvidia/pytorch:21.12-py3\n\npython -c 'import torch ; print(torch.__version__)'\n"})}),"\n",(0,o.jsx)(n.p,{children:"Pretty cool right?"}),"\n",(0,o.jsx)(n.h2,{id:"containers-on-sagemaker-hyperpod",children:"Containers on SageMaker HyperPod"}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsx)(n.p,{children:"So how do we set this all up with SageMaker HyperPod?"}),"\n"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["We've already setup docker, pyxis and enroot as part of the ",(0,o.jsx)(n.a,{href:"https://github.com/aws-samples/awsome-distributed-training/blob/main/1.architectures/5.sagemaker-hyperpod/LifecycleScripts/base-config/lifecycle_script.py#L155",children:"lifecycle scripts"}),". You should have both docker and enroot in your path:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"docker --help\nenroot --help\n"})}),"\n",(0,o.jsxs)(n.admonition,{type:"info",children:[(0,o.jsxs)(n.p,{children:["If you see the following error ",(0,o.jsx)(n.code,{children:"ERROR: permission denied while trying to connect to the Docker daemon socket at..."})," when trying to run ",(0,o.jsx)(n.code,{children:"docker"}),", you'll need to add the user to the ",(0,o.jsx)(n.code,{children:"docker"})," group by running:"]}),(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"sudo usermod -aG docker ${USER}\n"})}),(0,o.jsxs)(n.p,{children:["Then log out with ",(0,o.jsx)(n.code,{children:"exit"})," and log back in."]})]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["We can test that pyxis and enroot installed correctly by running the ",(0,o.jsx)(n.a,{href:"https://catalog.ngc.nvidia.com/orgs/nvidia/containers/cuda",children:"cuda:11.6.2"})," ubuntu image:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"srun --container-image=nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi\n"})}),"\n",(0,o.jsx)(n.admonition,{type:"info",children:(0,o.jsxs)(n.p,{children:["If you see the following error ",(0,o.jsx)(n.code,{children:"[WARN] Kernel module nvidia_uvm is not loaded. Make sure the NVIDIA device driver is installed and loaded."})," make sure you're on ",(0,o.jsx)(n.strong,{children:"Nvidia GPU cluster"}),". this won't work with ",(0,o.jsx)(n.code,{children:"trn1.32xlarge"})," instances."]})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"[ec2-user@ip-172-31-28-27 ~]$ srun --container-image=nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi\npyxis: importing docker image: nvidia/cuda:11.6.2-base-ubuntu20.04\npyxis: imported docker image: nvidia/cuda:11.6.2-base-ubuntu20.04\nTue May 16 22:07:21 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n| N/A   40C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["This can also be run in a sbatch script like so. Think of this as the equivalent of ",(0,o.jsx)(n.code,{children:"docker run nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi"})," on the compute node."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"#!/bin/bash\n#SBATCH --container-image=nvidia/cuda:11.6.2-base-ubuntu20.04\n\nnvidia-smi\n"})}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.admonition,{type:"info",children:[(0,o.jsxs)(n.p,{children:["It is possible that you get an error that looks like ",(0,o.jsx)(n.code,{children:"srun: unrecognized option '--container-image'"}),". To fix this, you need to run:"]}),(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"NUM_NODES= <Replace with the number of compute nodes>\nsrun -N $NUM_NODES sudo scontrol reconfigure\n"})})]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var t=i(6540);const o={},r=t.createContext(o);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);