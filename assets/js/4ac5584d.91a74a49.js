"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5841],{1676:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/recipes-adapter-34bb424590865c4e8cc2ddace8c0baa3.png"},7072:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>l,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"hyperpod-recipes/index.en","title":"SageMaker HyperPod Recipes Overview","description":"Amazon SageMaker HyperPod recipes help you get started with training and fine-tuning publicly available foundation models. The recipes provide a pre-packaged set of training stack configurations that enable state-of-art training performance on SageMaker HyperPod. You can also easily switch between GPU-based instances and TRN-based instances with a simple recipe change.","source":"@site/docs/03-hyperpod-recipes/index.en.md","sourceDirName":"03-hyperpod-recipes","slug":"/hyperpod-recipes/index.en","permalink":"/docs/hyperpod-recipes/index.en","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"SageMaker HyperPod Recipes Overview","sidebar_position":1,"weight":110},"sidebar":"tutorialSidebar","previous":{"title":"Sagemaker Hyperpod Recipes","permalink":"/docs/category/sagemaker-hyperpod-recipes"},"next":{"title":"Setup and Launch training - EKS","permalink":"/docs/hyperpod-recipes/eks-hyperpod-recipes/hyperpod-recipes"}}');var t=i(4848),a=i(8453);const s={title:"SageMaker HyperPod Recipes Overview",sidebar_position:1,weight:110},o=void 0,d={},c=[{value:"Verified instance types, instance counts",id:"verified-instance-types-instance-counts",level:2},{value:"Supported Models",id:"supported-models",level:2},{value:"Pre-Training",id:"pre-training",level:3},{value:"Fine-Tuning",id:"fine-tuning",level:3}];function p(e){const n={a:"a",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"Amazon SageMaker HyperPod recipes help you get started with training and fine-tuning publicly available foundation models. The recipes provide a pre-packaged set of training stack configurations that enable state-of-art training performance on SageMaker HyperPod. You can also easily switch between GPU-based instances and TRN-based instances with a simple recipe change."}),"\n",(0,t.jsx)(n.p,{children:"The recipes are pre-configured training configurations for the following model families:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Llama 3.1"}),"\n",(0,t.jsx)(n.li,{children:"Llama 3.2"}),"\n",(0,t.jsx)(n.li,{children:"Mistral"}),"\n",(0,t.jsx)(n.li,{children:"Mixtral"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["To run the recipes within SageMaker HyperPod you use the Amazon SageMaker HyperPod training adapter as the framework to help you run end-to-end training workflows. The training adapter is built on ",(0,t.jsx)(n.a,{href:"https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html",children:"NVIDIA NeMo framework"})," and ",(0,t.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/index.html",children:"Neuronx Distributed Training"})," package. If you\u2019re familiar with using NeMo, the process of using the training adapter is the same. The training adapter runs the recipe on your cluster."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"HyperPod recipes",src:i(1676).A+"",width:"1208",height:"1037"})}),"\n",(0,t.jsx)(n.p,{children:"You can also train your own model by defining your own custom recipe."}),"\n",(0,t.jsx)(n.h2,{id:"verified-instance-types-instance-counts",children:"Verified instance types, instance counts"}),"\n",(0,t.jsx)(n.p,{children:"P5.48xlarge"}),"\n",(0,t.jsx)(n.h2,{id:"supported-models",children:"Supported Models"}),"\n",(0,t.jsx)(n.h3,{id:"pre-training",children:"Pre-Training"}),"\n",(0,t.jsxs)(n.p,{children:["Please refer to the link ",(0,t.jsx)(n.a,{href:"https://github.com/aws/sagemaker-hyperpod-recipes?tab=readme-ov-file#pre-training",children:"here"})," for the list of supported model configs for Pre training."]}),"\n",(0,t.jsx)(n.h3,{id:"fine-tuning",children:"Fine-Tuning"}),"\n",(0,t.jsxs)(n.p,{children:["Please refer to the link ",(0,t.jsx)(n.a,{href:"https://github.com/aws/sagemaker-hyperpod-recipes?tab=readme-ov-file#fine-tuning",children:"here"})," for the list of supported model configs for fine tuning."]})]})}function l(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>o});var r=i(6540);const t={},a=r.createContext(t);function s(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);