"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[4954],{8453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>r});var s=n(6540);const a={},i=s.createContext(a);function o(e){const t=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(i.Provider,{value:t},e.children)}},9713:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"slurm-blueprints/training/trainium/Llama3-70B/download-dataset","title":"Downloading the Wiki-corpus datasets","description":"In this section, we will download and preprocess the wiki-corpus and tokenize it for training.","source":"@site/docs/02-slurm-blueprints/training/trainium/Llama3-70B/03-download-dataset.md","sourceDirName":"02-slurm-blueprints/training/trainium/Llama3-70B","slug":"/slurm-blueprints/training/trainium/Llama3-70B/download-dataset","permalink":"/docs/slurm-blueprints/training/trainium/Llama3-70B/download-dataset","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Downloading the Wiki-corpus datasets","sidebar_position":4,"weight":33},"sidebar":"tutorialSidebar","previous":{"title":"Downloading the Llama3-70b model","permalink":"/docs/slurm-blueprints/training/trainium/Llama3-70B/download-model"},"next":{"title":"Running Continual Pre-training with NeuronX Distributed","permalink":"/docs/slurm-blueprints/training/trainium/Llama3-70B/training"}}');var a=n(4848),i=n(8453);const o={title:"Downloading the Wiki-corpus datasets",sidebar_position:4,weight:33},r=void 0,l={},d=[];function c(e){const t={a:"a",code:"code",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(t.p,{children:["In this section, we will download and preprocess the ",(0,a.jsx)(t.a,{href:"https://www.corpusdata.org/wikipedia.asp",children:"wiki-corpus"})," and tokenize it for training."]}),"\n",(0,a.jsxs)(t.p,{children:["We will utilize the ",(0,a.jsx)(t.code,{children:"get_dataset.py"})," script inside the same ",(0,a.jsx)(t.code,{children:"llama"})," sample directory. Again, we use ",(0,a.jsx)(t.code,{children:"sbatch"})," to submit the data processing job to the cluster."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:'sbatch --job-name=get-dataset --output=logs/get-dataset.out \\\n       --wrap "srun python3 get_dataset.py --llama-version 3"\n'})}),"\n",(0,a.jsx)(t.p,{children:"You can tail the logs:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"tail -f logs/get-dataset.out\n"})}),"\n",(0,a.jsx)(t.p,{children:"An example output:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"$ python get_dataset.py --llama-version 3\nThe repository for wikicorpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikicorpus.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N] y\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.35G/1.35G [03:02<00:00, 7.36MB/s]\nGenerating train split: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1359146/1359146 [01:12<00:00, 18644.90 examples/s]\nRunning tokenizer on dataset:  37%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                         | 497000/1359146 [02:28<04:18, 3341.01 examples/s]\nToken indices sequence length is longer than the specified maximum sequence length for this model (172677 > 131072). Running this sequence through the model will result in indexing errors\nRunning tokenizer on dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1359146/1359146 [06:45<00:00, 3352.65 examples/s]\nGrouping texts in chunks of 8192: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1359146/1359146 [09:48<00:00, 2308.18 examples/s]\n94025\nSaving the dataset (21/21 shards): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 94025/94025 [00:18<00:00, 4951.30 examples/s]\n"})}),"\n",(0,a.jsxs)(t.p,{children:["The resultant data is saved under ",(0,a.jsx)(t.code,{children:"/fsx/ubuntu/example_datasets"}),":"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"/fsx/ubuntu/examples_datasets/wikicorpus_llama3_tokenized_8k/\n\u251c\u2500\u2500 data-00000-of-00021.arrow\n...\n\u251c\u2500\u2500 data-00020-of-00021.arrow\n\u251c\u2500\u2500 dataset_info.json\n\u2514\u2500\u2500 state.json\n"})}),"\n",(0,a.jsx)(t.p,{children:"We're now ready for continual pre-training!"})]})}function u(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);