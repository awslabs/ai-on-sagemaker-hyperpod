"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[9840],{313:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/open-file-da21c8b00df91abb247663311bf7fd94.png"},584:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/serve-dashboard-68145b7863d632f4dd5ecf813d82480b.png"},5238:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/viu-image-51eb2300eaa78c887448c53e9af5af3b.png"},7341:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/serve-dashboard-tui-484cbba945741762833df04185234cc6.png"},7366:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/dashboard-cd5cb6ac407349ca796f6465597b3758.png"},8215:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"eks-blueprints/inference/ray-service/ray-service-readme","title":"Serving Stable Diffusion Model for Inference with Ray Serve","description":"Ray Serve is a scalable model serving library for building online inference APIs. Serve is framework-agnostic, so you can use a single toolkit to serve everything from deep learning models built with frameworks like PyTorch, TensorFlow, and Keras, to Scikit-Learn models, to arbitrary Python business logic.","source":"@site/docs/01-eks-blueprints/inference/ray-service/ray-service-readme.md","sourceDirName":"01-eks-blueprints/inference/ray-service","slug":"/eks-blueprints/inference/ray-service/ray-service-readme","permalink":"/ai-on-sagemaker-hyperpod/docs/eks-blueprints/inference/ray-service/ray-service-readme","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Mistral 7B with Load Balancer","permalink":"/ai-on-sagemaker-hyperpod/docs/eks-blueprints/inference/load-balancer-inference/inference-with-loadbalancer"},"next":{"title":"SLURM Blueprints","permalink":"/ai-on-sagemaker-hyperpod/docs/category/slurm-blueprints"}}');var i=s(4848),a=s(8453);const o={},t="Serving Stable Diffusion Model for Inference with Ray Serve",l={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"1. Create a RayService",id:"1-create-a-rayservice",level:2},{value:"1.1 Download Required Code",id:"11-download-required-code",level:3},{value:"1.2 Explore Stable Diffusion GPU Files",id:"12-explore-stable-diffusion-gpu-files",level:3},{value:"1.3 Deploy the RayService",id:"13-deploy-the-rayservice",level:3},{value:"2. Access Ray Dashboard (Optional)",id:"2-access-ray-dashboard-optional",level:2},{value:"Option A: Port-forward Locally",id:"option-a-port-forward-locally",level:3},{value:"Option B: Deploy Load Balancer",id:"option-b-deploy-load-balancer",level:3},{value:"3. Inference",id:"3-inference",level:2},{value:"3.1 Set Up Port Forwarding",id:"31-set-up-port-forwarding",level:3},{value:"3.2 Send Inference Request",id:"32-send-inference-request",level:3},{value:"3.3 View Generated Images",id:"33-view-generated-images",level:3},{value:"Option 1: VIU Tool (Terminal Viewer)",id:"option-1-viu-tool-terminal-viewer",level:4},{value:"Option 2: SageMaker Code Editor",id:"option-2-sagemaker-code-editor",level:4},{value:"4. Cleanup",id:"4-cleanup",level:2},{value:"What&#39;s Next?",id:"whats-next",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"serving-stable-diffusion-model-for-inference-with-ray-serve",children:"Serving Stable Diffusion Model for Inference with Ray Serve"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://docs.ray.io/en/latest/serve/index.html",children:"Ray Serve"})," is a scalable model serving library for building online inference APIs. Serve is framework-agnostic, so you can use a single toolkit to serve everything from deep learning models built with frameworks like PyTorch, TensorFlow, and Keras, to Scikit-Learn models, to arbitrary Python business logic."]}),"\n",(0,i.jsx)(n.p,{children:"A Ray Serve manages these components:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"RayCluster"}),": Manages resources in a Kubernetes cluster"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Ray Serve Applications"}),": Manages users' applications"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.p,{children:"Before proceeding, ensure you have:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A functional HyperPod EKS cluster with GPU nodes"}),"\n",(0,i.jsxs)(n.li,{children:["Ray cluster deployed (see ",(0,i.jsx)(n.a,{href:"/ai-on-sagemaker-hyperpod/docs/eks-blueprints/training/ray-train/ray-train-readme",children:"Ray Train documentation"}),")"]}),"\n",(0,i.jsx)(n.li,{children:"Access to the aws-do-ray repository"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"1-create-a-rayservice",children:"1. Create a RayService"}),"\n",(0,i.jsx)(n.h3,{id:"11-download-required-code",children:"1.1 Download Required Code"}),"\n",(0,i.jsx)(n.p,{children:"Ensure you have aws-do-ray downloaded:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/aws-samples/aws-do-ray.git\n"})}),"\n",(0,i.jsx)(n.p,{children:"Navigate to the Ray service directory:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd aws-do-ray/Container-Root/ray/rayservice/\nls\n"})}),"\n",(0,i.jsxs)(n.p,{children:["As you can see, there are multiple models we can serve for inference. For this workshop we will be deploying the ",(0,i.jsx)(n.a,{href:"https://docs.ray.io/en/latest/serve/tutorials/stable-diffusion.html",children:"Stable Diffusion"})," model."]}),"\n",(0,i.jsx)(n.h3,{id:"12-explore-stable-diffusion-gpu-files",children:"1.2 Explore Stable Diffusion GPU Files"}),"\n",(0,i.jsx)(n.p,{children:"Navigate to the Stable Diffusion GPU directory:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd stable_diffusion_gpu\nls -alh\n"})}),"\n",(0,i.jsx)(n.p,{children:"In here we have 3 files:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"rayservice.stable_diffusion_gpu.yaml"})}),": Ray Service YAML configuration"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Here is more information regarding the configuration for Ray Serve applications in the YAML. More details ",(0,i.jsx)(n.a,{href:"https://docs.ray.io/en/latest/serve/production-guide/kubernetes.html",children:"here"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"applications"}),": A list of applications to be deployed"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"name"}),": The name of the application, in this case, ",(0,i.jsx)(n.code,{children:"stable-diffusion-gpu"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"import_path"}),": The import path for the application's module, ",(0,i.jsx)(n.code,{children:"stable_diffusion_gpu:entrypoint"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"route_prefix"}),": The route prefix for accessing the application"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"runtime_env"}),": Specifies the runtime environment for the application"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"working_dir"}),": The working directory for the application, must point to a zip file locally or through a URI"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"pip"}),": A list of Python packages to be installed in the runtime environment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"deployments"}),": A list of deployments for the application"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"name"}),": The name of the deployment, ",(0,i.jsx)(n.code,{children:"stable-diffusion-gpu"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"num_replicas"}),": The number of replicas for the deployment, set to 1"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ray_actor_options"}),": Options for the Ray actors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"num_gpus"}),": The number of GPUs allocated for each actor, set to 1"]}),"\n"]}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"stable_diffusion_gpu.py"})}),": Python code for processing stable diffusion requests"]}),"\n"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scalable Deployment"}),": Ray Serve dynamically scales the APIIngress and StableDiffusionV2 components based on traffic, managing resources efficiently by adjusting the number of replicas"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Asynchronous Execution"}),": Using ",(0,i.jsx)(n.code,{children:".remote()"}),", Ray handles multiple requests concurrently, distributing tasks across the cluster for responsive API performance"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GPU Resource Management"}),": Ray ensures each model replica has a dedicated GPU and optimizes performance through mixed-precision, making inference faster and memory-efficient"]}),"\n"]}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"stable_diffusion_req.py"})}),": Python code to send stable diffusion requests"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"13-deploy-the-rayservice",children:"1.3 Deploy the RayService"}),"\n",(0,i.jsx)(n.p,{children:"To create a Ray Service deployment, run:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f rayservice.stable_diffusion_gpu.yaml\n"})}),"\n",(0,i.jsx)(n.p,{children:"As our cluster is deploying, we will wait for the RayService Kubernetes service, which is created after the Serve applications are ready and running. This process may take approximately 1 minute after all pods in the RayCluster are running."}),"\n",(0,i.jsx)(n.p,{children:"To check pods:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get pods\n"})}),"\n",(0,i.jsx)(n.h2,{id:"2-access-ray-dashboard-optional",children:"2. Access Ray Dashboard (Optional)"}),"\n",(0,i.jsx)(n.p,{children:"We will provide two ways through which the Ray dashboard can be accessed."}),"\n",(0,i.jsx)(n.h3,{id:"option-a-port-forward-locally",children:"Option A: Port-forward Locally"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Open a new bash shell"})," by using the ",(0,i.jsx)(n.code,{children:"+"})," button in the upper-right corner of your terminal, then execute the following command block to display the dashboard using a terminal-based browser:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward svc/$(kubectl get svc | grep stable-diffusion-gpu-raycluster | grep head | cut -d ' ' -f 1) 8265 &\n"})}),"\n",(0,i.jsxs)(n.p,{children:["You can now access the dashboard locally at ",(0,i.jsx)(n.code,{children:"https://localhost:8265"})," or, if you are using SageMaker Studio which won't allow you to access these ports locally, you can use ",(0,i.jsx)(n.a,{href:"https://github.com/fathyb/carbonyl",children:(0,i.jsx)(n.code,{children:"fathyb/carbonyl"})})," image which will transform your terminal into a browser:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"docker run $DOCKER_NETWORK --rm -it fathyb/carbonyl http://localhost:8265\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Terminal-based Ray Dashboard",src:s(7341).A+"",width:"1774",height:"950"})}),"\n",(0,i.jsx)(n.p,{children:"To exit the browser, just close the shell window."}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["If you are having issues port-forwarding, please run: ",(0,i.jsx)(n.code,{children:'pkill -f "kubectl port-forward"'}),", then retry."]})}),"\n",(0,i.jsx)(n.h3,{id:"option-b-deploy-load-balancer",children:"Option B: Deploy Load Balancer"}),"\n",(0,i.jsxs)(n.p,{children:["Before we can use an ingress object, we must deploy the Load Balancer Controller to the cluster. Please follow the instructions in the ",(0,i.jsx)(n.a,{href:"/ai-on-sagemaker-hyperpod/docs/eks-blueprints/inference/load-balancer-inference/inference-with-loadbalancer",children:"Load Balancer Inference section"}),", then return here to create the ingress object."]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Follow load balancer setup steps"})," from the Load Balancer Controller documentation"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Create an Ingress template"}),":"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Retrieves name of head pod service\nexport HEAD_SERVICE=$(kubectl get service --all-namespaces -o jsonpath=\'{range .items[*]}{.metadata.name}{"\\n"}{end}\' | grep "raycluster")\n\nexport PUBLIC_SUBNETS=$(aws ec2 describe-subnets --filters "[ {\\"Name\\":\\"vpc-id\\",\\"Values\\":[\\"${VPC_ID}\\"]}, {\\"Name\\":\\"map-public-ip-on-launch\\",\\"Values\\":[\\"true\\"]} ]" --query \'Subnets[*].{SubnetId:SubnetId}\' --output text)\n\nexport SUBNET1=$(echo $PUBLIC_SUBNETS | cut -d \' \' -f 1)\n\nexport SUBNET2=$(echo $PUBLIC_SUBNETS | cut -d \' \' -f 2)\n\n\ncat << \'EOF\' > ray-dashboard.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ray-cluster-ingress\n  annotations:\n    # WARNING: Do not expose this ALB publicly without additional authentication/authorization.\n    # The Ray Dashboard provides read and write access to the cluster. Anyone with access to the\n    # ALB can launch arbitrary code execution on the Ray Cluster.\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    # See ingress.md for more details about how to choose subnets.\n    alb.ingress.kubernetes.io/subnets: ${SUBNET1}, ${SUBNET2}\n    alb.ingress.kubernetes.io/target-type: ip\n    # Restrict access to your IP address only (replace with your actual IP)\n    # alb.ingress.kubernetes.io/inbound-cidrs: YOUR_IP_ADDRESS/32\nspec:\n  ingressClassName: alb\n  rules:\n    - http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: ${HEAD_SERVICE} # Your head pod service\n                port:\n                  number: 8265\nEOF\n'})}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:["For security reasons, it's recommended to uncomment ",(0,i.jsx)(n.code,{children:"alb.ingress.kubernetes.io/inbound-cidrs: YOUR_IP_ADDRESS/32"})," and replace ",(0,i.jsx)(n.code,{children:"YOUR_IP_ADDRESS"})," with your IP to limit traffic to only allow for your IP."]})}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Generate and apply the Ingress manifest"}),":"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"envsubst < ray-dashboard.yaml | kubectl apply -f -\nkubectl get ingress\n"})}),"\n",(0,i.jsx)(n.p,{children:"It takes about 6 minutes for the load balancer to be created and become active. Then you can paste the address you find on the ingress into your local browser to access the Ray dashboard."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"NAME CLASS HOSTS ADDRESS PORTS AGE\nray-cluster-ingress alb * k8s-default-rayclust-14ceb20792-1681229536.us-west-2.elb.amazonaws.com 80 5m52s\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Ray Dashboard",src:s(7366).A+"",width:"862",height:"456"})}),"\n",(0,i.jsxs)(n.p,{children:["Please visit the ",(0,i.jsx)(n.code,{children:"Serve"})," Tab to see your application:"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Ray Serve Dashboard",src:s(584).A+"",width:"1848",height:"995"})}),"\n",(0,i.jsx)(n.p,{children:"To clean up the load balancer, remove the Ingress object:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl delete ingress ray-cluster-ingress\n"})}),"\n",(0,i.jsx)(n.h2,{id:"3-inference",children:"3. Inference"}),"\n",(0,i.jsx)(n.h3,{id:"31-set-up-port-forwarding",children:"3.1 Set Up Port Forwarding"}),"\n",(0,i.jsx)(n.p,{children:"To check for the Kubernetes service:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get services\n"})}),"\n",(0,i.jsxs)(n.p,{children:["When you see the service ",(0,i.jsx)(n.code,{children:"stable-diffusion-gpu-serve-svc"}),", we can now forward the port for Stable Diffusion query:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward svc/stable-diffusion-gpu-serve-svc 8000 > /dev/null 2>&1 &\n"})}),"\n",(0,i.jsx)(n.h3,{id:"32-send-inference-request",children:"3.2 Send Inference Request"}),"\n",(0,i.jsx)(n.p,{children:"Now let's send a query request."}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["If you'd like to change the prompt request, please enter ",(0,i.jsx)(n.code,{children:"vi stable_diffusion_gpu/stable_diffusion_gpu_req.py"})," and change the variable ",(0,i.jsx)(n.code,{children:"prompt"})," to your request."]})}),"\n",(0,i.jsx)(n.p,{children:"To send query request, please run:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python3 stable_diffusion_gpu_req.py\n"})}),"\n",(0,i.jsx)(n.p,{children:"On GPU, a new image is created within 10 seconds (compared to about 5 minutes on CPU)."}),"\n",(0,i.jsx)(n.h3,{id:"33-view-generated-images",children:"3.3 View Generated Images"}),"\n",(0,i.jsx)(n.p,{children:"Now we have a few options to view the image:"}),"\n",(0,i.jsx)(n.h4,{id:"option-1-viu-tool-terminal-viewer",children:"Option 1: VIU Tool (Terminal Viewer)"}),"\n",(0,i.jsxs)(n.p,{children:["A fun way to view the image output, we can use the ",(0,i.jsx)(n.a,{href:"https://github.com/atanunq/viu",children:"VIU tool"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"To download the tool, please run:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Download Rust\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n\n# Download VIU\ncargo install viu\n"})}),"\n",(0,i.jsx)(n.p,{children:"To quickly view a low-resolution version of the image, please run:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"viu output.png\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"VIU Image Viewer",src:s(5238).A+"",width:"664",height:"632"})}),"\n",(0,i.jsx)(n.h4,{id:"option-2-sagemaker-code-editor",children:"Option 2: SageMaker Code Editor"}),"\n",(0,i.jsx)(n.p,{children:"If you are using SageMaker Code Editor, we can view the image directly in code editor."}),"\n",(0,i.jsxs)(n.p,{children:["Please use the Code editor explorer to navigate to ",(0,i.jsx)(n.code,{children:"output.png"})," and open the image."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Open File in Code Editor",src:s(313).A+"",width:"1774",height:"951"})}),"\n",(0,i.jsxs)(n.p,{children:["The following prompt was used to generate this image: ",(0,i.jsx)(n.code,{children:'"cat wearing a fancy hat on a purple mat"'}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["If you'd like to download the image to your desktop, you can right-click the file ",(0,i.jsx)(n.code,{children:"output.png"}),' in the Code Editor File Explorer and select "Download...".']}),"\n",(0,i.jsx)(n.h2,{id:"4-cleanup",children:"4. Cleanup"}),"\n",(0,i.jsx)(n.p,{children:"To clean up the Ray Service deployment:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl delete -f rayservice.stable_diffusion_gpu.yaml\n"})}),"\n",(0,i.jsx)(n.p,{children:"To verify cleanup:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get pods\nkubectl get services\n"})}),"\n",(0,i.jsx)(n.h2,{id:"whats-next",children:"What's Next?"}),"\n",(0,i.jsx)(n.p,{children:"That was our inference example using Ray Service! You can:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Try other prompts"}),": Experiment with different text prompts for image generation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Explore other models"}),": Check out other inference examples available within the rayservice directory"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scale the service"}),": Modify the ",(0,i.jsx)(n.code,{children:"num_replicas"})," in the YAML to handle more concurrent requests"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitor performance"}),": Use the Ray dashboard to monitor GPU utilization and request latency"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Add authentication"}),": Implement proper authentication for production deployments"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["For more advanced Ray Serve configurations, refer to the ",(0,i.jsx)(n.a,{href:"https://docs.ray.io/en/latest/serve/index.html",children:"Ray Serve documentation"}),"."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>t});var r=s(6540);const i={},a=r.createContext(i);function o(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);