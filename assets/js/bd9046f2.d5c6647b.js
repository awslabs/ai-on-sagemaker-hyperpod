"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[8708],{3847:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"hyperpod-recipes/slurm-hyperpod-recipes/setup-and-train","title":"Setup and Launch training - Slurm","description":"Prerequisites","source":"@site/docs/03-hyperpod-recipes/slurm-hyperpod-recipes/setup-and-train.md","sourceDirName":"03-hyperpod-recipes/slurm-hyperpod-recipes","slug":"/hyperpod-recipes/slurm-hyperpod-recipes/setup-and-train","permalink":"/docs/hyperpod-recipes/slurm-hyperpod-recipes/setup-and-train","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Setup and Launch training - Slurm","sidebar_position":2,"weight":71},"sidebar":"tutorialSidebar","previous":{"title":"Setup and Launch training - EKS","permalink":"/docs/hyperpod-recipes/eks-hyperpod-recipes/hyperpod-recipes"},"next":{"title":"Add-Ons","permalink":"/docs/category/add-ons"}}');var i=r(4848),s=r(8453);const a={title:"Setup and Launch training - Slurm",sidebar_position:2,weight:71},o=void 0,l={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Environment",id:"environment",level:2},{value:"Data",id:"data",level:2},{value:"Launch training",id:"launch-training",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["You'll need to create a cluster with Amazon SageMaker HyperPod. Instructions can be found in ",(0,i.jsx)(n.a,{href:"/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup",children:"1. Cluster Setup"}),". Please follow them if you haven't done so already."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"environment",children:"Environment"}),"\n",(0,i.jsx)(n.p,{children:"SSH into the head node of the cluster and run the following instructions."}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Set up the virtual environment. Make sure you're using Python 3.9 or greater."}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python3 -m venv ${PWD}/venv\nsource venv/bin/activate\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsx)(n.li,{children:"Clone the SageMaker HyperPod recipes and SageMaker HyperPod adapter repositories to a shared storage location."}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"git clone --recursive https://github.com/aws/sagemaker-hyperpod-recipes.git\ncd sagemaker-hyperpod-recipes\npip3 install -r requirements.txt\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsxs)(n.li,{children:["Create a squash file using Enroot. To find the most recent release of the SMP container, see ",(0,i.jsx)(n.a,{href:"https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-model-parallel-support-v2.html",children:"SMP release notes"}),". To gain a deeper understanding of how to use the Enroot file, see Build ",(0,i.jsx)(n.a,{href:"https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/2.nemo-launcher#2-build-aws-optimized-nemo-launcher-image",children:"AWS-optimized Nemo-Launcher image"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Use the below command to create a sqsh file. Set the AWS_REGION Env variable if not already set."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'REGION=$AWS_REGION\nIMAGE="658645717510.dkr.ecr.${REGION}.amazonaws.com/smdistributed-modelparallel:2.4.1-gpu-py311-cu121"\naws ecr get-login-password --region "${REGION}" | docker login --username AWS --password-stdin 855988369404.dkr.ecr.${REGION}.amazonaws.com\nenroot import -o $PWD/smdistributed-modelparallel.sqsh dockerd://${IMAGE}\n'})}),"\n",(0,i.jsxs)(n.p,{children:["To use the Enroot squash file to start training, use the following example to modify the ",(0,i.jsx)(n.code,{children:"recipes_collection/config.yaml"})," file in the sagemaker-hyperpod-recipes repository as shown below"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"container: /fsx/path/to/your/smdistributed-modelparallel.sqsh\n"})}),"\n",(0,i.jsx)(n.h2,{id:"data",children:"Data"}),"\n",(0,i.jsx)(n.p,{children:"HyperPod recipes support tokenized data in any of the below formats"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"JSON"}),"\n",(0,i.jsx)(n.li,{children:"JSONGZ (Compressed JSON)"}),"\n",(0,i.jsx)(n.li,{children:"ARROW"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"launch-training",children:"Launch training"}),"\n",(0,i.jsxs)(n.p,{children:["After you install the necessary dependencies, start a training job from the ",(0,i.jsx)(n.code,{children:"sagemaker-hyperpod-recipes/launcher_scripts"})," directory. You get the dependencies by cloning the SageMaker HyperPod recipes repository:"]}),"\n",(0,i.jsxs)(n.p,{children:["First, pick your training recipe from the ",(0,i.jsx)(n.a,{href:"https://github.com/aws/sagemaker-hyperpod-recipes/tree/main?tab=readme-ov-file#model-support",children:"available recipes"}),", the model name is specified as part of the recipe and you can locate the corresponding launcher scripts under ",(0,i.jsx)(n.code,{children:"launcher_scripts"})," directory."]}),"\n",(0,i.jsx)(n.p,{children:"As an example We'll use the launcher_scripts/llama/run_hf_llama3_8b_seq16k_gpu_p5x16_pretrain.sh script to launch a Llama 8b with sequence length 8192 pre-training recipe. The launcher script as shown below runs a python script which is responsible to set up the slurm job. You need to modify the launcher script to change the paths to training and validation data."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'\nSAGEMAKER_TRAINING_LAUNCHER_DIR=${SAGEMAKER_TRAINING_LAUNCHER_DIR:-"$(pwd)"}\n\nTRAIN_DIR="${TRAIN_DIR}" # Location of training dataset\nVAL_DIR="${VAL_DIR}" # Location of validation dataset\n\nEXP_DIR="${EXP_DIR}" # Location to save experiment info including logging, checkpoints, etc.\n\n\nHYDRA_FULL_ERROR=1 python3 ${SAGEMAKER_TRAINING_LAUNCHER_DIR}/main.py \\\n    recipes=training/llama/hf_llama3_8b_seq8k_gpu_p5x16_pretrain \\\n    base_results_dir=${SAGEMAKER_TRAINING_LAUNCHER_DIR}/results \\\n    recipes.run.name="hf-llama3-8b" \\\n    recipes.exp_manager.exp_dir=$EXP_DIR \\\n    recipes.model.data.train_dir=$TRAIN_DIR \\\n    recipes.model.data.val_dir=$VAL_DIR \\\n'})}),"\n",(0,i.jsx)(n.admonition,{title:"Important",type:"info",children:(0,i.jsxs)(n.p,{children:["If you want to do a dry run without using actual data add the following parameter to the python command ",(0,i.jsx)(n.code,{children:"recipes.model.data.use_synthetic_data=true"}),"."]})}),"\n",(0,i.jsx)(n.admonition,{title:"Important",type:"info",children:(0,i.jsxs)(n.p,{children:["You can provide the HuggingFace token if you need pre-trained weights from HuggingFace by adding the following key-value pair to the python command ",(0,i.jsx)(n.code,{children:"recipes.model.hf_access_token=<your_hf_token>"})]})}),"\n",(0,i.jsx)(n.p,{children:"Once you have the launcher script updated, you can run the below command to start the training job"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"bash launcher_scripts/llama/run_hf_llama3_8b_seq8k_gpu_lora.sh\n"})}),"\n",(0,i.jsx)(n.p,{children:"You should be able to see the job created by using squeue."}),"\n",(0,i.jsx)(n.p,{children:"The corresponding logs can be found under the results folder."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var t=r(6540);const i={},s=t.createContext(i);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);