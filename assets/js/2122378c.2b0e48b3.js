"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6229],{1237:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/nvtop-ca4f2a5a3bf9b1e3fe5b00f1bd699b0e.png"},8142:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"EKS Blueprints/Training/Fully Sharded Data Parallelism/Fully Sharded Data Parallelism","title":"Fully Sharded Data Parallelism (FSDP)","description":"This example showcases an easy way to get started with multi node FSDP training on Amazon EKS on SageMaker HyperPod. It is designed to be as simple as possible, requires no data preparation, and uses a docker image.","source":"@site/docs/1. EKS Blueprints/Training/Fully Sharded Data Parallelism/Fully Sharded Data Parallelism.md","sourceDirName":"1. EKS Blueprints/Training/Fully Sharded Data Parallelism","slug":"/EKS Blueprints/Training/Fully Sharded Data Parallelism/","permalink":"/docs/EKS Blueprints/Training/Fully Sharded Data Parallelism/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Fully Sharded Data Parallelism (FSDP)","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Data Distributed Parallelism (DDP)","permalink":"/docs/EKS Blueprints/Training/Data Distributed Parallelism/"},"next":{"title":"Amazon Sagemaker Hyperpod Recipes","permalink":"/docs/EKS Blueprints/Training/Hyperpod Recipes/"}}');var r=a(4848),t=a(8453);const i={title:"Fully Sharded Data Parallelism (FSDP)",sidebar_position:1},o="Get Started Training Llama 2 with PyTorch FSDP in 5 Minutes",l={},d=[{value:"Prerequisites",id:"prerequisites",level:3},{value:"Verified instance types, instance counts",id:"verified-instance-types-instance-counts",level:3},{value:"Setup the docker image",id:"setup-the-docker-image",level:2},{value:"Build a docker image and dataset",id:"build-a-docker-image-and-dataset",level:3},{value:"Data",id:"data",level:3},{value:"Start your training run",id:"start-your-training-run",level:2},{value:"Create HuggingFace Token",id:"create-huggingface-token",level:3},{value:"Install envsubst",id:"install-envsubst",level:3},{value:"Generate manifest from template",id:"generate-manifest-from-template",level:3},{value:"Deploy the training job",id:"deploy-the-training-job",level:3},{value:"Monitor your training job",id:"monitor-your-training-job",level:3},{value:"Stop the training",id:"stop-the-training",level:3},{value:"Start training with the Hyperpod CLI",id:"start-training-with-the-hyperpod-cli",level:2},{value:"Set environment variables",id:"set-environment-variables",level:3},{value:"Generate a job configuration file",id:"generate-a-job-configuration-file",level:3},{value:"Start training job",id:"start-training-job",level:3},{value:"Monitor",id:"monitor",level:3},{value:"Troubleshoot",id:"troubleshoot",level:3},{value:"Stop",id:"stop",level:3}];function c(e){const n={a:"a",code:"code",div:"div",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"get-started-training-llama-2-with-pytorch-fsdp-in-5-minutes",children:"Get Started Training Llama 2 with PyTorch FSDP in 5 Minutes"})}),"\n",(0,r.jsxs)(n.p,{children:["This example showcases an easy way to get started with multi node ",(0,r.jsx)(n.a,{href:"https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html",children:"FSDP"})," training on Amazon EKS on SageMaker HyperPod. It is designed to be as simple as possible, requires no data preparation, and uses a docker image."]}),"\n",(0,r.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"Before running this training, you'll need to create a cluster with Amazon EKS on SageMaker HyperPod."}),"\n",(0,r.jsxs)(n.p,{children:["Instructions can be found in the ",(0,r.jsx)(n.a,{href:"/docs/category/getting-started",children:"0. Deploy Your HyperPod Cluster"})," section. Please follow them if you haven't done so already."]}),"\n",(0,r.jsx)(n.p,{children:"Please also make sure that you deployed GPU device plugin, EFA device plugin, and Kubeflow training operator to your cluster."}),"\n",(0,r.jsxs)(n.p,{children:["See ",(0,r.jsx)(n.a,{href:"/docs/category/getting-started",children:"What Dependencies are Installed on Your EKS Cluster"})," for details."]}),"\n",(0,r.jsx)(n.p,{children:"To build a container image, you need a x86-64 based development environment with Docker installed. If you use recent Mac with Apple Silicon, they are not x86-64 based but ARM based. You can either SageMaker Code Editor for this purpose or make sure you build the container for a x86_64 platform (instructions on that later)."}),"\n",(0,r.jsx)(n.h3,{id:"verified-instance-types-instance-counts",children:"Verified instance types, instance counts"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ml.g5.8xlarge x 8"}),"\n",(0,r.jsx)(n.li,{children:"ml.p5en.48xlarge x 2"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"setup-the-docker-image",children:"Setup the docker image"}),"\n",(0,r.jsx)(n.h3,{id:"build-a-docker-image-and-dataset",children:"Build a docker image and dataset"}),"\n",(0,r.jsx)(n.p,{children:"On your x86-64 based development environment:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Navigate to your home directory or your preferred project directory, clone the repo."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ~\ngit clone https://github.com/aws-samples/awsome-distributed-training/\ncd awsome-distributed-training/3.test_cases/pytorch/FSDP\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Build the ",(0,r.jsx)(n.code,{children:"fsdp"})," image."]}),"\n",(0,r.jsx)(n.p,{children:"Build a container image for this example using the code below:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/hpc-cloud\nexport REGION=$(aws ec2 describe-availability-zones --output text --query 'AvailabilityZones[0].[RegionName]')\nexport ACCOUNT=$(aws sts get-caller-identity --query Account --output text)\nexport REGISTRY=${ACCOUNT}.dkr.ecr.${REGION}.amazonaws.com/\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"docker build $DOCKER_NETWORK -t ${REGISTRY}fsdp:pytorch2.5.1 .    \n"})}),"\n",(0,r.jsx)(n.div,{children:(0,r.jsx)(n.div,{children:(0,r.jsxs)(n.p,{children:["The environment variable",(0,r.jsx)(n.code,{children:"$DOCKER_NETWORK"})," is set to ",(0,r.jsx)(n.code,{children:"--network=sagemaker"})," only if you deployed the SageMaker Studio Code Editor CloudFormation stack in the ",(0,r.jsx)(n.a,{href:"/docs/category/getting-started",children:"Set Up Your Development Environment"})," section. This is necessary because SageMaker Studio uses a specific network configuration for its containers. Otherwise, it remains unset."]})})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Alternatively"}),", ff you are on a Mac, use ",(0,r.jsx)(n.code,{children:"buildx"})," to target ",(0,r.jsx)(n.code,{children:"linux/amd64"})," architecture:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"docker buildx build --platform linux/amd64 -t ${REGISTRY}fsdp:pytorch2.5.1 .\n"})}),"\n",(0,r.jsx)(n.p,{children:"Building image can take 5~7min. If successful, you should see following success message at the end."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Successfully built 123ab12345cd\nSuccessfully tagged 123456789012.dkr.ecr.us-west-2.amazonaws.com/fsdp:pytorch2.5.1\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Push the image to Amazon ECR"}),"\n",(0,r.jsx)(n.p,{children:"In this step we create a container registry if one does not exist, and push the container image to it."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Create registry if needed\nREGISTRY_COUNT=$(aws ecr describe-repositories | grep "fsdp" | wc -l)\nif [ "$REGISTRY_COUNT" -eq 0 ]; then\n    aws ecr create-repository --repository-name fsdp\nfi\n\n# Login to registry\necho "Logging in to $REGISTRY ..."\naws ecr get-login-password | docker login --username AWS --password-stdin $REGISTRY\n\n# Push image to registry\ndocker image push ${REGISTRY}fsdp:pytorch2.5.1\n'})}),"\n",(0,r.jsx)(n.p,{children:"Pushing the image may take some time depending on your network bandwidth. If you use EC2 / CloudShell as your development machine, it will take 6~8 min."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"data",children:"Data"}),"\n",(0,r.jsxs)(n.p,{children:["For this example, we'll be using the ",(0,r.jsx)(n.a,{href:"https://huggingface.co/datasets/allenai/c4",children:"allenai/c4"})," dataset. Instead of downloading the whole thing, the ",(0,r.jsx)(n.code,{children:"create_streaming_dataloaders"})," function will stream the dataset from ",(0,r.jsx)(n.a,{href:"https://huggingface.co/datasets",children:"HuggingFace"}),", so there's no data prep required for running this training."]}),"\n",(0,r.jsxs)(n.p,{children:["If you'd like to instead use your own dataset, you can do so by ",(0,r.jsx)(n.a,{href:"https://huggingface.co/docs/datasets/create_dataset",children:"formatting it as a HuggingFace dataset"}),", and passing its location to the ",(0,r.jsx)(n.code,{children:"--dataset_path"})," argument."]}),"\n",(0,r.jsx)(n.h2,{id:"start-your-training-run",children:"Start your training run"}),"\n",(0,r.jsx)(n.h3,{id:"create-huggingface-token",children:"Create HuggingFace Token"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"For this dataset, we will need a Hugging Face access token"}),". First, create a ",(0,r.jsx)(n.a,{href:"https://huggingface.co/welcome",children:"Hugging Face account"}),". Then ",(0,r.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"generate your access token with read permissions"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"We will reference this token in the next step by setting it as an environment variable."}),"\n",(0,r.jsx)(n.h3,{id:"install-envsubst",children:"Install envsubst"}),"\n",(0,r.jsxs)(n.p,{children:["This example uses ",(0,r.jsx)(n.a,{href:"https://github.com/a8m/envsubst",children:(0,r.jsx)(n.code,{children:"envsubst"})})," to generate a Kubernetes manifest file from a template file and parameters. If you don't have ",(0,r.jsx)(n.code,{children:"envsubst"})," on your development environment, install it by following the ",(0,r.jsx)(n.a,{href:"https://github.com/a8m/envsubst?tab=readme-ov-file#installation",children:"Installation instruction"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"generate-manifest-from-template",children:"Generate manifest from template"}),"\n",(0,r.jsxs)(n.p,{children:["With the ",(0,r.jsx)(n.code,{children:"envsubst"})," command, generate ",(0,r.jsx)(n.code,{children:"fsdp.yaml"})," from ",(0,r.jsx)(n.code,{children:"fsdp.yaml-template"}),". Please configure instance type, number of nodes, number of GPUs, number of EFAs, based on your cluster's specification."]}),"\n",(0,r.jsx)(n.p,{children:"You can check your cluster's specification by running following command:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'kubectl get nodes "-o=custom-columns=NAME:.metadata.name,INSTANCETYPE:.metadata.labels.node\\.kubernetes\\.io/instance-type,GPU:.status.allocatable.nvidia\\.com/gpu,EFA:.status.allocatable.vpc\\.amazonaws\\.com/efa"\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"NAME                           INSTANCETYPE    GPU   EFA\nhyperpod-i-055aeff9546187dee   ml.g5.8xlarge   1     1\nhyperpod-i-09662f64f615c96f5   ml.g5.8xlarge   1     1\nhyperpod-i-099e2a84aba621d52   ml.g5.8xlarge   1     1\nhyperpod-i-0a6fea3329235be91   ml.g5.8xlarge   1     1\nhyperpod-i-0ac3feb733dc0f00e   ml.g5.8xlarge   1     1\nhyperpod-i-0bf7dce836e063fa6   ml.g5.8xlarge   1     1\nhyperpod-i-0ddf28f3ff2870f1b   ml.g5.8xlarge   1     1\nhyperpod-i-0fe48912b03d2c22e   ml.g5.8xlarge   1     1\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Change directories to the ",(0,r.jsx)(n.code,{children:"kubernetes"})," directory:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd kubernetes/\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Set environment variables and run ",(0,r.jsx)(n.code,{children:"envsubst"})," to generate ",(0,r.jsx)(n.code,{children:"fsdp.yaml"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"For ml.g5.8xlarge x 8:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"export IMAGE_URI=${REGISTRY}fsdp:pytorch2.5.1\nexport INSTANCE_TYPE=ml.g5.8xlarge\nexport NUM_NODES=8\nexport GPU_PER_NODE=1\nexport EFA_PER_NODE=1\nexport FI_PROVIDER=efa\nexport HF_TOKEN=<Your HuggingFace Token>\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cat fsdp.yaml-template | envsubst > fsdp.yaml\n"})}),"\n",(0,r.jsx)(n.h3,{id:"deploy-the-training-job",children:"Deploy the training job"}),"\n",(0,r.jsxs)(n.p,{children:["Now the manifest file ",(0,r.jsx)(n.code,{children:"fsdp.yaml"})," is generated, and you are ready to deploy the training workload. Run following command to deploy the training workload."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f ./fsdp.yaml\n"})}),"\n",(0,r.jsx)(n.p,{children:"You should see following message."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"pytorchjob.kubeflow.org/fsdp created\n"})}),"\n",(0,r.jsx)(n.h3,{id:"monitor-your-training-job",children:"Monitor your training job"}),"\n",(0,r.jsx)(n.p,{children:"To see the status of your job, use the commands below:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get pytorchjob\nkubectl get pods\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"NAME   STATE     AGE\nfsdp   Running   5m\n\nNAME                    READY   STATUS              RESTARTS   AGE\netcd-7787559c74-pw4jp   1/1     Running             0          74s\nfsdp-worker-0           0/1     ContainerCreating   0          74s\nfsdp-worker-1           0/1     ContainerCreating   0          74s\nfsdp-worker-2           0/1     ContainerCreating   0          74s\nfsdp-worker-3           0/1     ContainerCreating   0          74s\nfsdp-worker-4           0/1     ContainerCreating   0          74s\nfsdp-worker-5           0/1     ContainerCreating   0          74s\nfsdp-worker-6           0/1     ContainerCreating   0          74s\nfsdp-worker-7           0/1     ContainerCreating   0          74s\n"})}),"\n",(0,r.jsxs)(n.p,{children:["When you run for the first time, it takes 3~4min until the Pod statuses change from ",(0,r.jsx)(n.code,{children:"ContainerCreating"})," to ",(0,r.jsx)(n.code,{children:"Running"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"NAME                    READY   STATUS    RESTARTS   AGE\netcd-7787559c74-pw4jp   1/1     Running   0          3m43s\nfsdp-worker-0           1/1     Running   0          3m43s\nfsdp-worker-1           1/1     Running   0          3m43s\nfsdp-worker-2           1/1     Running   0          3m43s\nfsdp-worker-3           1/1     Running   0          3m43s\nfsdp-worker-4           1/1     Running   0          3m43s\nfsdp-worker-5           1/1     Running   0          3m43s\nfsdp-worker-6           1/1     Running   0          3m43s\nfsdp-worker-7           1/1     Running   0          3m43s\n"})}),"\n",(0,r.jsx)(n.p,{children:"Each of the pods produces job logs. One of the pods is elected master during job initialization. Only this pod will show the progress of the training job in its log. To find out which pod is currently the master, run the command below."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl logs fsdp-worker-0 | grep master_addr=\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"[2024-06-25 22:20:17,556] torch.distributed.elastic.agent.server.api: [INFO]   master_addr=fsdp-worker-1\n"})}),"\n",(0,r.jsx)(n.p,{children:"This shows that the pod fsdp-worker-1 is currently the master. To look at the current job logs, use the command below:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl logs -f fsdp-worker-1\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"    :\n2024-06-25 22:22:36 I [train.py:102] Batch 0 Loss: 11.63946, Speed: 0.27 samples/sec, lr: 0.000006\n2024-06-25 22:22:57 I [train.py:102] Batch 1 Loss: 11.66096, Speed: 0.39 samples/sec, lr: 0.000013\n2024-06-25 22:23:17 I [train.py:102] Batch 2 Loss: 11.56659, Speed: 0.40 samples/sec, lr: 0.000019\n2024-06-25 22:23:37 I [train.py:102] Batch 3 Loss: 11.14039, Speed: 0.40 samples/sec, lr: 0.000025\n    :\n"})}),"\n",(0,r.jsxs)(n.p,{children:["You can execute ",(0,r.jsx)(n.code,{children:"nvtop"})," command inside a running container within a Pod to see GPU utilization."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl exec -it fsdp-worker-4 -- nvtop\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"nvtop",src:a(1237).A+"",width:"1792",height:"934"})}),"\n",(0,r.jsx)(n.h3,{id:"stop-the-training",children:"Stop the training"}),"\n",(0,r.jsx)(n.p,{children:"To stop the current training job, use the following command."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl delete -f ./fsdp.yaml\n"})}),"\n",(0,r.jsx)(n.h2,{id:"start-training-with-the-hyperpod-cli",children:"Start training with the Hyperpod CLI"}),"\n",(0,r.jsx)(n.div,{children:(0,r.jsxs)(n.p,{children:["This page shows how to run the sample application with HyperPod CLI, instead of ",(0,r.jsx)(n.code,{children:"kubectl"}),". If you didn't install the HyperPod CLI, see the ",(0,r.jsx)(n.a,{href:"/docs/Getting%20Started/Installing%20the%20Hyperpod%20CLI",children:"Install HyperPod CLI"})," page."]})}),"\n",(0,r.jsx)(n.h3,{id:"set-environment-variables",children:"Set environment variables"}),"\n",(0,r.jsx)(n.p,{children:"Check your cluster's specification by running following command:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'kubectl get nodes "-o=custom-columns=NAME:.metadata.name,INSTANCETYPE:.metadata.labels.node\\.kubernetes\\.io/instance-type,GPU:.status.allocatable.nvidia\\.com/gpu,EFA:.status.allocatable.vpc\\.amazonaws\\.com/efa"\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"NAME                           INSTANCETYPE    GPU   EFA\nhyperpod-i-055aeff9546187dee   ml.g5.8xlarge   1     1\nhyperpod-i-09662f64f615c96f5   ml.g5.8xlarge   1     1\nhyperpod-i-099e2a84aba621d52   ml.g5.8xlarge   1     1\nhyperpod-i-0a6fea3329235be91   ml.g5.8xlarge   1     1\nhyperpod-i-0ac3feb733dc0f00e   ml.g5.8xlarge   1     1\nhyperpod-i-0bf7dce836e063fa6   ml.g5.8xlarge   1     1\nhyperpod-i-0ddf28f3ff2870f1b   ml.g5.8xlarge   1     1\nhyperpod-i-0fe48912b03d2c22e   ml.g5.8xlarge   1     1\n"})}),"\n",(0,r.jsx)(n.p,{children:"Set following environment variables based on your cluster configuration."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"export IMAGE_URI=${REGISTRY}fsdp:pytorch2.5.1\nexport INSTANCE_TYPE=ml.g5.8xlarge\nexport NUM_NODES=8\nexport GPU_PER_NODE=1\n"})}),"\n",(0,r.jsx)(n.h3,{id:"generate-a-job-configuration-file",children:"Generate a job configuration file"}),"\n",(0,r.jsxs)(n.p,{children:["Run following command to generate a job configuration file (",(0,r.jsx)(n.code,{children:"hpcli-fsdp.yaml"}),") for HyperPod CLI."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'cat > hpcli-fsdp.yaml << EOL\ndefaults:\n - override hydra/job_logging: stdout\n\nhydra:\n run:\n  dir: .\n output_subdir: null\n\ntraining_cfg:\n entry_script: /fsdp/train.py\n script_args:\n    - --max_context_width: 4096\n    - --num_key_value_heads: 32\n    - --intermediate_size: 11008\n    - --hidden_width: 4096\n    - --num_layers: 32\n    - --num_heads: 32\n    - --model_type: llama_v2\n    - --tokenizer: hf-internal-testing/llama-tokenizer\n    - --checkpoint_freq: 5000\n    - --validation_freq: 500\n    - --max_steps: 5000\n    - --checkpoint_dir: /checkpoints\n    - --dataset: allenai/c4\n    - --dataset_config_name: en\n    - --resume_from_checkpoint: /checkpoints\n    - --train_batch_size: 1\n    - --val_batch_size: 1\n    - --sharding_strategy: full\n    - --offload_activation: 1\n\n run:\n  name: fsdp\n  nodes: ${NUM_NODES}\n  ntasks_per_node: ${GPU_PER_NODE}\ncluster:\n cluster_type: k8s\n instance_type: ${INSTANCE_TYPE}\n cluster_config:\n  service_account_name: null\n\n  volumes:\n    - volumeName: local\n      hostPath: "/mnt/k8s-disks/0"\n      mountPath: "/local"\n\n  namespace: kubeflow\n  label_selector:\n      required:\n          sagemaker.amazonaws.com/node-health-status:\n              - Schedulable\n      preferred:\n          sagemaker.amazonaws.com/deep-health-check-status:\n              - Passed\n      weights:\n          - 100\n  pullPolicy: Always\n  restartPolicy: OnFailure\n\n  annotations:\n    sagemaker.amazonaws.com/enable-job-auto-resume: True\n    sagemaker.amazonaws.com/job-max-retry-count: 10\n\nbase_results_dir: ./result\ncontainer: ${IMAGE_URI}\n\nenv_vars:\n LOGLEVEL: DEBUG\n TORCH_DISTRIBUTED_DEBUG: DETAIL\n TORCH_NCCL_ENABLE_MONITORING: 1\n TORCH_NCCL_TRACE_BUFFER_SIZE: 20000\n TORCH_NCCL_DUMP_ON_TIMEOUT: 1\n TORCH_NCCL_DEBUG_INFO_TEMP_FILE: /local/nccl_trace_rank_\n PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"\n NCCL_DEBUG: INFO\n NCCL_SOCKET_IFNAME: ^lo\n TORCH_NCCL_ASYNC_ERROR_HANDLING: 1\nEOL\n'})}),"\n",(0,r.jsx)(n.h3,{id:"start-training-job",children:"Start training job"}),"\n",(0,r.jsxs)(n.p,{children:["Now the job configuration file ",(0,r.jsx)(n.code,{children:"hpcli-fsdp.yaml"})," is generated, and you are ready to start the training job."]}),"\n",(0,r.jsxs)(n.p,{children:["Before startuing the job, you need to select the cluster with ",(0,r.jsx)(n.code,{children:"hyperpod connect-cluster"})," command."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"hyperpod connect-cluster --cluster-name ml-cluster\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Then run ",(0,r.jsx)(n.code,{children:"hyperpod start-job"})," command to start the job."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"hyperpod start-job --config-file ./hpcli-fsdp.yaml\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'{\n "Console URL": "https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/cluster-management/ml-cluster"\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"monitor",children:"Monitor"}),"\n",(0,r.jsx)(n.p,{children:"To see the status of your job, use the commands below:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"hyperpod get-job --job-name fsdp -n kubeflow\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n "Name": "fsdp",\n "Namespace": "kubeflow",\n "Label": {\n  "app": "fsdp",\n  "app.kubernetes.io/managed-by": "Helm"\n },\n "CreationTimestamp": "2024-09-26T01:06:51Z",\n "Status": {\n  "conditions": [\n   {\n    "lastTransitionTime": "2024-09-26T01:06:51Z",\n    "lastUpdateTime": "2024-09-26T01:06:51Z",\n    "message": "PyTorchJob fsdp is created.",\n    "reason": "PyTorchJobCreated",\n    "status": "True",\n    "type": "Created"\n   },\n   {\n    "lastTransitionTime": "2024-09-26T01:07:02Z",\n    "lastUpdateTime": "2024-09-26T01:07:02Z",\n    "message": "PyTorchJob kubeflow/fsdp is running.",\n    "reason": "PyTorchJobRunning",\n    "status": "True",\n    "type": "Running"\n   }\n  ],\n  "replicaStatuses": {\n   "Worker": {\n    "active": 8,\n    "selector": "training.kubeflow.org/job-name=fsdp,training.kubeflow.org/operator-name=pytorchjob-controller,training.kubeflow.org/replica-type=worker"\n   }\n  },\n  "startTime": "2024-09-26T01:07:00Z"\n },\n "ConsoleURL": "https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/cluster-management/k8-g5-8x-4"\n}\n'})}),"\n",(0,r.jsxs)(n.p,{children:["If you need more detailed information of the job, you can use ",(0,r.jsx)(n.code,{children:"--verbose"})," option."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"hyperpod get-job --job-name fsdp -n kubeflow --verbose\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n "Name": "fsdp",\n "Namespace": "kubeflow",\n "Label": {\n  "app": "fsdp",\n  "app.kubernetes.io/managed-by": "Helm"\n },\n "Annotations": {\n  "meta.helm.sh/release-name": "fsdp",\n  "meta.helm.sh/release-namespace": "kubeflow",\n  "sagemaker.amazonaws.com/enable-job-auto-resume": "true",\n  "sagemaker.amazonaws.com/job-max-retry-count": "10"\n },\n "Metadata": {\n  "CreationTimestamp": "2024-09-26T01:06:51Z",\n  "Generation": 1,\n  "ResourceVersion": "4240104",\n  "UID": "39364a40-70c7-4d03-abab-160c124e7367"\n },\n "Kind": "PyTorchJob",\n "ApiVersion": "kubeflow.org/v1",\n "Spec": {\n  "pytorchReplicaSpecs": {\n   "Worker": {\n    "replicas": 8,\n    "template": {\n     "spec": {\n      "affinity": {\n       "nodeAffinity": {\n        "preferredDuringSchedulingIgnoredDuringExecution": [\n         {\n          "preference": {\n           "matchExpressions": [\n            {\n             "key": "sagemaker.amazonaws.com/deep-health-check-status",\n             "operator": "In",\n             "values": [\n              "Passed"\n             ]\n            }\n           ]\n          },\n          "weight": 100\n         }\n        ],\n        "requiredDuringSchedulingIgnoredDuringExecution": {\n         "nodeSelectorTerms": [\n          {\n           "matchExpressions": [\n            {\n             "key": "sagemaker.amazonaws.com/node-health-status",\n             "operator": "In",\n             "values": [\n              "Schedulable"\n             ]\n            }\n           ]\n          }\n         ]\n        }\n       }\n      },\n      "containers": [\n       {\n        "command": [\n         "/etc/config/train-script.sh"\n        ],\n        "env": [\n         {\n          "name": "CUDA_DEVICE_MAX_CONNECTIONS",\n          "value": "1"\n         },\n         {\n          "name": "CUDA_VISIBLE_DEVICES",\n          "value": "0"\n         },\n         {\n          "name": "FI_EFA_FORK_SAFE",\n          "value": "1"\n         },\n         {\n          "name": "FI_PROVIDER",\n          "value": "efa"\n         },\n         {\n          "name": "LOGLEVEL",\n          "value": "DEBUG"\n         },\n         {\n          "name": "NCCL_DEBUG",\n          "value": "INFO"\n         },\n         {\n          "name": "NCCL_IGNORE_DISABLED_P2P",\n          "value": "1"\n         },\n         {\n          "name": "NCCL_PROTO",\n          "value": "simple"\n         },\n         {\n          "name": "NCCL_SOCKET_IFNAME",\n          "value": "^lo,docker0"\n         },\n         {\n          "name": "PYTORCH_CUDA_ALLOC_CONF",\n          "value": "expandable_segments:True"\n         },\n         {\n          "name": "TORCH_DISTRIBUTED_DEBUG",\n          "value": "DETAIL"\n         },\n         {\n          "name": "TORCH_DIST_INIT_BARRIER",\n          "value": "1"\n         },\n         {\n          "name": "TORCH_NCCL_ASYNC_ERROR_HANDLING",\n          "value": "1"\n         },\n         {\n          "name": "TORCH_NCCL_DEBUG_INFO_TEMP_FILE",\n          "value": "/local/nccl_trace_rank_"\n         },\n         {\n          "name": "TORCH_NCCL_DUMP_ON_TIMEOUT",\n          "value": "1"\n         },\n         {\n          "name": "TORCH_NCCL_ENABLE_MONITORING",\n          "value": "1"\n         },\n         {\n          "name": "TORCH_NCCL_TRACE_BUFFER_SIZE",\n          "value": "20000"\n         }\n        ],\n        "image": "842413447717.dkr.ecr.us-west-2.amazonaws.com/fsdp:pytorch2.2",\n        "imagePullPolicy": "Always",\n        "name": "pytorch",\n        "resources": {\n         "limits": {\n          "nvidia.com/gpu": 1,\n          "vpc.amazonaws.com/efa": 1\n         },\n         "requests": {\n          "nvidia.com/gpu": 1,\n          "vpc.amazonaws.com/efa": 1\n         }\n        },\n        "securityContext": {\n         "capabilities": {\n          "add": [\n           "IPC_LOCK"\n          ]\n         }\n        },\n        "volumeMounts": [\n         {\n          "mountPath": "/local",\n          "name": "local"\n         },\n         {\n          "mountPath": "/etc/config",\n          "name": "train-script"\n         },\n         {\n          "mountPath": "/dev/shm",\n          "name": "shm"\n         }\n        ]\n       }\n      ],\n      "restartPolicy": "OnFailure",\n      "volumes": [\n       {\n        "hostPath": {\n         "path": "/mnt/k8s-disks/0"\n        },\n        "name": "local"\n       },\n       {\n        "hostPath": {\n         "path": "/dev/shm",\n         "type": "Directory"\n        },\n        "name": "shm"\n       },\n       {\n        "configMap": {\n         "defaultMode": 420,\n         "items": [\n          {\n           "key": "train-script.sh",\n           "mode": 365,\n           "path": "train-script.sh"\n          }\n         ],\n         "name": "train-script-fsdp"\n        },\n        "name": "train-script"\n       }\n      ]\n     }\n    }\n   }\n  }\n },\n "Status": {\n  "conditions": [\n   {\n    "lastTransitionTime": "2024-09-26T01:06:51Z",\n    "lastUpdateTime": "2024-09-26T01:06:51Z",\n    "message": "PyTorchJob fsdp is created.",\n    "reason": "PyTorchJobCreated",\n    "status": "True",\n    "type": "Created"\n   },\n   {\n    "lastTransitionTime": "2024-09-26T01:07:02Z",\n    "lastUpdateTime": "2024-09-26T01:07:02Z",\n    "message": "PyTorchJob kubeflow/fsdp is running.",\n    "reason": "PyTorchJobRunning",\n    "status": "True",\n    "type": "Running"\n   }\n  ],\n  "replicaStatuses": {\n   "Worker": {\n    "active": 8,\n    "selector": "training.kubeflow.org/job-name=fsdp,training.kubeflow.org/operator-name=pytorchjob-controller,training.kubeflow.org/replica-type=worker"\n   }\n  },\n  "startTime": "2024-09-26T01:07:00Z"\n },\n "ConsoleURL": "https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/cluster-management/k8-g5-8x-4"\n}\n'})}),"\n",(0,r.jsxs)(n.p,{children:["You can use ",(0,r.jsx)(n.code,{children:"hyperpod list-pods"})," command to list pods."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"hyperpod list-pods --job-name fsdp -n kubeflow\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n "pods": [\n  {\n   "PodName": "fsdp-worker-0",\n   "Namespace": "kubeflow",\n   "Status": "Running",\n   "CreationTime": "2024-09-26 01:07:01+00:00"\n  },\n  {\n   "PodName": "fsdp-worker-1",\n   "Namespace": "kubeflow",\n   "Status": "Running",\n   "CreationTime": "2024-09-26 01:07:01+00:00"\n  },\n  {\n   "PodName": "fsdp-worker-2",\n   "Namespace": "kubeflow",\n   "Status": "Running",\n   "CreationTime": "2024-09-26 01:07:01+00:00"\n  },\n  {\n   "PodName": "fsdp-worker-3",\n   "Namespace": "kubeflow",\n   "Status": "Running",\n   "CreationTime": "2024-09-26 01:07:01+00:00"\n  },\n  {\n   "PodName": "fsdp-worker-4",\n   "Namespace": "kubeflow",\n   "Status": "Running",\n   "CreationTime": "2024-09-26 01:07:01+00:00"\n  },\n  {\n   "PodName": "fsdp-worker-5",\n   "Namespace": "kubeflow",\n   "Status": "Running",\n   "CreationTime": "2024-09-26 01:07:01+00:00"\n  },\n  {\n   "PodName": "fsdp-worker-6",\n   "Namespace": "kubeflow",\n   "Status": "Running",\n   "CreationTime": "2024-09-26 01:07:01+00:00"\n  },\n  {\n   "PodName": "fsdp-worker-7",\n   "Namespace": "kubeflow",\n   "Status": "Running",\n   "CreationTime": "2024-09-26 01:07:01+00:00"\n  }\n ]\n}\n'})}),"\n",(0,r.jsxs)(n.p,{children:["You can use ",(0,r.jsx)(n.code,{children:"hyperpod get-log"})," command to print logs from a pod."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"hyperpod get-log --job-name fsdp --pod fsdp-worker-0 -n kubeflow\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"    :\n2024-09-26 01:09:17 I [train.py:102] Batch 0 Loss: 11.67824, Speed: 0.40 samples/sec, lr: 0.000006\n2024-09-26 01:09:34 I [train.py:102] Batch 1 Loss: 11.71413, Speed: 0.47 samples/sec, lr: 0.000013\n2024-09-26 01:09:52 I [train.py:102] Batch 2 Loss: 11.55315, Speed: 0.46 samples/sec, lr: 0.000019\n2024-09-26 01:10:09 I [train.py:102] Batch 3 Loss: 11.21573, Speed: 0.47 samples/sec, lr: 0.000025\n2024-09-26 01:10:26 I [train.py:102] Batch 4 Loss: 10.91101, Speed: 0.46 samples/sec, lr: 0.000031\n    :\n"})}),"\n",(0,r.jsx)(n.h3,{id:"troubleshoot",children:"Troubleshoot"}),"\n",(0,r.jsxs)(n.p,{children:["When you don't see logs from pods, use ",(0,r.jsx)(n.code,{children:"kubectl"})," to check the status of underlying Kubernetes resources."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# List PyTorchJobs\nkubectl get pytorchjobs -n kubeflow\n\n# Get details of a PyTorchJob\nkubectl describe pytorchjob fsdp -n kubeflow\n\n# List Pods\nkubectl get pods -n kubeflow\n\n# Get details of a Pod\nkubectl describe pod fsdp-worker-0 -n kubeflow\n"})}),"\n",(0,r.jsx)(n.h3,{id:"stop",children:"Stop"}),"\n",(0,r.jsx)(n.p,{children:"To stop the current training job, use the following command."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"hyperpod cancel-job --job-name fsdp -n kubeflow\n"})}),"\n",(0,r.jsx)(n.p,{children:"And verify that list of jobs is empty."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"hyperpod list-jobs -n kubeflow\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n "jobs": []\n}\n'})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>i,x:()=>o});var s=a(6540);const r={},t=s.createContext(r);function i(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);