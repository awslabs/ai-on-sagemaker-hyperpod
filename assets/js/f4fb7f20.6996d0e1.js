"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6676],{2315:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/10-filesystem-check-7f7bcab96def312ec40a3bd59f75254f.png"},3177:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/09-studio-user-be6188e930c0cd2a2df29c4cb4641712.png"},3707:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/01-studio-hyperpod-architecture-02b8b4219eb9818c6bfe4961f2ede093.png"},6995:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/08-fsx-partitioned-519ab511bdcbb6a8abaf7ca92f92b188.png"},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var t=s(6540);const i={},r=t.createContext(i);function o(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(r.Provider,{value:n},e.children)}},8458:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"getting-started/orchestrated-by-slurm/sagemaker-studio-integration","title":"SageMaker Studio Integration","description":"This guide provides step-by-step instructions for setting up Amazon SageMaker Studio with Amazon SageMaker Hyperpod SLURM, including FSx Lustre storage configuration.","source":"@site/docs/00-getting-started/orchestrated-by-slurm/sagemaker-studio-integration.md","sourceDirName":"00-getting-started/orchestrated-by-slurm","slug":"/getting-started/orchestrated-by-slurm/sagemaker-studio-integration","permalink":"/docs/getting-started/orchestrated-by-slurm/sagemaker-studio-integration","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"SageMaker Studio Integration","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"View the AWS Console","permalink":"/docs/getting-started/orchestrated-by-slurm/View the AWS Console"},"next":{"title":"SSH Into Your HyperPod Cluster","permalink":"/docs/getting-started/orchestrated-by-slurm/ssh-into-hyperpod"}}');var i=s(4848),r=s(8453);const o={title:"SageMaker Studio Integration",sidebar_position:3},a="SageMaker Studio + Hyperpod Integration Guide",d={},l=[{value:"Table of Contents",id:"table-of-contents",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Cluster Setup",id:"cluster-setup",level:2},{value:"FSx for Lustre Configuration",id:"fsx-for-lustre-configuration",level:2},{value:"SageMaker Studio Domain Setup",id:"sagemaker-studio-domain-setup",level:2},{value:"SageMaker Studio IDE Configuration",id:"sagemaker-studio-ide-configuration",level:2},{value:"Monitor SLURM Installation",id:"monitor-slurm-installation",level:2},{value:"Pitfalls and known issues",id:"pitfalls-and-known-issues",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"sagemaker-studio--hyperpod-integration-guide",children:"SageMaker Studio + Hyperpod Integration Guide"})}),"\n",(0,i.jsx)(n.p,{children:"This guide provides step-by-step instructions for setting up Amazon SageMaker Studio with Amazon SageMaker Hyperpod SLURM, including FSx Lustre storage configuration."}),"\n",(0,i.jsx)(n.p,{children:"We will help set up your Studio environment so that:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"You can use familiar environments such as JupyterLab and CodeEditor to interact with your SLURM SMHP cluster"}),"\n",(0,i.jsx)(n.li,{children:"You can access your cluster's FSxL file system from your JupyterLab/CodeEditor instance"}),"\n",(0,i.jsx)(n.li,{children:"Your CodeEditor/JupyterLab instance will essentially function as a login node to the SMHP SLURM cluster!"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Why login nodes?"}),"\nLogin nodes allow users to login to the cluster, submit jobs, and view and manipulate data without running on the critical ",(0,i.jsx)(n.code,{children:"slurmctld"})," scheduler node. This also allows you to run monitoring servers like ",(0,i.jsx)(n.a,{href:"https://github.com/aimhubio/aim",children:"aim"}),", ",(0,i.jsx)(n.a,{href:"https://www.tensorflow.org/tensorboard",children:"Tensorboard"}),", or ",(0,i.jsx)(n.a,{href:"https://prometheus.io/docs/visualization/grafana/",children:"Grafana/Prometheus"}),"."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"SageMaker Studio with Hyperpod integration",src:s(3707).A+"",width:"1908",height:"940"})}),"\n",(0,i.jsx)(n.h2,{id:"table-of-contents",children:"Table of Contents"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#prerequisites",children:"Prerequisites"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#cluster-setup",children:"Cluster Setup"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#fsx-for-lustre-configuration",children:"FSx for Lustre Configuration"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#sagemaker-studio-domain-setup",children:"SageMaker Studio Domain Setup"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#sagemaker-studio-ide-configuration",children:"SageMaker Studio IDE Configuration"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#monitor-slurm-installation",children:"Monitor SLURM Installation"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#pitfalls-and-known-issues",children:"Pitfalls"})}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.p,{children:"Before starting, ensure you have:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"AWS CLI configured with appropriate permissions"}),"\n",(0,i.jsx)(n.li,{children:"Access to AWS Management Console"}),"\n",(0,i.jsxs)(n.li,{children:["Familiarity with ",(0,i.jsx)(n.a,{href:"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html",children:"SageMaker HyperPod"}),", ",(0,i.jsx)(n.a,{href:"https://slurm.schedmd.com/documentation.html",children:"SLURM"}),", ",(0,i.jsx)(n.a,{href:"https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html",children:"SageMaker Studio"}),", and ",(0,i.jsx)(n.a,{href:"https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html",children:"FSxL"})]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"cluster-setup",children:"Cluster Setup"}),"\n",(0,i.jsx)(n.p,{children:"To create an Amazon SageMaker HyperPod SLURM cluster, you can follow one of these steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Option 1: ",(0,i.jsx)(n.a,{href:"/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup",children:"Initial Cluster Setup"})]}),"\n",(0,i.jsx)(n.li,{children:"Option 2: Using CloudFormation (see Infrastructure as Code section)"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"fsx-for-lustre-configuration",children:"FSx for Lustre Configuration"}),"\n",(0,i.jsxs)(n.p,{children:["A FSx for Lustre (FSxL) file system is created for you as part of the ",(0,i.jsx)(n.a,{href:"#cluster-setup",children:"cluster setup"}),"! We will use this file system for both your SMHP cluster nodes and your Studio Domain."]}),"\n",(0,i.jsx)(n.p,{children:"You can move on to the next section."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"sagemaker-studio-domain-setup",children:"SageMaker Studio Domain Setup"}),"\n",(0,i.jsxs)(n.p,{children:["You can deploy the CloudFormation template from ",(0,i.jsx)(n.a,{href:"https://github.com/aws-samples/awsome-distributed-training/blob/main/1.architectures/5.sagemaker-hyperpod/slurm-studio/studio-slurm.yaml",children:(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"studio-slurm.yaml"})})})," the ",(0,i.jsx)(n.a,{href:"https://github.com/aws-samples/awsome-distributed-training",children:"awsome-distributed-training repository"}),", which creates the following resources:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"SageMaker Studio domain"}),"\n",(0,i.jsx)(n.li,{children:"Lifecycle configurations for installing necessary packages for Studio IDE, including SLURM. Lifecycle configurations will be created for both JupyterLab and Code Editor. We will set it up so that your CodeEditor/JupyterLab instance will essentially be configured as a login node for your SageMaker HyperPod cluster!"}),"\n",(0,i.jsxs)(n.li,{children:["A Lambda function that:","\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Associates the created ",(0,i.jsx)(n.code,{children:"security-group-for-inbound-nfs"})," security group to the Studio domain"]}),"\n",(0,i.jsxs)(n.li,{children:["Associates the ",(0,i.jsx)(n.code,{children:"security-group-for-inbound-nfs"})," security group to the FSx for Lustre ENIs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Optional"}),": If  ",(0,i.jsx)(n.strong,{children:"SharedFSx"})," is set to ",(0,i.jsx)(n.strong,{children:"True"}),", creates the partition ",(0,i.jsx)(n.em,{children:"shared"})," in the FSx for Lustre volume, and associates it to the Studio domain"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Shared FSx Partition",src:s(9938).A+"",width:"1604",height:"658"})}),"\n",(0,i.jsxs)(n.ol,{start:"4",children:["\n",(0,i.jsxs)(n.li,{children:["If ",(0,i.jsx)(n.strong,{children:"SharedFSx"})," is set to ",(0,i.jsx)(n.strong,{children:"False"}),", a Lambda function that:","\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Creates the partition ",(0,i.jsx)(n.em,{children:"/{user_profile_name}"}),", and associates it to the Studio user profile"]}),"\n",(0,i.jsx)(n.li,{children:"Creates an Event bridge rule that invokes the previously defined Lambda function each time a new user is created."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Partitioned FSx",src:s(6995).A+"",width:"1576",height:"652"})}),"\n",(0,i.jsx)(n.p,{children:"The CloudFormation template requires the following parameters:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"AdditionalUsers"}),": Your configured SLURM users (POSIX) that you want to give access to write to your Studio's file system space (comma separated). ",(0,i.jsx)(n.code,{children:"ubuntu"})," is added by default, so you don't need to add it in."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"ExistingFSxLustreId"}),": Id of the created FSx for Lustre file system"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"ExistingSubnetIds"}),": Dropdown menu for selecting the SMHP cluster ",(0,i.jsx)(n.strong,{children:"Private Subnet IDs"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"ExistingVpcId"}),": Dropdown menu for selecting the SMHP cluster VPC"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"HeadNodeName"}),": The name of your SMHP SLURM cluster's head node (default ",(0,i.jsx)(n.code,{children:"controller-machine"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"HyperPodClusterName"}),": The name of your SMHP SLURM cluster (default: ",(0,i.jsx)(n.code,{children:"ml-cluster"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"SecurityGroupId"}),": Id of the security group that allows communication with the HyperPod Slurm controller node (for MUNGE authentication)"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"sagemaker-studio-ide-configuration",children:"SageMaker Studio IDE Configuration"}),"\n",(0,i.jsx)(n.p,{children:"As an admin user, once your SageMaker Studio Domain is provisioned, you may go in and create users as you see fit."}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["This step ",(0,i.jsx)(n.em,{children:"DOES NOT"})," assume that you already have a Studio Domain. To create one, check out the next section titled ",(0,i.jsx)(n.strong,{children:'"SageMaker Studio Domain Setup"'}),".\n",(0,i.jsx)(n.img,{alt:"alt text",src:s(3177).A+"",width:"2404",height:"792"})]})}),"\n",(0,i.jsx)(n.p,{children:"You can now select your preferred IDE from SageMaker Studio."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"SageMaker Studio Home",src:s(9146).A+"",width:"2878",height:"1516"})}),"\n",(0,i.jsx)(n.p,{children:"For the purpose of this workshop, we are going to create a Code Editor environment."}),"\n",(0,i.jsx)(n.p,{children:"From the top-left menu:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Click on ",(0,i.jsx)(n.strong,{children:"Code Editor"})]}),"\n",(0,i.jsxs)(n.li,{children:["Click on ",(0,i.jsx)(n.strong,{children:"Create Code Editor Space"})]}),"\n",(0,i.jsx)(n.li,{children:"Enter a name"}),"\n",(0,i.jsxs)(n.li,{children:["Click on ",(0,i.jsx)(n.strong,{children:"Create Space"})]}),"\n",(0,i.jsxs)(n.li,{children:["From the ",(0,i.jsx)(n.strong,{children:"Attach custom filesystem - optional"})," dropdown menu, select the FSx for Lustre volume"]}),"\n",(0,i.jsxs)(n.li,{children:["From the ",(0,i.jsx)(n.strong,{children:"Lifecycle configuration"})," dropdown menu, select the available lifecycle configuration"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Code Editor setup",src:s(8960).A+"",width:"2338",height:"1230"})}),"\n",(0,i.jsxs)(n.p,{children:["Click on ",(0,i.jsx)(n.strong,{children:"Run Space"}),". Wait until the space is created, then click ",(0,i.jsx)(n.strong,{children:"Open Code Editor"})]}),"\n",(0,i.jsxs)(n.p,{children:["To verify that your file system was mounted, you can check if you have a path mounted in the Code Editor space ",(0,i.jsx)(n.code,{children:"custom-file-system/fsx_lustre/$FSX_ID"}),":"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Code Editor setup",src:s(2315).A+"",width:"2880",height:"1522"})}),"\n",(0,i.jsx)(n.p,{children:"You can also run:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"df -h\n"})}),"\n",(0,i.jsxs)(n.p,{children:["If you set ",(0,i.jsx)(n.code,{children:"SharedFSx"})," to ",(0,i.jsx)(n.code,{children:"False"}),", you can verify separate partitions for two users.\nExample output from user1:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Filesystem                      Size  Used Avail Use% Mounted on\noverlay                          37G  494M   37G   2% /\ntmpfs                            64M     0   64M   0% /dev\ntmpfs                           1.9G     0  1.9G   0% /sys/fs/cgroup\nshm                             392M     0  392M   0% /dev/shm\n/dev/nvme1n1                    5.0G  529M  4.5G  11% /home/sagemaker-user\n/dev/nvme0n1p1                  180G   31G  150G  18% /opt/.sagemakerinternal\n10.1.53.46@tcp:/ylacfb4v/aman1  1.2T  7.5M  1.2T   1% /mnt/custom-file-systems/fsx_lustre/fs-0104f3de83efe0f33\n127.0.0.1:/                     8.0E     0  8.0E   0% /mnt/custom-file-systems/efs/fs-052756a07c3a5ba97_fsap-0b5e6e7c68f22fee3\ntmpfs                           1.9G     0  1.9G   0% /proc/acpi\ntmpfs                           1.9G     0  1.9G   0% /sys/firmware\n"})}),"\n",(0,i.jsx)(n.p,{children:"Example output from user2:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Filesystem                      Size  Used Avail Use% Mounted on\noverlay                          37G  478M   37G   2% /\ntmpfs                            64M     0   64M   0% /dev\ntmpfs                           1.9G     0  1.9G   0% /sys/fs/cgroup\nshm                             392M     0  392M   0% /dev/shm\n/dev/nvme0n1p1                  180G   31G  150G  18% /opt/.sagemakerinternal\n/dev/nvme1n1                    5.0G  529M  4.5G  11% /home/sagemaker-user\n127.0.0.1:/                     8.0E     0  8.0E   0% /mnt/custom-file-systems/efs/fs-052756a07c3a5ba97_fsap-0a323a3e5a27e1bdc\n10.1.53.46@tcp:/ylacfb4v/aman2  1.2T  7.5M  1.2T   1% /mnt/custom-file-systems/fsx_lustre/fs-0104f3de83efe0f33\ntmpfs                           1.9G     0  1.9G   0% /proc/acpi\ntmpfs                           1.9G     0  1.9G   0% /sys/firmware\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The difference here is the mountpoint for FSxl (",(0,i.jsx)(n.code,{children:"ylacfb4v"}),") has separate partitions set up. You can then ",(0,i.jsx)(n.code,{children:"cd /mnt/custom-file-systems/fsx_lustre/fs-0104f3de83efe0f33"})," and write from each user and verify that the other user isn't able to see those files!"]}),"\n",(0,i.jsxs)(n.p,{children:["Alternatively, if you set ",(0,i.jsx)(n.code,{children:"SharedFSx"})," to ",(0,i.jsx)(n.code,{children:"True"}),", you can check the the mount using ",(0,i.jsx)(n.code,{children:"df -h"}),", and it will show something like:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Filesystem                       Size  Used Avail Use% Mounted on\noverlay                           37G  478M   37G   2% /\ntmpfs                             64M     0   64M   0% /dev\ntmpfs                            1.9G     0  1.9G   0% /sys/fs/cgroup\nshm                              392M     0  392M   0% /dev/shm\n/dev/nvme0n1p1                   180G   31G  150G  18% /opt/.sagemakerinternal\n/dev/nvme1n1                     5.0G  529M  4.5G  11% /home/sagemaker-user\n10.1.53.46@tcp:/ylacfb4v/shared  1.2T  7.5M  1.2T   1% /mnt/custom-file-systems/fsx_lustre/fs-0104f3de83efe0f33\n127.0.0.1:/                      8.0E     0  8.0E   0% /mnt/custom-file-systems/efs/fs-0e16e272aba907ad3_fsap-08ae9b9f68be028d7\ntmpfs                            1.9G     0  1.9G   0% /proc/acpi\ntmpfs                            1.9G     0  1.9G   0% /sys/firmware\n"})}),"\n",(0,i.jsxs)(n.p,{children:["with the ",(0,i.jsx)(n.code,{children:"/shared"})," partition."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"monitor-slurm-installation",children:"Monitor SLURM Installation"}),"\n",(0,i.jsx)(n.p,{children:"Once you create your JupyterLab/CodeEditor instance, it will kick off the LifeCycleConfiguration (LCC). We've configured the LCC so that:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"It installs necessary packages and dependencies"}),"\n",(0,i.jsx)(n.li,{children:"Downloads a script to install SLURM and set up MUNGE authentication"}),"\n",(0,i.jsx)(n.li,{children:"Logs progress to a file on your CodeEditor/JupyterLab instance"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Before being able to run SLURM commands, please wait until the LCC fully installs SLURM and configures your instance as a login node. You can monitor the progress in the logs. To find the log file, head over to ",(0,i.jsx)(n.strong,{children:"CloudWatch"})," --\x3e ",(0,i.jsx)(n.strong,{children:"Logs"})," --\x3e ",(0,i.jsx)(n.strong,{children:"Log Groups"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["In the search box, search for ",(0,i.jsx)(n.strong,{children:"/aws/sagemaker/studio"})," and select it. You will be redirected to all the Log Streams under the ",(0,i.jsx)(n.code,{children:"/aws/sagemaker/studio"})," log group."]}),"\n",(0,i.jsxs)(n.p,{children:["Under ",(0,i.jsx)(n.strong,{children:"Log Streams"}),", search for ",(0,i.jsx)(n.strong,{children:"<your-domain-id>/j/CodeEditor/default/LifecycleConfigOnStart"})," (you can find the domain id from your CloudFormation stack outputs). In the logs, you will see"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Starting background installation. Check /tmp/slurm_lifecycle_20250326_053740.log for progress...\nInstallation started in the background. Monitor the progress with:\ntail -f /tmp/slurm_lifecycle_20250326_053740.log\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Grab the ",(0,i.jsx)(n.code,{children:"tail"})," command, and paste it onto your CodeEditor/JupyterLab terminal. You will see that SLURM is getting installed and configured. This process takes ~5-7 minutes, so go grab a cup of coffee!"]}),"\n",(0,i.jsx)(n.p,{children:"You'll know that SLURM is installed when you see"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Testing Slurm configuration...\nPARTITION     AVAIL  TIMELIMIT  NODES  STATE NODELIST\ndev*             up   infinite      4   idle ip-10-1-4-244,ip-10-1-31-165,ip-10-1-52-212,ip-10-1-90-199\nml.c5.4xlarge    up   infinite      4   idle ip-10-1-4-244,ip-10-1-31-165,ip-10-1-52-212,ip-10-1-90-199\n=======================================\n=======================================\nSLURM is now configured! You can now interact with your cluster from your Studio environment!!\n=======================================\n=======================================\n"})}),"\n",(0,i.jsx)(n.h2,{id:"pitfalls-and-known-issues",children:"Pitfalls and known issues"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["You can't run ",(0,i.jsx)(n.code,{children:"srun"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["You can run all other slurm commands, including ",(0,i.jsx)(n.code,{children:"sbatch"}),", ",(0,i.jsx)(n.code,{children:"squeue"}),", and ",(0,i.jsx)(n.code,{children:"sinfo"}),". However, ",(0,i.jsx)(n.code,{children:"srun"})," requires ",(0,i.jsx)(n.a,{href:"https://slurm.schedmd.com/network.html#client",children:"specific ports"})," to be open for I/O, which isn't possible on Studio IDE containers today. As a workaround, if you MUST run ",(0,i.jsx)(n.code,{children:"srun"}),", try"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Source environment variables, written by your LCC\nsource env_vars\n\n# Run your srun command via ssm\naws ssm start-session \\\n    --target "sagemaker-cluster:${CLUSTER_ID}_${HEAD_NODE_NAME}-${CONTROLLER_ID}" \\\n    --document-name AWS-StartInteractiveCommand \\\n    --parameters \'{\n        "command":["srun -N 4 hostname"]\n    }\'\n'})}),"\n",(0,i.jsxs)(n.p,{children:["By using ssm, you are still using the controller machine to submit ",(0,i.jsx)(n.code,{children:"srun"})," jobs to your cluster nodes."]}),"\n",(0,i.jsxs)(n.p,{children:["We recommend running ",(0,i.jsx)(n.code,{children:"sbatch"})," commands directly instead."]}),"\n",(0,i.jsxs)(n.p,{children:["Example ",(0,i.jsx)(n.code,{children:"sbatch"})," script:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --output=/fsx/&lt;partition_name&gt;/slurm_%j.out\n#SBATCH --error=/fsx/&lt;partition_name&gt;/slurm_%j.err\n#SBATCH --nodes=1\n\necho "Testing write access to /fsx/&lt;partition_name&gt;"\ndate\nhostname\nwhoami\nnvidia-smi\n'})}),"\n",(0,i.jsx)(n.admonition,{title:"Choosing a directory for files:",type:"info",children:(0,i.jsxs)(n.p,{children:["Note: When creating ",(0,i.jsx)(n.code,{children:"sbatch"})," files, make sure you specify your ",(0,i.jsx)(n.code,{children:"--output"})," and ",(0,i.jsx)(n.code,{children:"--error"})," paths to an fsx path that both your Studio User and SLURM user (specified in LCC) have permission to write to. The safest bet would be to specify ",(0,i.jsx)(n.code,{children:"/fsx/&lt;partition_name&gt;/"}),", where ",(0,i.jsx)(n.code,{children:"&lt;partition_name&gt;"})," will either be ",(0,i.jsx)(n.code,{children:"shared"})," or your studio user name, depending on what you set for ",(0,i.jsx)(n.code,{children:"SharedFSx"}),". The permissions are handled via an ACL and automatically done by the LCC scripts."]})}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsx)(n.li,{children:"SLURM failed to set up"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This is a rare occurence, but it may happen because the MUNGE authentication key was incorrectly copied over from the controller machine. To remediate, you can follow the steps in the logs:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'Here are the manual steps you can try:\n\n################################################################################\n#                        Manual MUNGE Key Installation                          #\n################################################################################\n\n1. Source environment variables & create a temporary file for the MUNGE key:\n   source env_vars\n   TEMP_FILE=$(mktemp)\n\n2. Get MUNGE key hexdump:\n   aws ssm start-session \\\n       --target "sagemaker-cluster:${CLUSTER_ID}_${HEAD_NODE_NAME}-${CONTROLLER_ID}" \\\n       --document-name AWS-StartInteractiveCommand \\\n       --parameters \'{"command":["\\n\\n sudo hexdump -C /etc/munge/munge.key"]}\' \\\n       > "${TEMP_FILE}"\n\n3. Convert hexdump to binary and install:\n   cat "${TEMP_FILE}" | grep "^[0-9a-f].*  |" | \\\n       sed \'s/^[0-9a-f]\\{8\\}  //\' | \\\n       cut -d\'|\' -f2 | \\\n       tr -d \'|\\n\' | \\\n       sudo tee /etc/munge/munge.key > /dev/null\n\n4. Restart MUNGE service:\n   sudo service munge restart\n\n5. Verify cluster status:\n   sinfo\n\n6. Cleanup:\n   rm ${TEMP_FILE}\n\nsinfo should work now!\n################################################################################\n'})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8960:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/03-codeditor-fsx-58e0ed3cec176616b5e856742f7e528f.png"},9146:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/02-studio-home-cee67b3fd29407afc9abaec315dbc340.png"},9938:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/07-fsx-shared-ddcbaa129d1adf04cc3d4f34c40991a7.png"}}]);