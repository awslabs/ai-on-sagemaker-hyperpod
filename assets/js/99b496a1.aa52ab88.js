"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6768],{2681:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"eks-blueprints/training/ddp/distributed-data-parallel","title":"Distributed Data Parallel (DDP)","description":"This guide provides step-by-step instructions for setting up Distributed Data Parallel (DDP) training on EKS using PyTorch.","source":"@site/docs/01-eks-blueprints/training/ddp/distributed-data-parallel.md","sourceDirName":"01-eks-blueprints/training/ddp","slug":"/eks-blueprints/training/ddp/distributed-data-parallel","permalink":"/docs/eks-blueprints/training/ddp/distributed-data-parallel","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Distributed Data Parallel (DDP)","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Training","permalink":"/docs/category/training"},"next":{"title":"Fully Sharded Data Parallelism (FSDP)","permalink":"/docs/eks-blueprints/training/fsdp/fully-sharded-data-parallel"}}');var t=s(4848),r=s(8453);const o={title:"Distributed Data Parallel (DDP)",sidebar_position:2},a="Get Started Training a Model using PyTorch DDP in 5 Minutes (CPU)",l={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Infrastructure Requirements",id:"infrastructure-requirements",level:3},{value:"Development Environment",id:"development-environment",level:3},{value:"AWS Permissions",id:"aws-permissions",level:3},{value:"Cluster Validation",id:"cluster-validation",level:3},{value:"Step 1: Setup Your Training Job Image",id:"step-1-setup-your-training-job-image",level:2},{value:"1.1 Clone the Repository",id:"11-clone-the-repository",level:3},{value:"1.2 Build a Docker Image",id:"12-build-a-docker-image",level:3},{value:"1.3 Push the Image to Amazon ECR",id:"13-push-the-image-to-amazon-ecr",level:3},{value:"Step 2: Preparing Your Training Job Script",id:"step-2-preparing-your-training-job-script",level:2},{value:"2.1 Install envsubst",id:"21-install-envsubst",level:3},{value:"2.2 Generate Manifest from Template",id:"22-generate-manifest-from-template",level:3},{value:"Step 3: Deploy Training Workload",id:"step-3-deploy-training-workload",level:2},{value:"3.1 Deploy the Training Job",id:"31-deploy-the-training-job",level:3},{value:"3.2 Monitor",id:"32-monitor",level:3},{value:"3.3 Stop",id:"33-stop",level:3},{value:"Alternative: Start Your Training Job Execution (Simple Method)",id:"alternative-start-your-training-job-execution-simple-method",level:2},{value:"4.1 Clone the Repository",id:"41-clone-the-repository",level:3},{value:"4.2 Deploy Training Workload",id:"42-deploy-training-workload",level:3},{value:"4.3 Monitor",id:"43-monitor",level:3},{value:"4.4 Stop",id:"44-stop",level:3}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{Details:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"get-started-training-a-model-using-pytorch-ddp-in-5-minutes-cpu",children:"Get Started Training a Model using PyTorch DDP in 5 Minutes (CPU)"})}),"\n",(0,t.jsx)(n.p,{children:"This guide provides step-by-step instructions for setting up Distributed Data Parallel (DDP) training on EKS using PyTorch."}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before starting, ensure you have completed the following setup:"}),"\n",(0,t.jsx)(n.h3,{id:"infrastructure-requirements",children:"Infrastructure Requirements"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"SageMaker HyperPod EKS cluster"})," deployed and running"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"EKS node groups"})," with appropriate instance types (e.g., ml.m5.2xlarge)"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"FSx for Lustre filesystem"})," provisioned and mounted to the cluster"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Kubeflow Training Operator"})," installed on the cluster"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"development-environment",children:"Development Environment"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"AWS CLI v2"})," installed and configured with appropriate permissions"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"kubectl"})," installed and configured to access your EKS cluster"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Docker"})," installed on your development machine"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"envsubst"})," utility for template processing"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Git"})," for cloning repositories"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"aws-permissions",children:"AWS Permissions"}),"\n",(0,t.jsx)(n.p,{children:"Your AWS credentials should have permissions for:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Amazon ECR"})," - push/pull container images"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Amazon EKS"})," - access cluster resources"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Amazon FSx"})," - access shared filesystem"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Amazon EC2"})," - describe instances and availability zones"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"AWS STS"})," - get caller identity"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"cluster-validation",children:"Cluster Validation"}),"\n",(0,t.jsx)(n.p,{children:"Verify your cluster is ready:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check cluster status\nkubectl get nodes\n\n# Verify Kubeflow Training Operator is running\nkubectl get pods -n kubeflow\n\n# Check FSx storage is available\nkubectl get pvc\n\n# Verify you can create resources\nkubectl auth can-i create pytorchjobs\n"})}),"\n",(0,t.jsx)(n.h2,{id:"step-1-setup-your-training-job-image",children:"Step 1: Setup Your Training Job Image"}),"\n",(0,t.jsx)(n.h3,{id:"11-clone-the-repository",children:"1.1 Clone the Repository"}),"\n",(0,t.jsx)(n.p,{children:"The first step is to get the training code and Docker configuration. We'll clone the AWS distributed training examples repository which contains pre-built PyTorch DDP examples optimized for Kubernetes."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ~\ngit clone https://github.com/aws-samples/awsome-distributed-training/\ncd awsome-distributed-training/3.test_cases/pytorch/cpu-ddp/kubernetes\n"})}),"\n",(0,t.jsx)(n.h3,{id:"12-build-a-docker-image",children:"1.2 Build a Docker Image"}),"\n",(0,t.jsxs)(n.p,{children:["Now we'll build a container image that includes PyTorch, the training code, and all necessary dependencies. The ",(0,t.jsx)(n.code,{children:"$DOCKER_NETWORK"})," variable handles SageMaker Studio's specific network requirements if you're running from that environment."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"export AWS_REGION=$(aws ec2 describe-availability-zones --output text --query 'AvailabilityZones[0].[RegionName]')\nexport ACCOUNT=$(aws sts get-caller-identity --query Account --output text)\nexport REGISTRY=${ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com/\ndocker build $DOCKER_NETWORK -t ${REGISTRY}fsdp:pytorch2.2-cpu ..\n"})}),"\n",(0,t.jsxs)(s,{children:[(0,t.jsx)("summary",{children:"Why $DOCKER_NETWORK?"}),(0,t.jsxs)(n.p,{children:["The environment variable ",(0,t.jsx)("code",{children:"$DOCKER_NETWORK"})," is set to ",(0,t.jsx)("code",{children:"--network=sagemaker"})," only if you deployed the SageMaker Studio Code Editor CloudFormation stack in the ",(0,t.jsx)("a",{href:"/docs/category/getting-started",children:"Set Up Your Development Environment"})," section. This is necessary because SageMaker Studio uses a specific network configuration for its containers. Otherwise, it remains unset."]})]}),"\n",(0,t.jsx)(n.p,{children:"Building the image can take 3~5 minutes. If successful, you should see the following success message at the end:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell-session",children:"Successfully built 123ab12345cd\nSuccessfully tagged 123456789012.dkr.ecr.us-east-2.amazonaws.com/fsdp:pytorch2.2-cpu\n"})}),"\n",(0,t.jsx)(n.h3,{id:"13-push-the-image-to-amazon-ecr",children:"1.3 Push the Image to Amazon ECR"}),"\n",(0,t.jsx)(n.p,{children:"In this step we create a container registry if one does not exist, and push the container image to it. This makes the image available to your EKS cluster nodes."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Create registry if needed\nREGISTRY_COUNT=$(aws ecr describe-repositories | grep \\"fsdp\\" | wc -l)\nif [ "$REGISTRY_COUNT" == "0" ]; then\n        aws ecr create-repository --repository-name fsdp\nfi\n\n# Login to registry\necho "Logging in to $REGISTRY ..."\naws ecr get-login-password | docker login --username AWS --password-stdin $REGISTRY\n\n# Push image to registry\ndocker image push ${REGISTRY}fsdp:pytorch2.2-cpu\n'})}),"\n",(0,t.jsx)(n.p,{children:"Pushing the image may take some time depending on your network bandwidth. If you use EC2 / CloudShell as your development machine, it will take 6~8 minutes."}),"\n",(0,t.jsx)(n.h2,{id:"step-2-preparing-your-training-job-script",children:"Step 2: Preparing Your Training Job Script"}),"\n",(0,t.jsx)(n.h3,{id:"21-install-envsubst",children:"2.1 Install envsubst"}),"\n",(0,t.jsxs)(n.p,{children:["This example uses ",(0,t.jsx)(n.a,{href:"https://github.com/a8m/envsubst",children:(0,t.jsx)(n.code,{children:"envsubst"})})," to generate a Kubernetes manifest file from a template file and parameters. If you don't have ",(0,t.jsx)(n.code,{children:"envsubst"})," on your development environment, install it by following the ",(0,t.jsx)(n.a,{href:"https://github.com/a8m/envsubst?tab=readme-ov-file#installation",children:"Installation instruction"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"22-generate-manifest-from-template",children:"2.2 Generate Manifest from Template"}),"\n",(0,t.jsxs)(n.p,{children:["With the ",(0,t.jsx)(n.code,{children:"envsubst"})," command, generate ",(0,t.jsx)(n.code,{children:"fsdp.yaml"})," from ",(0,t.jsx)(n.code,{children:"fsdp.yaml-template"}),". Please configure instance type, number of nodes, number of CPUs, based on your cluster's specification."]}),"\n",(0,t.jsx)(n.p,{children:"You can check your cluster's specification by running the following command:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'kubectl get nodes "-o=custom-columns=NAME:.metadata.name,INSTANCETYPE:.metadata.labels.node\\.kubernetes\\.io/instance-type,CPU:.status.capacity.cpu"\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"NAME                           INSTANCETYPE    CPU\nhyperpod-i-0427a1830f8e4a49e   ml.m5.2xlarge   4\nhyperpod-i-052768f9f54856cd6   ml.m5.2xlarge   4\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Set environment variables and run ",(0,t.jsx)(n.code,{children:"envsubst"})," to generate ",(0,t.jsx)(n.code,{children:"fsdp.yaml"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"For ml.m5.2xlarge x 2:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"export IMAGE_URI=${REGISTRY}fsdp:pytorch2.2-cpu\nexport INSTANCE_TYPE=ml.m5.2xlarge\nexport NUM_NODES=2\nexport CPU_PER_NODE=4\ncat fsdp.yaml-template | envsubst > fsdp.yaml\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The template file assumes that the FSx Lustre volume is claimed as ",(0,t.jsx)(n.code,{children:"fsx-pvc"}),". You can check the claim name of the FSx Lustre filesystem in your cluster by executing the following command."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get pvc\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE\nfsx-claim   Bound    pvc-ed0fd2cb-33da-498d-bab4-bf08cb4d555c   1200Gi     RWX            fsx-sc         <unset>                 6d22h\n"})}),"\n",(0,t.jsxs)(n.p,{children:["If your FSx Lustre volume is claimed in different name than ",(0,t.jsx)(n.code,{children:"fsx-pvc"})," (e.g., ",(0,t.jsx)(n.code,{children:"fsx-claim"}),"), you can execute the following command to update the claim name in ",(0,t.jsx)(n.code,{children:"fsdp.yaml"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sed 's/fsx-pv/fsx-claim/g' -i ./fsdp.yaml\n"})}),"\n",(0,t.jsxs)(n.p,{children:["If you wish the training job to run for longer so you may test resiliency against a running job, increase the number of epochs by increasing the number of epochs specified by the torchrun command in ",(0,t.jsx)(n.code,{children:"fsdp.yaml"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sed 's/5000/50000/g' -i ./fsdp.yaml\n"})}),"\n",(0,t.jsxs)(n.p,{children:["In this example the number of epochs (line 107) was increased from ",(0,t.jsx)(n.code,{children:"5000"})," to ",(0,t.jsx)(n.code,{children:"50000"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"step-3-deploy-training-workload",children:"Step 3: Deploy Training Workload"}),"\n",(0,t.jsx)(n.h3,{id:"31-deploy-the-training-job",children:"3.1 Deploy the Training Job"}),"\n",(0,t.jsxs)(n.p,{children:["Now the manifest file ",(0,t.jsx)(n.code,{children:"fsdp.yaml"})," is generated, and you are ready to deploy the training workload. Run the following command to deploy the training workload."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f ./fsdp.yaml\n"})}),"\n",(0,t.jsx)(n.p,{children:"You should see the following message:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"service/etcd created\ndeployment.apps/etcd created\npytorchjob.kubeflow.org/fsdp created\n"})}),"\n",(0,t.jsx)(n.h3,{id:"32-monitor",children:"3.2 Monitor"}),"\n",(0,t.jsx)(n.p,{children:"To see the status of your job, use the commands below:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get pytorchjob\nkubectl get pods\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"NAME   STATE     AGE\nfsdp   Running   40s\n\nNAME                    READY   STATUS    RESTARTS   AGE\netcd-7787559c74-msgpq   1/1     Running   0          49s\nfsdp-worker-0           1/1     Running   0          49s\nfsdp-worker-1           1/1     Running   0          49s\n"})}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["Note: When you run for the first time, it takes 2~3min until the Pod statuses change from ",(0,t.jsx)(n.code,{children:"ContainerCreating"})," to ",(0,t.jsx)(n.code,{children:"Running"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Each of the pods produces job logs. You can monitor the logs by running the command below."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl logs -f fsdp-worker-0\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"2024-07-19 04:39:07,890] torch.distributed.run: [WARNING] *****************************************\nINFO 2024-07-19 04:39:07,958 Etcd machines: ['http://0.0.0.0:2379']\nINFO 2024-07-19 04:39:07,964 Attempting to join next rendezvous\nINFO 2024-07-19 04:39:07,965 Observed existing rendezvous state: {'status': 'joinable', 'version': '1', 'participants': [0]}\nINFO 2024-07-19 04:39:08,062 Joined rendezvous version 1 as rank 1. Full state: {'status': 'frozen', 'version': '1', 'participants': [0, 1], 'keep_alives': []}\nINFO 2024-07-19 04:39:08,062 Waiting for remaining peers.\nINFO 2024-07-19 04:39:08,063 All peers arrived. Confirming membership.\nINFO 2024-07-19 04:39:08,149 Waiting for confirmations from all peers.\nINFO 2024-07-19 04:39:08,161 Rendezvous version 1 is complete. Final state: {'status': 'final', 'version': '1', 'participants': [0, 1], 'keep_alives': ['/torchelastic/p2p/run_none/rdzv/v_1/rank_1', '/torchelastic/p2p/run_none/rdzv/v_1/rank_0'], 'num_workers_waiting': 0}\nINFO 2024-07-19 04:39:08,161 Creating EtcdStore as the c10d::Store implementation\n...\n[RANK 1] Epoch 4991 | Batchsize: 32 | Steps: 8\nEpoch 4990 | Training snapshot saved at /fsx/snapshot.pt\n[RANK 0] Epoch 4991 | Batchsize: 32 | Steps: 8\n[RANK 1] Epoch 4992 | Batchsize: 32 | Steps: 8\n[RANK 0] Epoch 4992 | Batchsize: 32 | Steps: 8\n[RANK 3] Epoch 4992 | Batchsize: 32 | Steps: 8\n[RANK 2] Epoch 4992 | Batchsize: 32 | Steps: 8\n[RANK 1] Epoch 4993 | Batchsize: 32 | Steps: 8\n[RANK 2] Epoch 4993 | Batchsize: 32 | Steps: 8\n"})}),"\n",(0,t.jsx)(n.h3,{id:"33-stop",children:"3.3 Stop"}),"\n",(0,t.jsx)(n.p,{children:"To stop the current training job, use the following command."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl delete -f ./fsdp.yaml\n"})}),"\n",(0,t.jsx)(n.h2,{id:"alternative-start-your-training-job-execution-simple-method",children:"Alternative: Start Your Training Job Execution (Simple Method)"}),"\n",(0,t.jsx)(n.p,{children:"If you prefer a simpler approach without building your own container image, you can use the pre-built example that doesn't require custom image building."}),"\n",(0,t.jsx)(n.h3,{id:"41-clone-the-repository",children:"4.1 Clone the Repository"}),"\n",(0,t.jsx)(n.p,{children:"Navigate to your home directory or your preferred project directory, clone the repo."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ~\ngit clone https://github.com/aws-samples/awsome-distributed-training/\ncd awsome-distributed-training/3.test_cases/pytorch/cpu-ddp/kubernetes\n"})}),"\n",(0,t.jsx)(n.p,{children:"If you wish to test the resiliency feature, please run the following command to increase the number of training epochs so the job runs longer:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sed 's/5000/50000/g' -i ./fsdp-simple.yaml\n"})}),"\n",(0,t.jsx)(n.h3,{id:"42-deploy-training-workload",children:"4.2 Deploy Training Workload"}),"\n",(0,t.jsx)(n.p,{children:"Run the following command to deploy the training workload."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f ./fsdp-simple.yaml\n"})}),"\n",(0,t.jsx)(n.p,{children:"You should see the following message:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"service/etcd created\ndeployment.apps/etcd created\npytorchjob.kubeflow.org/fsdp created\n"})}),"\n",(0,t.jsx)(n.h3,{id:"43-monitor",children:"4.3 Monitor"}),"\n",(0,t.jsx)(n.p,{children:"To see the status of your job, use the commands below:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get pytorchjob\nkubectl get pods\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"NAME   STATE     AGE\nfsdp   Running   40s\n\nNAME                    READY   STATUS    RESTARTS   AGE\netcd-7787559c74-msgpq   1/1     Running   0          49s\nfsdp-worker-0           1/1     Running   0          49s\nfsdp-worker-1           1/1     Running   0          49s\n"})}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["Note: When you run for the first time, it takes 2~3min until the Pod statuses change from ",(0,t.jsx)(n.code,{children:"ContainerCreating"})," to ",(0,t.jsx)(n.code,{children:"Running"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Each of the pods produces job logs. You can monitor the logs by running the command below."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl logs -f fsdp-worker-0\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"2024-07-19 04:39:07,890] torch.distributed.run: [WARNING] *****************************************\nINFO 2024-07-19 04:39:07,958 Etcd machines: ['http://0.0.0.0:2379']\nINFO 2024-07-19 04:39:07,964 Attempting to join next rendezvous\nINFO 2024-07-19 04:39:07,965 Observed existing rendezvous state: {'status': 'joinable', 'version': '1', 'participants': [0]}\nINFO 2024-07-19 04:39:08,062 Joined rendezvous version 1 as rank 1. Full state: {'status': 'frozen', 'version': '1', 'participants': [0, 1], 'keep_alives': []}\nINFO 2024-07-19 04:39:08,062 Waiting for remaining peers.\nINFO 2024-07-19 04:39:08,063 All peers arrived. Confirming membership.\nINFO 2024-07-19 04:39:08,149 Waiting for confirmations from all peers.\nINFO 2024-07-19 04:39:08,161 Rendezvous version 1 is complete. Final state: {'status': 'final', 'version': '1', 'participants': [0, 1], 'keep_alives': ['/torchelastic/p2p/run_none/rdzv/v_1/rank_1', '/torchelastic/p2p/run_none/rdzv/v_1/rank_0'], 'num_workers_waiting': 0}\nINFO 2024-07-19 04:39:08,161 Creating EtcdStore as the c10d::Store implementation\n...\n[RANK 1] Epoch 4991 | Batchsize: 32 | Steps: 8\nEpoch 4990 | Training snapshot saved at /fsx/snapshot.pt\n[RANK 0] Epoch 4991 | Batchsize: 32 | Steps: 8\n[RANK 1] Epoch 4992 | Batchsize: 32 | Steps: 8\n[RANK 0] Epoch 4992 | Batchsize: 32 | Steps: 8\n[RANK 3] Epoch 4992 | Batchsize: 32 | Steps: 8\n[RANK 2] Epoch 4992 | Batchsize: 32 | Steps: 8\n[RANK 1] Epoch 4993 | Batchsize: 32 | Steps: 8\n[RANK 2] Epoch 4993 | Batchsize: 32 | Steps: 8\n"})}),"\n",(0,t.jsx)(n.h3,{id:"44-stop",children:"4.4 Stop"}),"\n",(0,t.jsx)(n.p,{children:"To stop the current training job, use the following command."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl delete -f ./fsdp-simple.yaml\n"})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var i=s(6540);const t={},r=i.createContext(t);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);