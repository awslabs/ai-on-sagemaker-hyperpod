"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6402],{652:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/auto-resume-3-ef90e85ae2e391e764a0889f8fbbe167.png"},1918:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/auto-resume-1-25e49419d76fd807db88e7cbf8a840f3.png"},2297:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"validation-and-testing/resiliency/eks-resiliency","title":"Testing Resiliency with HyperPod EKS","description":"This guide demonstrates how to test and validate the resiliency features of SageMaker HyperPod when using EKS as the orchestrator. You\'ll learn how to monitor node health, manually trigger node replacement/reboot, simulate failures, and test job auto-resume functionality.","source":"@site/docs/06-validation-and-testing/resiliency/eks-resiliency.md","sourceDirName":"06-validation-and-testing/resiliency","slug":"/validation-and-testing/resiliency/eks-resiliency","permalink":"/docs/validation-and-testing/resiliency/eks-resiliency","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Testing Resiliency with HyperPod EKS","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Resiliency Overview","permalink":"/docs/validation-and-testing/resiliency/overview"},"next":{"title":"Testing Resiliency with HyperPod Slurm","permalink":"/docs/validation-and-testing/resiliency/slurm-resiliency"}}');var o=a(4848),s=a(8453);const i={title:"Testing Resiliency with HyperPod EKS",sidebar_position:2},r="Testing Resiliency with HyperPod EKS",l={},c=[{value:"1.Manual Replacement or Reboot",id:"1manual-replacement-or-reboot",level:2},{value:"2.Emulate Instance Failure",id:"2emulate-instance-failure",level:2},{value:"connect to one of the nodes in the cluster using SSM agent",id:"connect-to-one-of-the-nodes-in-the-cluster-using-ssm-agent",level:4},{value:"Inject the following commands on the instance to emulate the instance failure to trigger instance replacement:",id:"inject-the-following-commands-on-the-instance-to-emulate-the-instance-failure-to-trigger-instance-replacement",level:4},{value:"Inject the following commands on the instance to emulate the instance failure to trigger instance reboot:",id:"inject-the-following-commands-on-the-instance-to-emulate-the-instance-failure-to-trigger-instance-reboot",level:4},{value:"3.Enable Job Auto Resume",id:"3enable-job-auto-resume",level:2},{value:"Add Auto Resume annotations",id:"add-auto-resume-annotations",level:3},{value:"Trigger job failure",id:"trigger-job-failure",level:3},{value:"Check job status and Node status",id:"check-job-status-and-node-status",level:3}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",div:"div",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"testing-resiliency-with-hyperpod-eks",children:"Testing Resiliency with HyperPod EKS"})}),"\n",(0,o.jsx)(n.p,{children:"This guide demonstrates how to test and validate the resiliency features of SageMaker HyperPod when using EKS as the orchestrator. You'll learn how to monitor node health, manually trigger node replacement/reboot, simulate failures, and test job auto-resume functionality."}),"\n",(0,o.jsx)(n.h2,{id:"1manual-replacement-or-reboot",children:"1.Manual Replacement or Reboot"}),"\n",(0,o.jsx)(n.p,{children:"To watch the status of your cluster nodes, please run the following command:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"watch kubectl get nodes -L sagemaker.amazonaws.com/node-health-status\n"})}),"\n",(0,o.jsxs)(n.p,{children:["You can press Ctrl-C anytime to exit the watch or execute the line without the ",(0,o.jsx)(n.code,{children:"watch"})," prefix to show node list just one time."]}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsx)(n.p,{children:"What if we want to manually replace or reboot a node?"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["In order to manually reboot a node, we can run the following command, where ",(0,o.jsx)(n.code,{children:"hyperpod-i-0220224e40218ce3a"})," is the name of the node you want to reboot:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"  kubectl label node hyperpod-i-0220224e40218ce3a \\\n  sagemaker.amazonaws.com/node-health-status=UnschedulablePendingReboot \\\n  --overwrite=true\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Once executed, you would be able to see the node being labeled as ",(0,o.jsx)(n.code,{children:"UnschedulablePendingReboot"})," and see status change to ",(0,o.jsx)(n.code,{children:"NotReady"}),"."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:"NAME                           STATUS     ROLES    AGE     VERSION                NODE-HEALTH-STATUS\nhyperpod-i-0220224e40218ce3a   NotReady   <none>   6m     v1.29.3-eks-ae9a62a    UnschedulablePendingReboot\nhyperpod-i-06c561302ab149bb7   Ready      <none>   4m28s   v1.29.3-eks-ae9a62a    Schedulable\n"})}),"\n",(0,o.jsx)(n.p,{children:"Soon after, the node would reboot and become available again."}),"\n",(0,o.jsxs)(n.p,{children:["In order to manually replace a node we can run the following command, where ",(0,o.jsx)(n.code,{children:"hyperpod-i-0220224e40218ce3a"})," is the name of the node you want to replace:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"  kubectl label node hyperpod-i-0220224e40218ce3a \\\n  sagemaker.amazonaws.com/node-health-status=UnschedulablePendingReplacement \\\n  --overwrite=true\n"})}),"\n",(0,o.jsxs)(n.p,{children:["After a while (< 1min), the node status changes from ",(0,o.jsx)(n.code,{children:"Ready"})," to ",(0,o.jsx)(n.code,{children:"NotReady"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:"NAME                           STATUS     ROLES    AGE     VERSION                NODE-HEALTH-STATUS\nhyperpod-i-0220224e40218ce3a   NotReady   <none>   13m     v1.29.3-eks-ae9a62a    UnschedulablePendingReplacement\nhyperpod-i-06c561302ab149bb7   Ready      <none>   4m28s   v1.29.3-eks-ae9a62a    Schedulable\n"})}),"\n",(0,o.jsx)(n.p,{children:"After that, the node disappears from the node list:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes -L sagemaker.amazonaws.com/node-health-status\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:"NAME                           STATUS   ROLES    AGE     VERSION                  NODE-HEALTH-STATUS\nhyperpod-i-06c561302ab149bb7   Ready    <none>   5m16s   v1.29.3-eks-ae9a62a      Schedulable\n"})}),"\n",(0,o.jsx)(n.p,{children:"When a new node is initialized, it is added to the list."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes -L sagemaker.amazonaws.com/node-health-status\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:"NAME                           STATUS   ROLES    AGE     VERSION                  NODE-HEALTH-STATUS\nhyperpod-i-06c561302ab149bb7   Ready    <none>   7m56s   v1.29.3-eks-ae9a62a      Schedulable\nhyperpod-i-0cb64f158c17be463   Ready    <none>   16s     v1.29.3-eks-ae9a62a      Schedulable\n"})}),"\n",(0,o.jsx)(n.p,{children:"You can monitor the progress of the node replacement also on the HyperPod management console."}),"\n",(0,o.jsx)(n.h2,{id:"2emulate-instance-failure",children:"2.Emulate Instance Failure"}),"\n",(0,o.jsx)(n.p,{children:"This section depicts an example on how to inject an error in order to test automatic node replacement."}),"\n",(0,o.jsx)(n.h4,{id:"connect-to-one-of-the-nodes-in-the-cluster-using-ssm-agent",children:"connect to one of the nodes in the cluster using SSM agent"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"aws ssm start-session --target sagemaker-cluster:<hyperpod-cluster-id>_<node-group-name>-<instance-id>  --region <aws-region>\n\n"})}),"\n",(0,o.jsx)(n.h4,{id:"inject-the-following-commands-on-the-instance-to-emulate-the-instance-failure-to-trigger-instance-replacement",children:"Inject the following commands on the instance to emulate the instance failure to trigger instance replacement:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'sudo sh -c "sleep 1 && echo \\"$(date \'+%b %d %H:%M:%S\') $(hostname) kernel: NVRM: Xid (PCI:0000:b9:00): 74, pid=<unknown>, name=<unknown>, NVLink: fatal error detected on link 6(0x10000, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\\" >> /var/log/messages"\n'})}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsx)(n.p,{children:"change the date to be current before injecting."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Once this is done, you can notice the node label will change to 'UnschedulablePendingReplacement'"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes --show-labels\n"})}),"\n",(0,o.jsx)(n.h4,{id:"inject-the-following-commands-on-the-instance-to-emulate-the-instance-failure-to-trigger-instance-reboot",children:"Inject the following commands on the instance to emulate the instance failure to trigger instance reboot:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'sudo sh -c "sleep 1 && echo \\"$(date \'+%b %d %H:%M:%S\') $(hostname) kernel: NVRM: Xid (PCI:0000:b9:00): 73, pid=<unknown>, name=<unknown>, NVLink: fatal error detected on link 6(0x10000, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\\" >> /var/log/messages"\n'})}),"\n",(0,o.jsx)(n.p,{children:"Once this is done, you will see the node label change to 'UnschedulablePendingReboot'"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes --show-labels\n"})}),"\n",(0,o.jsx)(n.h2,{id:"3enable-job-auto-resume",children:"3.Enable Job Auto Resume"}),"\n",(0,o.jsx)(n.p,{children:"This section describes how to run a training job with the SageMaker HyperPod Job auto-resume functionality, which provides a zero-touch resiliency infrastructure to automatically recover a training job from the last saved checkpoint in the event of a hardware failure for clusters. SageMaker HyperPod with EKS currently supports Job auto-resume feature when using Pytorch Training Operator for orchestrating jobs."}),"\n",(0,o.jsx)(n.p,{children:"Below steps explain how to setup and test Job Auto Resume for your training job."}),"\n",(0,o.jsx)(n.h3,{id:"add-auto-resume-annotations",children:"Add Auto Resume annotations"}),"\n",(0,o.jsx)(n.p,{children:"The following code snippet is an example of how to modify a Kubeflow PyTorch job YAML configuration to enable the job auto-resume functionality. You need to add two annotations and set restartPolicy to OnFailure as shown below. It is recommended to also set nodeSelector to use node that have node-health-status as Schedulable."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'\n#Add auto resume annotations\napiVersion: "kubeflow.org/v1"\nkind: PyTorchJob\nmetadata:\n  name: fsdp\n  namespace: kubeflow\n  annotations: {\n      sagemaker.amazonaws.com/enable-job-auto-resume: "true",\n      sagemaker.amazonaws.com/job-max-retry-count: "2"\n  }\n# Set restart policy to onFailure\nspec:\n ....\n  pytorchReplicaSpecs:\n    Worker:\n      replicas: 2\n      restartPolicy: OnFailure\n\n# Set node selector to only use Schedulable nodes\n      spec:\n          ....\n          nodeSelector:\n            sagemaker.amazonaws.com/node-health-status : Schedulable\n'})}),"\n",(0,o.jsxs)(n.div,{children:["when using etcd for Rendezvous increase the timeout for the launcher by passing ",(0,o.jsx)(n.code,{children:"--rdzv-conf=timeout=1800"})," as a parameter to torchrun as shown below. This is needed to account for the time taken to replace the node and run health checks."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"- /usr/local/bin/torchrun\n  - --nproc_per_node=8\n  - --nnodes=2\n  - --rdzv-conf=timeout=1800\n"})}),"\n",(0,o.jsx)(n.h3,{id:"trigger-job-failure",children:"Trigger job failure"}),"\n",(0,o.jsxs)(n.p,{children:["Once the above changes are made and the job is running successfully. In order to test auto-resume we can emulate failure by either injecting an error into one of the node or manually triggering node replacement. Please follow the previous sections ",(0,o.jsx)(n.a,{href:"#2emulate-instance-failure",children:"Emulate Failure"})," / ",(0,o.jsx)(n.a,{href:"#1manual-replacement-or-reboot",children:"Manual Replacement"})," to trigger job failure."]}),"\n",(0,o.jsx)(n.h3,{id:"check-job-status-and-node-status",children:"Check job status and Node status"}),"\n",(0,o.jsx)(n.p,{children:"Once you inject the failure , the job status should automatically show that the job is restarting. Use the kubectl describe command to"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"kubectl describe pytorchjob <jobname>\n"})}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsx)(n.p,{children:"Note - Replace the jobname in the above command with the actual jobname."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"The Job AutoResume watcher automatically brings down the job and restarts it. You should see in the events section an event for job restar as shown below."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"import grafana dashboard",src:a(1918).A+"",width:"3238",height:"362"})}),"\n",(0,o.jsx)(n.p,{children:"The pod status should show as pending as shown below when you run"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -o wide\n"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"import grafana dashboard",src:a(3717).A+"",width:"2946",height:"172"})}),"\n",(0,o.jsx)(n.p,{children:"You should also notice the faulty node marked as unschedulablependingreplacement when you check the node label"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes -L node.kubernetes.io/instance-type,sagemaker.amazonaws.com/node-health-status,sagemaker.amazonaws.com/deep-health-check-status\n"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"import grafana dashboard",src:a(652).A+"",width:"1732",height:"181"})}),"\n",(0,o.jsx)(n.p,{children:"Once a node becomes available the pod that is in pending should get scheduled and the job should restart again."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},3717:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/auto-resume-2-951c429af0713a789fd315b800b92b83.png"},8453:(e,n,a)=>{a.d(n,{R:()=>i,x:()=>r});var t=a(6540);const o={},s=t.createContext(o);function i(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);