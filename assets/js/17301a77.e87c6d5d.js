"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[426],{1110:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"slurm-blueprints/training/fsdp/fully-sharded-data-parallel","title":"Fully Sharded Data Parallel","description":"TODO: do we really need that?","source":"@site/docs/02-slurm-blueprints/training/fsdp/fully-sharded-data-parallel.md","sourceDirName":"02-slurm-blueprints/training/fsdp","slug":"/slurm-blueprints/training/fsdp/fully-sharded-data-parallel","permalink":"/docs/slurm-blueprints/training/fsdp/fully-sharded-data-parallel","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Fully Sharded Data Parallel","sidebar_position":1,"sidebar_title":"Fully Sharded Data Parallel"},"sidebar":"tutorialSidebar","previous":{"title":"PyTorch DDP on CPU","permalink":"/docs/slurm-blueprints/training/ddp/distributed-data-parallel"},"next":{"title":"NVIDIA Megatron-LM","permalink":"/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme"}}');var i=t(4848),a=t(8453);const s={title:"Fully Sharded Data Parallel",sidebar_position:1,sidebar_title:"Fully Sharded Data Parallel"},l="Get Started Training Llama 2 with PyTorch FSDP in 5 Minutes",d={},o=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Setup",id:"setup",level:2},{value:"Create Environment",id:"create-environment",level:3},{value:"Data",id:"data",level:3},{value:"Training",id:"training",level:2},{value:"Create HuggingFace Token",id:"create-huggingface-token",level:3},{value:"Launch Training",id:"launch-training",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"TODO: do we really need that?"}),"\n",(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"get-started-training-llama-2-with-pytorch-fsdp-in-5-minutes",children:"Get Started Training Llama 2 with PyTorch FSDP in 5 Minutes"})}),"\n",(0,i.jsxs)(n.p,{children:["These scripts provide an easy way to get started with multinode ",(0,i.jsx)(n.a,{href:"https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html",children:"FSDP"})," training on Slurm. It is designed to be as simple as possible, requires no data preparation, and uses a simple Conda environment."]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.p,{children:["Before running this training, you'll need to create a Hyperpod cluster with an FSx for Lustre file system. Instructions can be found in ",(0,i.jsx)(n.a,{href:"/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup",children:"1. Cluster Setup"}),". Please follow them if you haven't done so already."]}),"\n",(0,i.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,i.jsx)(n.h3,{id:"create-environment",children:"Create Environment"}),"\n",(0,i.jsx)(n.p,{children:"On your cluster head node:"}),"\n",(0,i.jsx)(n.p,{children:"TODO: we should stick with a single place for the ADT repo. Make this part of the initial cluster setup. Add it ot FSx. Every customer use it, so let's make it part of the required steps to deploy the cluster."}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Navigate to your home directory (assuming this was setup as a shared directory) and clone the repo:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ~\ngit clone https://github.com/aws-samples/awsome-distributed-training/\ncd awsome-distributed-training/3.test_cases/pytorch/FSDP/slurm\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Run the ",(0,i.jsx)(n.code,{children:"create_venv.sh"})," script."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["This script will first download and install ",(0,i.jsx)(n.a,{href:"https://docs.conda.io/projects/miniconda/en/latest/",children:"Miniconda"}),", then create a Conda env called ",(0,i.jsx)(n.code,{children:"pt_fsdp"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"By creating this environment on the shared FSx for Lustre volume, all compute nodes in our cluster will have access to it."}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:". ./create_venv.sh\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"data",children:"Data"}),"\n",(0,i.jsxs)(n.p,{children:["For this example, we'll be using the ",(0,i.jsx)(n.a,{href:"https://huggingface.co/datasets/allenai/c4",children:"allenai/c4"})," dataset. Instead of downloading the whole thing, the ",(0,i.jsx)(n.code,{children:"create_streaming_dataloaders"})," function will stream the dataset from ",(0,i.jsx)(n.a,{href:"https://huggingface.co/datasets",children:"HuggingFace"}),", so there's no data prep required for running this training."]}),"\n",(0,i.jsxs)(n.p,{children:["If you'd like to instead use your own dataset, you can do so by ",(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/datasets/create_dataset",children:"formatting it as a HuggingFace dataset"}),", and passing its location to the ",(0,i.jsx)(n.code,{children:"--dataset_path"})," argument."]}),"\n",(0,i.jsx)(n.h2,{id:"training",children:"Training"}),"\n",(0,i.jsx)(n.h3,{id:"create-huggingface-token",children:"Create HuggingFace Token"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"For this dataset, we will need a Hugging Face access token"}),". First, create a ",(0,i.jsx)(n.a,{href:"https://huggingface.co/welcome",children:"Hugging Face account"}),". Then ",(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"generate your access token with read permissions"}),". Set your HuggingFace Token as an environment variable in your Python Virtual Environment by running:"]}),"\n",(0,i.jsx)(n.p,{children:"TODO: we ask customers to create HF TOKENS all the time. As this is a pre-requisites for most of the content on the ADT repo, we SHOULD move this to the initial cluster setup part of the content."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"export HF_TOKEN=<YOUR HF ACCESS TOKEN>\n"})}),"\n",(0,i.jsx)(n.h3,{id:"launch-training",children:"Launch Training"}),"\n",(0,i.jsxs)(n.p,{children:["The script to launch a Slurm batch training job can be found in ",(0,i.jsx)(n.code,{children:"llama2_7b-training.sbatch"}),". You can adjust the number of training nodes by modifying ",(0,i.jsx)(n.code,{children:"#SBATCH --nodes=4"}),". You can also adjust the training parameters in ",(0,i.jsx)(n.code,{children:"TRAINING_ARGS"}),". Additional parameters can be found in ",(0,i.jsx)(n.code,{children:"model_utils/arguments.py"}),". Note that we use the same directory for both ",(0,i.jsx)(n.code,{children:"--checkpoint_dir"})," and ",(0,i.jsx)(n.code,{children:"--resume_from_checkpoint"}),". If there are multiple checkpoints, ",(0,i.jsx)(n.code,{children:"--resume_from_checkpoint"})," will automatically select the most recent one. This way if our training is interrupted for any reason, it will automatically pick up the most recent checkpoint."]}),"\n",(0,i.jsx)(n.p,{children:"To launch your training, run"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"sbatch llama2_7b-training.sbatch\n"})}),"\n",(0,i.jsxs)(n.p,{children:["You'll find a new file in the ",(0,i.jsx)(n.code,{children:"logs"})," directory of the form ",(0,i.jsx)(n.code,{children:"logs/llama2_7b-FSDP_[JOB ID].out"}),". This will be continuously updated with your training logs. Don't be worried if you see a long stream of NCCL logs (we prefer to use ",(0,i.jsx)(n.code,{children:"NCCL_DEBUG=INFO"})," for verbose logging). After about a minute, you should see your model training, with an output similar to below for Llama2 :"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"+ TORCHRUN_ARGS=('--nproc_per_node=8' '--nnodes=4' '--rdzv_id=2513' '--rdzv_backend=c10d' '--rdzv_endpoint=p5-dy-gpu-1')\n+ TORCHRUN=torchrun\n+ export TRAIN_SCRIPT=./train.py\n+ TRAIN_SCRIPT=./train.py\n+ TRAINING_ARGS=('--max_context_width=4096' '--num_key_value_heads=32' '--intermediate_size=11008' '--hidden_width=4096' '--num_layers=32' '--num_heads=32' '--model_type=llama_v2' '--tokenizer=hf-internal-testing/llama-tokenizer' '--checkpoint_freq=5000' '--validation_freq=500' '--max_steps=5000' '--checkpoint_dir=./checkpoints' '--dataset=c4' '--dataset_config_name=en' '--resume_from_checkpoint=./checkpoints' '--train_batch_size=1' '--val_batch_size=1' '--sharding_strategy=full' '--offload_activations=1')\n...\n0: 2025-04-04 19:56:52 I [train.py:156] Creating Model\n0: 2025-04-04 19:57:57 I [train.py:172] Created model with total parameters: 6889410560 (6.89 B)\n...\n1: p5-dy-gpu-2:62571:62571 [1] NCCL INFO NCCL version 2.26.2+cuda12.2\n1: p5-dy-gpu-2:62574:62574 [4] NCCL INFO cudaDriverVersion 12040\n2: p5-dy-gpu-3:60823:61204 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0\n2: p5-dy-gpu-3:60823:61204 [2] NCCL INFO NET/OFI Using Libfabric version 1.22\n...\n0: 2025-04-04 19:58:26 I [train.py:103] Batch 0 Loss: 11.63327, Speed: 2.80 samples/sec, lr: 0.000006\n0: 2025-04-04 19:58:28 I [train.py:103] Batch 1 Loss: 11.64674, Speed: 17.06 samples/sec, lr: 0.000013\n0: 2025-04-04 19:58:30 I [train.py:103] Batch 2 Loss: 11.56934, Speed: 17.61 samples/sec, lr: 0.000019\n0: 2025-04-04 19:58:32 I [train.py:103] Batch 3 Loss: 11.30075, Speed: 17.66 samples/sec, lr: 0.000025\n0: 2025-04-04 19:58:33 I [train.py:103] Batch 4 Loss: 11.00539, Speed: 17.66 samples/sec, lr: 0.000031\n0: 2025-04-04 19:58:35 I [train.py:103] Batch 5 Loss: 10.39471, Speed: 17.28 samples/sec, lr: 0.000038\n"})}),"\n",(0,i.jsxs)(n.p,{children:["To modify training for a 13 or 70B Llama 2 model, just change the corresponding parameters based on the values in the ",(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/2307.09288",children:"Llama 2 paper"}),"."]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Param"}),(0,i.jsx)(n.th,{children:"7B"}),(0,i.jsx)(n.th,{children:"13B"}),(0,i.jsx)(n.th,{children:"70B"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"llama_intermediate_size"}),(0,i.jsx)(n.td,{children:"11008"}),(0,i.jsx)(n.td,{children:"13824"}),(0,i.jsx)(n.td,{children:"28672"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"num_key_value_heads"}),(0,i.jsx)(n.td,{children:"32"}),(0,i.jsx)(n.td,{children:"40"}),(0,i.jsx)(n.td,{children:"8"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"hidden_width"}),(0,i.jsx)(n.td,{children:"4096"}),(0,i.jsx)(n.td,{children:"5120"}),(0,i.jsx)(n.td,{children:"8192"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"num_layers"}),(0,i.jsx)(n.td,{children:"32"}),(0,i.jsx)(n.td,{children:"40"}),(0,i.jsx)(n.td,{children:"80"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"num_heads"}),(0,i.jsx)(n.td,{children:"32"}),(0,i.jsx)(n.td,{children:"40"}),(0,i.jsx)(n.td,{children:"64"})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:["If you need to cancel or modify your job, see the Slurm commands available in the ",(0,i.jsx)(n.a,{href:"https://slurm.schedmd.com/quickstart.html",children:"Slurm documentation"}),"."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>l});var r=t(6540);const i={},a=r.createContext(i);function s(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);