"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5260],{483:(n,e,t)=>{t.d(e,{A:()=>i});const i=t.p+"assets/images/sky_interactive-5cfc59a3cffcb19cd443bcc2ad51963b.png"},4239:(n,e,t)=>{t.d(e,{A:()=>i});const i=t.p+"assets/images/sky_check-6fbb237db20099351d59498e6fe22a8c.png"},5388:(n,e,t)=>{t.d(e,{A:()=>i});const i=t.p+"assets/images/sky_show-83eef74a531f5cdca48c3229c851e2e0.png"},5914:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"add-ons/integrations/skypilot/SkyPilot","title":"SkyPilot","description":"Setup SkyPilot","source":"@site/docs/04-add-ons/integrations/skypilot/SkyPilot.md","sourceDirName":"04-add-ons/integrations/skypilot","slug":"/add-ons/integrations/skypilot/","permalink":"/docs/add-ons/integrations/skypilot/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"SkyPilot","sidebar_position":2,"sidebar_title":"SkyPilot"},"sidebar":"tutorialSidebar","previous":{"title":"Ray","permalink":"/docs/add-ons/integrations/Ray/"},"next":{"title":"Cluster Validation","permalink":"/docs/add-ons/Scripts/Cluster Validation/"}}');var s=t(4848),a=t(8453);const o={title:"SkyPilot",sidebar_position:2,sidebar_title:"SkyPilot"},r="Installing SkyPilot and integrating SLURM and EKS",l={},c=[{value:"Setup SkyPilot",id:"setup-skypilot",level:2},{value:"Install SkyPilot with Kubernetes Support",id:"install-skypilot-with-kubernetes-support",level:3},{value:"Connect to Your SageMaker HyperPod EKS Cluster",id:"connect-to-your-sagemaker-hyperpod-eks-cluster",level:3},{value:"Verify SkyPilot&#39;s Connection to the Kubernetes Cluster",id:"verify-skypilots-connection-to-the-kubernetes-cluster",level:3},{value:"Discover Available GPUs in the Cluster",id:"discover-available-gpus-in-the-cluster",level:3},{value:"Using SkyPilot interactively",id:"using-skypilot-interactively",level:2},{value:"Launch an Interactive Development Environment",id:"launch-an-interactive-development-environment",level:3},{value:"Launching training jobs using SkyPilot",id:"launching-training-jobs-using-skypilot",level:2},{value:"Creating and submitting your training job",id:"creating-and-submitting-your-training-job",level:2},{value:"Running Multi-node Training Jobs with EFA",id:"running-multi-node-training-jobs-with-efa",level:3}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",p:"p",pre:"pre",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"installing-skypilot-and-integrating-slurm-and-eks",children:"Installing SkyPilot and integrating SLURM and EKS"})}),"\n",(0,s.jsx)(e.h2,{id:"setup-skypilot",children:"Setup SkyPilot"}),"\n",(0,s.jsx)(e.h3,{id:"install-skypilot-with-kubernetes-support",children:"Install SkyPilot with Kubernetes Support"}),"\n",(0,s.jsx)(e.p,{children:"Start by installing SkyPilot with Kubernetes support using pip:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"pip install skypilot-nightly[kubernetes]\n"})}),"\n",(0,s.jsx)(e.p,{children:"This installs the latest nightly build of SkyPilot, which includes the necessary Kubernetes integrations."}),"\n",(0,s.jsx)(e.h3,{id:"connect-to-your-sagemaker-hyperpod-eks-cluster",children:"Connect to Your SageMaker HyperPod EKS Cluster"}),"\n",(0,s.jsx)(e.p,{children:"First, configure your kubectl to connect to your EKS cluster:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"aws eks update-kubeconfig --name $EKS_CLUSTER_NAME --region $EKS_REGION\n"})}),"\n",(0,s.jsxs)(e.p,{children:["Replace ",(0,s.jsx)(e.code,{children:"$EKS_CLUSTER_NAME"})," and ",(0,s.jsx)(e.code,{children:"$EKS_REGION"})," with your specific EKS cluster name and AWS region."]}),"\n",(0,s.jsx)(e.h3,{id:"verify-skypilots-connection-to-the-kubernetes-cluster",children:"Verify SkyPilot's Connection to the Kubernetes Cluster"}),"\n",(0,s.jsx)(e.p,{children:"Check if SkyPilot can connect to your Kubernetes cluster:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"sky check k8s\n"})}),"\n",(0,s.jsx)(e.p,{children:"The output should look similar to:"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.img,{alt:"Verify SkyPilot&#39;s Connection to the Cluster",src:t(4239).A+"",width:"1695",height:"323"})}),"\n",(0,s.jsx)(e.p,{children:"If this is your first time using SkyPilot with this Kubernetes cluster, you may see a hint to create GPU labels for your nodes. Follow the instructions by running:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"python -m sky.utils.kubernetes.gpu_labeler --context <your-eks-context>\n"})}),"\n",(0,s.jsx)(e.p,{children:"This script helps SkyPilot identify what GPU resources are available on each node in your cluster."}),"\n",(0,s.jsx)(e.h3,{id:"discover-available-gpus-in-the-cluster",children:"Discover Available GPUs in the Cluster"}),"\n",(0,s.jsx)(e.p,{children:"To see what GPU resources are available in your SageMaker HyperPod cluster:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"sky show-gpus --cloud k8s\n"})}),"\n",(0,s.jsx)(e.p,{children:"This will list all available GPU types and their counts:"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.img,{alt:"Available GPUs",src:t(5388).A+"",width:"1553",height:"305"})}),"\n",(0,s.jsx)(e.h2,{id:"using-skypilot-interactively",children:"Using SkyPilot interactively"}),"\n",(0,s.jsx)(e.h3,{id:"launch-an-interactive-development-environment",children:"Launch an Interactive Development Environment"}),"\n",(0,s.jsx)(e.p,{children:"You can also launch a SkyPilot cluster for interactive development:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"sky launch -c dev --gpus H100\n"})}),"\n",(0,s.jsx)(e.p,{children:'This command creates a development environment named "dev" with a single H100 GPU. SkyPilot handles the pod creation, resource allocation, and setup of the development environment.'}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.img,{alt:"Interactive Mode",src:t(483).A+"",width:"1547",height:"349"})}),"\n",(0,s.jsx)(e.p,{children:"Once launched, you can connect to your development environment:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"ssh dev\n"})}),"\n",(0,s.jsx)(e.p,{children:"This gives you an interactive shell in your development environment, where you can run your code, install packages, and perform ML experiments."}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.img,{alt:"SkyPilot interactive",src:t(7985).A+"",width:"1393",height:"354"})}),"\n",(0,s.jsx)(e.h2,{id:"launching-training-jobs-using-skypilot",children:"Launching training jobs using SkyPilot"}),"\n",(0,s.jsx)(e.h2,{id:"creating-and-submitting-your-training-job",children:"Creating and submitting your training job"}),"\n",(0,s.jsx)(e.p,{children:"SkyPilot makes it easy to run distributed training jobs on your SageMaker HyperPod cluster. Here's an example of launching a distributed training job on a single p5.48xlarge instance using a YAML configuration file:"}),"\n",(0,s.jsx)(e.p,{children:"First, create a file named train.yaml with your training job configuration:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-yaml",children:'resources:\n    accelerators: H100:8\n\nnum_nodes: 1\n\nsetup: |\n    git clone --depth 1 https://github.com/pytorch/examples || true\n    cd examples\n    git filter-branch --prune-empty --subdirectory-filter distributed/minGPT-ddp\n    uv venv --python 3.10\n    source .venv/bin/activate\n    uv pip install -r requirements.txt "numpy<2" "torch==1.12.1+cu113" --extra-index-url https://download.pytorch.org/whl/cu113\n\nrun: |\n    cd examples\n    source .venv/bin/activate\n    cd mingpt\n    export LOGLEVEL=INFO\n\n    MASTER_ADDR=$(echo "$SKYPILOT_NODE_IPS" | head -n1)\n    echo "Starting distributed training, head node: $MASTER_ADDR"\n\n    torchrun \\\n    --nnodes=$SKYPILOT_NUM_NODES \\\n    --nproc_per_node=$SKYPILOT_NUM_GPUS_PER_NODE \\\n    --master_addr=$MASTER_ADDR \\\n    --master_port=8008 \\\n    --node_rank=${SKYPILOT_NODE_RANK} \\\n    main.py\n'})}),"\n",(0,s.jsx)(e.p,{children:"Then launch your training job:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"sky launch -c train train.yaml\n"})}),"\n",(0,s.jsx)(e.p,{children:"This will create a distributed training job that using 1 p5.48xlarge nodes, each with 8 H100 NVIDIA GPUs. You can monitor the output with:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"sky logs train\n"})}),"\n",(0,s.jsx)(e.h3,{id:"running-multi-node-training-jobs-with-efa",children:"Running Multi-node Training Jobs with EFA"}),"\n",(0,s.jsx)(e.p,{children:"For multi-node training, we leverage the Elastic Fabric Adapter (EFA). Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables you to run applications requiring high levels of inter-node communications at scale on AWS. Its custom-built operating system bypass hardware interface enhances the performance of inter-instance communications, which is critical to scaling these applications. With EFA, High Performance Computing (HPC) applications using the Message Passing Interface (MPI) and Machine Learning (ML) applications using NVIDIA Collective Communications Library (NCCL) can scale to thousands of CPUs or GPUs. As a result, you get the application performance of on-premises HPC clusters with the on-demand elasticity and flexibility of the AWS cloud. Integrating EFA with applications running on Amazon EKS clusters can reduce the time to complete large scale distributed training workloads without having to add additional instances to your cluster."}),"\n",(0,s.jsx)(e.p,{children:"Below is a code snippet showcasing how to run the NCCL tests across 2 p5.48xlarge nodes with EFA configuration in SkyPilot job:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-yaml",children:'name: nccl-test-efa\n\nresources:\n  cloud: kubernetes\n  accelerators: H100:8\n  image_id: docker:public.ecr.aws/hpc-cloud/nccl-tests:latest\n\nnum_nodes: 2\n\nenvs:\n  USE_EFA: "true"\n\nrun: |\n  if [ "${SKYPILOT_NODE_RANK}" == "0" ]; then\n    echo "Head node"\n\n    # Total number of processes, NP should be the total number of GPUs in the cluster\n    NP=$(($SKYPILOT_NUM_GPUS_PER_NODE * $SKYPILOT_NUM_NODES))\n\n    # Append :${SKYPILOT_NUM_GPUS_PER_NODE} to each IP as slots\n    nodes=""\n    for ip in $SKYPILOT_NODE_IPS; do\n      nodes="${nodes}${ip}:${SKYPILOT_NUM_GPUS_PER_NODE},"\n    done\n    nodes=${nodes::-1}\n    echo "All nodes: ${nodes}"\n\n    # Set environment variables\n    export PATH=$PATH:/usr/local/cuda-12.2/bin:/opt/amazon/efa/bin:/usr/bin\n    export LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64:/opt/amazon/openmpi/lib:/opt/nccl/build/lib:/opt/amazon/efa/lib:/opt/aws-ofi-nccl/install/lib:/usr/local/nvidia/lib:$LD_LIBRARY_PATH\n    export NCCL_HOME=/opt/nccl\n    export CUDA_HOME=/usr/local/cuda-12.2\n    export NCCL_DEBUG=INFO\n    export NCCL_BUFFSIZE=8388608\n    export NCCL_P2P_NET_CHUNKSIZE=524288\n    export NCCL_TUNER_PLUGIN=/opt/aws-ofi-nccl/install/lib/libnccl-ofi-tuner.so\n\n    if [ "${USE_EFA}" == "true" ]; then\n      export FI_PROVIDER="efa"\n    else\n      export FI_PROVIDER=""\n    fi\n\n    /opt/amazon/openmpi/bin/mpirun \\\n      --allow-run-as-root \\\n      --tag-output \\\n      -H $nodes \\\n      -np $NP \\\n      -N $SKYPILOT_NUM_GPUS_PER_NODE \\\n      --bind-to none \\\n      -x FI_PROVIDER \\\n      -x PATH \\\n      -x LD_LIBRARY_PATH \\\n      -x NCCL_DEBUG=INFO \\\n      -x NCCL_BUFFSIZE \\\n      -x NCCL_P2P_NET_CHUNKSIZE \\\n      -x NCCL_TUNER_PLUGIN \\\n      --mca pml ^cm,ucx \\\n      --mca btl tcp,self \\\n      --mca btl_tcp_if_exclude lo,docker0,veth_def_agent \\\n      /opt/nccl-tests/build/all_reduce_perf \\\n      -b 8 \\\n      -e 2G \\\n      -f 2 \\\n      -g 1 \\\n      -c 5 \\\n      -w 5 \\\n      -n 100\n  else\n    echo "Worker nodes"\n  fi\n\nconfig:\n  kubernetes:\n    pod_config:\n      spec:\n        containers:\n        - resources:\n            limits:\n              nvidia.com/gpu: 8\n              vpc.amazonaws.com/efa: 32\n            requests:\n              nvidia.com/gpu: 8\n              vpc.amazonaws.com/efa: 32\n'})})]})}function u(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},7985:(n,e,t)=>{t.d(e,{A:()=>i});const i=t.p+"assets/images/sky_ssh-ca679874ec7a5e2978085b8356be8f81.png"},8453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>r});var i=t(6540);const s={},a=i.createContext(s);function o(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);