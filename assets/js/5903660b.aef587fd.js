"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5892],{2863:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"eks-blueprints/training/trainium/aws-trainium","title":"AWS Trainium","description":"In this section, we showcase how to pre-train Llama3.1-8B, Llama3 8B model using Trn1.32xlarge/Trn1n.32xlarge instances using the Neuron Distributed library. To train the LLama model in this example, we will apply the following optimizations using the Neuron Distributed library:","source":"@site/docs/01-eks-blueprints/training/trainium/aws-trainium.md","sourceDirName":"01-eks-blueprints/training/trainium","slug":"/eks-blueprints/training/trainium/aws-trainium","permalink":"/ai-on-sagemaker-hyperpod/docs/eks-blueprints/training/trainium/aws-trainium","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"AWS Trainium","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Ray Train","permalink":"/ai-on-sagemaker-hyperpod/docs/eks-blueprints/training/ray-train/ray-train-readme"},"next":{"title":"Fine Tuning","permalink":"/ai-on-sagemaker-hyperpod/docs/category/fine-tuning"}}');var r=i(4848),a=i(8453);const s={title:"AWS Trainium",sidebar_position:1},o="Train Llama 3.1 8B model using SageMaker HyperPod",l={},d=[{value:"Setup your environment",id:"setup-your-environment",level:2},{value:"Create your training job and start it",id:"create-your-training-job-and-start-it",level:2},{value:"Generate Job Spec Files for tokenization and training",id:"generate-job-spec-files-for-tokenization-and-training",level:3},{value:"Tokenize Data",id:"tokenize-data",level:3},{value:"Train Model",id:"train-model",level:3}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",...(0,a.R)(),...e.components},{Details:i}=n;return i||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"train-llama-31-8b-model-using-sagemaker-hyperpod",children:"Train Llama 3.1 8B model using SageMaker HyperPod"})}),"\n",(0,r.jsx)(n.p,{children:"In this section, we showcase how to pre-train Llama3.1-8B, Llama3 8B model using Trn1.32xlarge/Trn1n.32xlarge instances using the Neuron Distributed library. To train the LLama model in this example, we will apply the following optimizations using the Neuron Distributed library:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/tensor_parallelism_overview.html#tensor-parallelism-overview",children:"Tensor Parallelism"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/activation_memory_reduction.html#sequence-parallelism",children:"Sequence Parallel"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/activation_memory_reduction.html#activation-memory-reduction",children:"Selective checkpointing"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/tutorials/training/zero1_gpt2.html#zero1-gpt2-pretraining-tutorial",children:"ZeRO-1"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"setup-your-environment",children:"Setup your environment"}),"\n",(0,r.jsxs)(n.p,{children:["Login to ECR and pull the ",(0,r.jsx)(n.code,{children:"pytorch-training-neuronx"})," image"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:"region=us-east-2\ndlc_account_id=763104351884\naws ecr get-login-password --region $region | docker login --username AWS --password-stdin $dlc_account_id.dkr.ecr.$region.amazonaws.com\n\ndocker pull 763104351884.dkr.ecr.us-east-2.amazonaws.com/pytorch-training-neuronx:2.1.2-neuronx-py310-sdk2.19.1-ubuntu20.04\n"})}),"\n",(0,r.jsx)(n.p,{children:"On your x86-64 based development environment:"}),"\n",(0,r.jsx)(n.p,{children:"Navigate to your home directory or your preferred project directory, clone the repo."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ~\ngit clone https://github.com/aws-samples/awsome-distributed-training/\ncd awsome-distributed-training/3.test_cases/pytorch/neuronx-distributed/llama3/kubernetes\n"})}),"\n",(0,r.jsxs)(n.p,{children:["We will build docker image using the ",(0,r.jsx)(n.a,{href:"/docs/add-ons/integrations/skypilot",children:"Dockerfile"})," in this directory."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:"export AWS_REGION=$(aws ec2 describe-availability-zones --output text --query 'AvailabilityZones[0].[RegionName]')\nexport ACCOUNT=$(aws sts get-caller-identity --query Account --output text)\nexport REGISTRY=${ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com/\nexport IMAGE=llama3_trn\nexport TAG=:latest\ndocker build $DOCKER_NETWORK -t ${REGISTRY}${IMAGE}${TAG} .\n"})}),"\n",(0,r.jsxs)(i,{children:[(0,r.jsx)("summary",{children:"Why $DOCKER_NETWORK?"}),(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["The environment variable ",(0,r.jsx)("code",{children:"$DOCKER_NETWORK"})," is set to ",(0,r.jsx)("code",{children:"--network=sagemaker"})," only if you deployed the SageMaker Studio Code Editor CloudFormation stack in the ",(0,r.jsx)("a",{href:"/docs/category/getting-started",children:"Set Up Your Development Environment"})," section. This is necessary because SageMaker Studio uses a specific network configuration for its containers. Otherwise, it remains unset."]}),"\n"]})]}),"\n",(0,r.jsx)(n.p,{children:"Then push the image to your private registry"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:'# Create registry if needed\nexport REGISTRY_COUNT=$(aws ecr describe-repositories | grep \\"${IMAGE}\\" | wc -l)\nif [ "${REGISTRY_COUNT//[!0-9]/}" == "0" ]; then\n    echo "Creating repository ${REGISTRY}${IMAGE} ..."\n    aws ecr create-repository --repository-name ${IMAGE}\nelse\n    echo "Repository ${REGISTRY}${IMAGE} already exists"\nfi\n\n# Login to registry\necho "Logging in to $REGISTRY ..."\naws ecr get-login-password | docker login --username AWS --password-stdin $REGISTRY\n\n# Push image to registry\ndocker image push ${REGISTRY}${IMAGE}${TAG}\n'})}),"\n",(0,r.jsx)(n.h2,{id:"create-your-training-job-and-start-it",children:"Create your training job and start it"}),"\n",(0,r.jsx)(n.h3,{id:"generate-job-spec-files-for-tokenization-and-training",children:"Generate Job Spec Files for tokenization and training"}),"\n",(0,r.jsx)(n.p,{children:"The default config in the script launches a 8B Llama 3.1 model. When you run the generate-jobspec.sh script it creates 2 yaml files tokenize_data.yaml and llama3_train.yaml"}),"\n",(0,r.jsx)(n.p,{children:"You will have to update the HF_ACCESS_TOKEN in order for the tokenization to work."}),"\n",(0,r.jsxs)(n.p,{children:["Please edit the ",(0,r.jsx)(n.code,{children:"./generate-jobspec.sh"})," script with your desired environment settings."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"./generate-jobspec.sh\n"})}),"\n",(0,r.jsx)(n.h3,{id:"tokenize-data",children:"Tokenize Data"}),"\n",(0,r.jsxs)(n.p,{children:["The example uses ",(0,r.jsx)(n.a,{href:"https://huggingface.co/datasets/gboleda/wikicorpus",children:"wikicorpus"})," dataset from Hugginface Hub. The tokenize_data.yaml job downloads the dataset and tokenizes it. Finally store the dataset in Fsx Lustre which can be used for training the model."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f ./tokenize_data.yaml\n"})}),"\n",(0,r.jsx)(n.h3,{id:"train-model",children:"Train Model"}),"\n",(0,r.jsx)(n.p,{children:"The train_llama3.yaml job spec file trains llama 3.1 8B model with the tokenized data from previous step. By default the code uses 1 trn1.32xlarge but can be changed to any number of nodes."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f ./llama3_train.yaml\n"})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>o});var t=i(6540);const r={},a=t.createContext(r);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);