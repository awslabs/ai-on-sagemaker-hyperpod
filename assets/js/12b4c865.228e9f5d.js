"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[2198],{16:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/example-settings-505b4cb27a4d13dbc4e186d3592e5100.png"},3531:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/settings-6fb331e167b14f6b84bd8c6425ba538d.png"},5173:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"add-ons/Task Governance/Task Governance for Training","title":"Task Governance","description":"SageMaker HyperPod task governance is a management system designed to streamline resource allocation and ensure efficient utilization of compute resources across teams and projects for your Amazon EKS clusters. It provides administrators with the capability to set priority levels for various tasks, allocate compute resources for each team, determine how idle compute is borrowed and lent between teams, and configure whether a team can preempt its own tasks.","source":"@site/docs/04-add-ons/Task Governance/Task Governance for Training.md","sourceDirName":"04-add-ons/Task Governance","slug":"/add-ons/Task Governance/Task Governance for Training","permalink":"/docs/add-ons/Task Governance/Task Governance for Training","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Task Governance","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Task Governance","permalink":"/docs/category/task-governance"},"next":{"title":"HyperPod Training Operator Overview","permalink":"/docs/add-ons/hp-training-operator/overview"}}');var t=a(4848),r=a(8453);const o={title:"Task Governance",sidebar_position:1},i="Using Task Governance to optimize operation performance",c={},l=[{value:"Installing the add-on",id:"installing-the-add-on",level:2},{value:"Setup Task Governance EKS add-on",id:"setup-task-governance-eks-add-on",level:3},{value:"Task governance concepts",id:"task-governance-concepts",level:2},{value:"Cluster policy",id:"cluster-policy",level:3},{value:"Idle compute allocation",id:"idle-compute-allocation",level:4},{value:"Task prioritization",id:"task-prioritization",level:4},{value:"Compute allocations",id:"compute-allocations",level:3},{value:"Examples",id:"examples",level:2},{value:"Setup cluster policy",id:"setup-cluster-policy",level:3},{value:"Setup compute allocations",id:"setup-compute-allocations",level:3},{value:"Clone the examples repository",id:"clone-the-examples-repository",level:3},{value:"Borrow compute from another team",id:"borrow-compute-from-another-team",level:2},{value:"Reclaim guaranteed compute",id:"reclaim-guaranteed-compute",level:2},{value:"Preempt low priority tasks",id:"preempt-low-priority-tasks",level:2},{value:"Inspecting workloads",id:"inspecting-workloads",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",div:"div",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"using-task-governance-to-optimize-operation-performance",children:"Using Task Governance to optimize operation performance"})}),"\n",(0,t.jsx)(n.p,{children:"SageMaker HyperPod task governance is a management system designed to streamline resource allocation and ensure efficient utilization of compute resources across teams and projects for your Amazon EKS clusters. It provides administrators with the capability to set priority levels for various tasks, allocate compute resources for each team, determine how idle compute is borrowed and lent between teams, and configure whether a team can preempt its own tasks."}),"\n",(0,t.jsx)(n.p,{children:"HyperPod task governance leverages Kueue for Kubernetes-native job queueing, scheduling, and quota management and is installed using the HyperPod task governance EKS add-on."}),"\n",(0,t.jsx)(n.h2,{id:"installing-the-add-on",children:"Installing the add-on"}),"\n",(0,t.jsx)(n.h3,{id:"setup-task-governance-eks-add-on",children:"Setup Task Governance EKS add-on"}),"\n",(0,t.jsx)(n.p,{children:"To install SageMaker HyperPod task governance, you will need Kubernetes version 1.30 or greater and you will need to remove any existing installations of Kueue."}),"\n",(0,t.jsxs)(n.div,{children:[(0,t.jsxs)(n.div,{children:[(0,t.jsxs)(n.p,{children:["Navigate to your HyperPod Cluster in the SageMaker AI console. In the ",(0,t.jsx)(n.strong,{children:"Dashboard"})," tab, click ",(0,t.jsx)(n.code,{children:"Install"})," under the Amazon SageMaker HyperPod task governance add-on."]}),(0,t.jsx)("div",{className:"text--center",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"AddOn",src:a(7516).A+"",width:"560",height:"151"})})})]}),(0,t.jsxs)(n.div,{children:[(0,t.jsxs)(n.p,{children:["To install the ",(0,t.jsx)(n.strong,{children:"SageMaker HyperPod task governance EKS add-on"}),", run the following command:"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"aws eks create-addon --region $REGION --cluster-name $EKS_CLUSTER_NAME --addon-name amazon-sagemaker-hyperpod-taskgovernance\n"})}),(0,t.jsx)(n.p,{children:"Verify successful installation with:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"aws eks describe-addon --region $REGION --cluster-name $EKS_CLUSTER_NAME --addon-name amazon-sagemaker-hyperpod-taskgovernance\n"})}),(0,t.jsx)(n.p,{children:"If the installation was successful, you should see details about the installed add-on in the output."})]})]}),"\n",(0,t.jsx)(n.h2,{id:"task-governance-concepts",children:"Task governance concepts"}),"\n",(0,t.jsxs)(n.p,{children:["Amazon SageMaker HyperPod task governance uses policies to define resource allocation and task prioritization. These policies are categorized into ",(0,t.jsx)(n.strong,{children:"compute prioritization"})," and ",(0,t.jsx)(n.strong,{children:"compute allocation"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"cluster-policy",children:"Cluster policy"}),"\n",(0,t.jsxs)(n.p,{children:["Compute prioritization, also known as ",(0,t.jsx)(n.strong,{children:"cluster policy"}),", determines how idle compute is borrowed and how tasks are prioritized across teams. A cluster policy consists of two key components:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Task Prioritization"}),"\n",(0,t.jsx)(n.li,{children:"Idle Compute Allocation"}),"\n"]}),"\n",(0,t.jsx)("div",{className:"text--center",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Cluster-Policy",src:a(9109).A+"",width:"1105",height:"208"})})}),"\n",(0,t.jsx)(n.p,{children:"As an administrator, you should define your organization's priorities and configure the cluster policy accordingly."}),"\n",(0,t.jsx)(n.h4,{id:"idle-compute-allocation",children:"Idle compute allocation"}),"\n",(0,t.jsx)(n.p,{children:"Idle compute allocation defines how idle compute are distributed among teams. This determines whether and how teams can borrow idle compute. You can choose between the following allocation strategies:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"First-come first-serve"}),": Teams are not prioritized over one another. Each incoming task has an equal chance of obtaining over-quota resources. Compute is allocated based on the order of task submission, meaning a single user could use 100% of the idle compute if they request it first."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fair-share"}),": Teams borrow idle compute based on their assigned ",(0,t.jsx)(n.strong,{children:"Fair-share weight"}),". These weights are defined in ",(0,t.jsx)(n.strong,{children:"Compute Allocation"})," and determine how compute is distributed among teams when idle resources are available."]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"task-prioritization",children:"Task prioritization"}),"\n",(0,t.jsx)(n.p,{children:"Task prioritization determines how tasks are queued as compute becomes available. You can choose between the following methods:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"First-come first-serve"}),": Tasks are queued in the order they are submitted."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task ranking"}),": Tasks are queued based on their assigned priority. Tasks within the same priority class are processed on a first-come, first-serve basis. If ",(0,t.jsx)(n.strong,{children:"Task Preemption"})," is enabled in ",(0,t.jsx)(n.strong,{children:"Compute Allocation"}),", higher-priority tasks can preempt lower-priority tasks within the same team"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Here's an example configuration for a cluster policy. In this example, we have ",(0,t.jsx)(n.code,{children:"inference"})," tasks as top priority, and have enabled the idle compute allocation to the fair-share strategy (based on team weights)."]}),"\n",(0,t.jsx)("div",{className:"text--center",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Cluster-Policy-Priorities",src:a(3531).A+"",width:"656",height:"730"})})}),"\n",(0,t.jsx)(n.h3,{id:"compute-allocations",children:"Compute allocations"}),"\n",(0,t.jsx)(n.p,{children:"Compute allocation, or compute quota, defines a team\u2019s compute allocation and what weight (or priority level) a team is given for fair-share idle compute allocation."}),"\n",(0,t.jsxs)(n.p,{children:["You will need at minimum two compute allocations created in order to borrow capacity and preempt tasks across teams. The total reserved quota should not surpass the cluster's available capacity for that resource, to ensure proper quota management. For example, if a cluster comprises 20 ml.c5.2xlarge instances, the cumulative quota assigned to teams should remain under 20. For more details on how compute is allocated in task governance, please follow the ",(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-operate-console-ui-governance.html",children:"documentation for task governance"}),"."]}),"\n",(0,t.jsx)("div",{className:"text--center",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"CQ1",src:a(5214).A+"",width:"650",height:"621"})})}),"\n",(0,t.jsx)(n.p,{children:"In the next section, we will walk through a detailed example with step-by-step instructions on configuring these settings."}),"\n",(0,t.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,t.jsx)(n.admonition,{title:"Note:",type:"note",children:(0,t.jsxs)(n.p,{children:["This section demonstrates how to configure task governance using an example cluster with 4 ",(0,t.jsx)(n.strong,{children:"g5.8xlarge"})," instances. If you haven't set up the HyperPod task governance add-on, refer to the ",(0,t.jsx)(n.a,{href:"/docs/add-ons/Task%20Governance/Task%20Governance%20for%20Training#installing-the-add-on",children:"Task Governance Setup"})," page."]})}),"\n",(0,t.jsx)(n.p,{children:"This section will guide you on an end to end example of using task governance. We use a cluster of 4 g5.8xlarge instances split between two teams (Team A and Team B)."}),"\n",(0,t.jsx)(n.p,{children:"Before running the examples, we will set up a Cluster Policy and define Compute Allocations for the teams. The policy will use task ranking instead of the default FIFO (First-In-First-Out) behavior, allowing higher-priority tasks to preempt lower-priority ones."}),"\n",(0,t.jsx)(n.h3,{id:"setup-cluster-policy",children:"Setup cluster policy"}),"\n",(0,t.jsx)(n.p,{children:"Setup env variables:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"export HYPERPOD_CLUSTER_ARN={your-hyperpod-cluster-arn}\n"})}),"\n",(0,t.jsx)(n.p,{children:"To update how tasks are prioritized and how idle compute is allocated, apply a Cluster Policy using the following configuration:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'aws sagemaker \\\n    create-cluster-scheduler-config \\\n    --name "example-cluster-scheduler-config" \\\n    --cluster-arn $HYPERPOD_CLUSTER_ARN \\\n    --scheduler-config "PriorityClasses=[{Name=inference,Weight=90},{Name=experimentation,Weight=80},{Name=fine-tuning,Weight=50},{Name=training,Weight=70}],FairShare=Enabled"\n'})}),"\n",(0,t.jsx)(n.p,{children:"This CLI command will output two values: CreateSchedulerConfigArn and ClusterSchedulerConfigId. This will generate a cluster policy with fair sharing enabled and the following priority classes:"}),"\n",(0,t.jsx)("div",{className:"text--center",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Cluster_policy",src:a(6344).A+"",width:"891",height:"242"})})}),"\n",(0,t.jsxs)(n.p,{children:["A higher weight indicates a higher priority. In this example, the ",(0,t.jsx)(n.code,{children:"inference"})," priority class has the highest priority."]}),"\n",(0,t.jsx)(n.h3,{id:"setup-compute-allocations",children:"Setup compute allocations"}),"\n",(0,t.jsx)(n.p,{children:"Each team requires a Compute Allocation to manage their compute capacity. Both teams will have 2 instances allocated, 0 fair-share weight, and 50% borrowing capability."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'aws sagemaker \\\n    create-compute-quota \\\n    --name "Team-A-Quota-Allocation" \\\n    --cluster-arn $HYPERPOD_CLUSTER_ARN \\\n    --compute-quota-config "ComputeQuotaResources=[{InstanceType=ml.g5.8xlarge,Count=2}],ResourceSharingConfig={Strategy=LendAndBorrow,BorrowLimit=50},PreemptTeamTasks=LowerPriority" \\\n    --activation-state "Enabled" \\\n    --compute-quota-target "TeamName=team-a,FairShareWeight=0"\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'aws sagemaker \\\n    create-compute-quota \\\n    --name "Team-B-Quota-Allocation" \\\n    --cluster-arn $HYPERPOD_CLUSTER_ARN \\\n    --compute-quota-config "ComputeQuotaResources=[{InstanceType=ml.g5.8xlarge,Count=2}],ResourceSharingConfig={Strategy=LendAndBorrow,BorrowLimit=50},PreemptTeamTasks=LowerPriority" \\\n    --activation-state "Enabled" \\\n    --compute-quota-target "TeamName=team-b,FairShareWeight=0"\n'})}),"\n",(0,t.jsx)("div",{className:"text--center",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Example",src:a(16).A+"",width:"1008",height:"701"})})}),"\n",(0,t.jsx)(n.h3,{id:"clone-the-examples-repository",children:"Clone the examples repository"}),"\n",(0,t.jsx)(n.p,{children:"Navigate to your home directory or your preferred project directory, clone the repo."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ~\ngit clone https://github.com/aws-samples/awsome-distributed-training/\ncd awsome-distributed-training/1.architectures/7.sagemaker-hyperpod-eks/task-governance\n"})}),"\n",(0,t.jsx)(n.h2,{id:"borrow-compute-from-another-team",children:"Borrow compute from another team"}),"\n",(0,t.jsx)(n.admonition,{title:"Note:",type:"note",children:(0,t.jsxs)(n.p,{children:["This section provides a walkthrough of a job submission using task governance, based on the setup created in the ",(0,t.jsx)(n.a,{href:"/docs/add-ons/Task%20Governance/Task%20Governance%20for%20Training#examples",children:"Setup for running the examples"})," page."]})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Scenario:"})," Team A submits a PyTorch job that requires ",(0,t.jsx)(n.strong,{children:"3 instances"})," but only has ",(0,t.jsx)(n.strong,{children:"2 allocated"}),". The system allows Team A to ",(0,t.jsx)(n.strong,{children:"borrow"})," 1 instance from Team B's idle capacity."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f 1-imagenet-gpu-team-a.yaml --namespace hyperpod-ns-team-a\n"})}),"\n",(0,t.jsx)(n.p,{children:"Verify the job is running (pulling the container image might take a moment):"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n hyperpod-ns-team-a\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"NAME                             READY   STATUS    RESTARTS   AGE\netcd-gpu-679b676b55-5xj5x        1/1     Running   0          5m\nimagenet-gpu-team-a-1-worker-0   1/1     Running   0          5m\nimagenet-gpu-team-a-1-worker-1   1/1     Running   0          5m\nimagenet-gpu-team-a-1-worker-2   1/1     Running   0          5m\n"})}),"\n",(0,t.jsx)(n.p,{children:"In the task tab of the console, you should see the job running:"}),"\n",(0,t.jsx)("div",{className:"text--center",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Example",src:a(8065).A+"",width:"1264",height:"169"})})}),"\n",(0,t.jsx)(n.p,{children:"Once the pods are running, you can check the output of logs to identify the elected master:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl logs imagenet-gpu-team-a-1-worker-0 --namespace hyperpod-ns-team-a | grep master_addr=\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"[2025-02-04 18:58:08,460] torch.distributed.elastic.agent.server.api: [INFO]   master_addr=imagenet-gpu-team-a-1-worker-2\n"})}),"\n",(0,t.jsxs)(n.p,{children:["You can then use the pod referenced in the ",(0,t.jsx)(n.code,{children:"master_addr"})," to look at the current training progress:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl logs imagenet-gpu-team-a-1-worker-2 --namespace hyperpod-ns-team-a\n"})}),"\n",(0,t.jsx)(n.h2,{id:"reclaim-guaranteed-compute",children:"Reclaim guaranteed compute"}),"\n",(0,t.jsx)(n.admonition,{title:"Note:",type:"note",children:(0,t.jsxs)(n.p,{children:["This section provides a walkthrough of a job submission using task governance, based on the setup created in the ",(0,t.jsx)(n.a,{href:"/docs/add-ons/Task%20Governance/Task%20Governance%20for%20Training#examples",children:"Setup for running the examples"}),". It also demonstrates how to run the sample application using the HyperPod CLI instead of kubectl. If you haven't installed the HyperPod CLI, refer to the ",(0,t.jsx)(n.a,{href:"/docs/add-ons/installing-the-hyperpod-cli",children:"Install HyperPod CLI"})," page."]})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Scenario:"})," Team B needs to reclaim its compute resources. By submitting a job requiring ",(0,t.jsx)(n.strong,{children:"2 instances"}),", Team B's job is ",(0,t.jsx)(n.strong,{children:"prioritized"}),", and Job 1 is ",(0,t.jsx)(n.strong,{children:"suspended"})," due to resource unavailability."]}),"\n",(0,t.jsx)(n.p,{children:"In this example, we'll be using the hyperpod CLI, but we could also use kubectl and have identical behavior."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"hyperpod start-job --config-file 2-hyperpod-cli-example-team-b.yaml\n"})}),"\n",(0,t.jsx)(n.p,{children:"This command will give you a similar output:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'{\n "Console URL": "https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/cluster-management/ml-cluster"\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:"After the job has been submitted, you can see that the workers from Job 1 have been preempted, and only the workers in Team B's namespace are running."}),"\n",(0,t.jsx)("div",{className:"text--center",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Example",src:a(7898).A+"",width:"1159",height:"374"})})}),"\n",(0,t.jsx)(n.p,{children:"Check running pods for Team B:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n hyperpod-ns-team-b\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"NAME                                 READY   STATUS    RESTARTS   AGE\nhyperpod-cli-mnist-team-b-worker-0   1/1     Running   0          3m47s\nhyperpod-cli-mnist-team-b-worker-1   1/1     Running   0          3m47s\n"})}),"\n",(0,t.jsx)(n.h2,{id:"preempt-low-priority-tasks",children:"Preempt low priority tasks"}),"\n",(0,t.jsx)(n.admonition,{title:"Note:",type:"note",children:(0,t.jsxs)(n.p,{children:["This section provides a walkthrough of a job submission using task governance, based on the setup created in the ",(0,t.jsx)(n.a,{href:"/docs/add-ons/Task%20Governance/Task%20Governance%20for%20Training#examples",children:"Setup for running the examples"})," page."]})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Scenario:"})," Team B submits a ",(0,t.jsx)(n.strong,{children:"high-priority job"})," requiring ",(0,t.jsx)(n.strong,{children:"2 instances"}),". Since high-priority jobs take precedence, ",(0,t.jsx)(n.strong,{children:"Job 2 is suspended"}),", ensuring Team B\u2019s critical workload runs first."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f 3-imagenet-gpu-team-b-higher-prio.yaml --namespace hyperpod-ns-team-b\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Since this job uses a ",(0,t.jsx)(n.strong,{children:"priority-class"})," with a higher weight than the other jobs, the lower-priority Job 2 is preempted:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n hyperpod-ns-team-b\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"NAME                                 READY   STATUS        RESTARTS   AGE\netcd-gpu-6584d647d4-5z564            1/1     Running       0          11s\nhyperpod-cli-mnist-team-b-worker-0   1/1     Terminating   0          28s\nhyperpod-cli-mnist-team-b-worker-1   1/1     Terminating   0          28s\nimagenet-gpu-team-b-2-worker-0       1/1     Running       0          10s\nimagenet-gpu-team-b-2-worker-1       1/1     Running       0          10s\n\n"})}),"\n",(0,t.jsx)("div",{className:"text--center",children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Example",src:a(5475).A+"",width:"1358",height:"346"})})}),"\n",(0,t.jsx)(n.h3,{id:"inspecting-workloads",children:"Inspecting workloads"}),"\n",(0,t.jsx)(n.p,{children:"We can also inspect the workloads on a particular namespace:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get workloads -n hyperpod-ns-team-b\n"})}),"\n",(0,t.jsx)(n.p,{children:"This is an example output of the command after running all 3 scenarios:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"NAME                                         QUEUE                           RESERVED IN                       ADMITTED   FINISHED   AGE\npod-etcd-gpu-6584d647d4-sp6xx-bb3f9          hyperpod-ns-team-b-localqueue   hyperpod-ns-team-b-clusterqueue   True                  11s\npytorchjob-hyperpod-cli-mnist-team-b-2c720   hyperpod-ns-team-b-localqueue                                     False                 45s\npytorchjob-imagenet-gpu-team-b-2-ef5c0       hyperpod-ns-team-b-localqueue   hyperpod-ns-team-b-clusterqueue   True                  11s\n"})}),"\n",(0,t.jsxs)(n.p,{children:["We can see that the workload for Job 2 has been set to ",(0,t.jsx)(n.code,{children:"ADMITTED: False"})," because the newly submitted workload took precedence."]}),"\n",(0,t.jsx)(n.p,{children:"When we describe the suspended workload, we can see the reason it was preempted."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl describe workload pytorchjob-hyperpod-cli-mnist-team-b-2c720 -n hyperpod-ns-team-b\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Status:\n  Conditions:\n    Last Transition Time:  2025-02-04T19:06:25Z\n    Message:               couldn't assign flavors to pod set worker: borrowing limit for nvidia.com/gpu in flavor ml.g5.8xlarge exceeded\n    Observed Generation:   1\n    Reason:                Pending\n    Status:                False\n    Type:                  QuotaReserved\n    Last Transition Time:  2025-02-04T19:06:25Z\n    Message:               Preempted to accommodate a workload (UID: d34468c2-1ce5-47cd-a61d-689be78b6121) due to prioritization in the ClusterQueue\n    Observed Generation:   1\n    Reason:                Preempted\n    Status:                True\n    Type:                  Evicted\n    Last Transition Time:  2025-02-04T19:06:25Z\n    Message:               The workload has no reservation\n    Observed Generation:   1\n    Reason:                NoReservation\n    Status:                False\n    Type:                  Admitted\n    Last Transition Time:  2025-02-04T19:06:25Z\n    Message:               Preempted to accommodate a workload (UID: d34468c2-1ce5-47cd-a61d-689be78b6121) due to prioritization in the ClusterQueue\n    Reason:                InClusterQueue\n    Status:                True\n    Type:                  Preempted\n    Last Transition Time:  2025-02-04T19:06:25Z\n    Message:               Preempted to accommodate a workload (UID: d34468c2-1ce5-47cd-a61d-689be78b6121) due to prioritization in the ClusterQueue\n    Observed Generation:   1\n    Reason:                Preempted\n    Status:                True\n    Type:                  Requeued\n"})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},5214:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/cq-1-b07b64b4aa5b8eaf82188cb09d56ae37.png"},5475:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/job-3-440069e1415a06a818667353621cbbc4.png"},6344:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/cluster-policy-example-28fa64954e381938774d073d12efe5ac.png"},7516:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/addon-85bd9877e25f4bdef5ff922067f9420a.png"},7898:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/job-2-62c167490e6cdf8d2e725464ae98e943.png"},8065:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/job-1-49dbb7c9122f7498630f954dd2a1e6aa.png"},8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>i});var s=a(6540);const t={},r=s.createContext(t);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(r.Provider,{value:n},e.children)}},9109:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/cluster-policy-dc6f47628edf5c177e2460fee8c02e97.png"}}]);