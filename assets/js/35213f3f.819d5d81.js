"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[3904],{1470:(e,n,r)=>{r.d(n,{A:()=>b});var o=r(6540),t=r(4164),s=r(3104),a=r(6347),i=r(205),l=r(7485),c=r(1682),u=r(679);function d(e){return o.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function p(e){const{values:n,children:r}=e;return(0,o.useMemo)(()=>{const e=n??function(e){return d(e).map(({props:{value:e,label:n,attributes:r,default:o}})=>({value:e,label:n,attributes:r,default:o}))}(r);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,r])}function h({value:e,tabValues:n}){return n.some(n=>n.value===e)}function g({queryString:e=!1,groupId:n}){const r=(0,a.W6)(),t=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,l.aZ)(t),(0,o.useCallback)(e=>{if(!t)return;const n=new URLSearchParams(r.location.search);n.set(t,e),r.replace({...r.location,search:n.toString()})},[t,r])]}function m(e){const{defaultValue:n,queryString:r=!1,groupId:t}=e,s=p(e),[a,l]=(0,o.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!h({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const r=n.find(e=>e.default)??n[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:n,tabValues:s})),[c,d]=g({queryString:r,groupId:t}),[m,f]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[r,t]=(0,u.Dv)(n);return[r,(0,o.useCallback)(e=>{n&&t.set(e)},[n,t])]}({groupId:t}),j=(()=>{const e=c??m;return h({value:e,tabValues:s})?e:null})();(0,i.A)(()=>{j&&l(j)},[j]);return{selectedValue:a,selectValue:(0,o.useCallback)(e=>{if(!h({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);l(e),d(e),f(e)},[d,f,s]),tabValues:s}}var f=r(2303);const j={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=r(4848);function w({className:e,block:n,selectedValue:r,selectValue:o,tabValues:a}){const i=[],{blockElementScrollPositionUntilNextRender:l}=(0,s.a_)(),c=e=>{const n=e.currentTarget,t=i.indexOf(n),s=a[t].value;s!==r&&(l(n),o(s))},u=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const r=i.indexOf(e.currentTarget)+1;n=i[r]??i[0];break}case"ArrowLeft":{const r=i.indexOf(e.currentTarget)-1;n=i[r]??i[i.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.A)("tabs",{"tabs--block":n},e),children:a.map(({value:e,label:n,attributes:o})=>(0,x.jsx)("li",{role:"tab",tabIndex:r===e?0:-1,"aria-selected":r===e,ref:e=>{i.push(e)},onKeyDown:u,onClick:c,...o,className:(0,t.A)("tabs__item",j.tabItem,o?.className,{"tabs__item--active":r===e}),children:n??e},e))})}function y({lazy:e,children:n,selectedValue:r}){const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=s.find(e=>e.props.value===r);return e?(0,o.cloneElement)(e,{className:(0,t.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:s.map((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==r}))})}function v(e){const n=m(e);return(0,x.jsxs)("div",{className:(0,t.A)("tabs-container",j.tabList),children:[(0,x.jsx)(w,{...n,...e}),(0,x.jsx)(y,{...n,...e})]})}function b(e){const n=(0,f.A)();return(0,x.jsx)(v,{...e,children:d(e.children)},String(n))}},2844:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>u,contentTitle:()=>c,default:()=>h,frontMatter:()=>l,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"Tips/Slurm/heterogenous-cluster","title":"Heterogenous Cluster","description":"Adding Worker Groups to an existing cluster","source":"@site/docs/08-Tips/Slurm/11-heterogenous-cluster.md","sourceDirName":"08-Tips/Slurm","slug":"/Tips/Slurm/heterogenous-cluster","permalink":"/ai-on-sagemaker-hyperpod/docs/Tips/Slurm/heterogenous-cluster","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"title":"Heterogenous Cluster","weight":51},"sidebar":"tutorialSidebar","previous":{"title":"Diagnose GPU Failures","permalink":"/ai-on-sagemaker-hyperpod/docs/Tips/Slurm/diagnose-gpus"},"next":{"title":"Configure Cgroups for Slurm","permalink":"/ai-on-sagemaker-hyperpod/docs/Tips/Slurm/enable-cgroups"}}');var t=r(4848),s=r(8453),a=r(1470),i=r(9365);const l={title:"Heterogenous Cluster",weight:51},c=void 0,u={},d=[{value:"Adding Worker Groups to an existing cluster",id:"adding-worker-groups-to-an-existing-cluster",level:3},{value:"Update Cluster Configuration",id:"update-cluster-configuration",level:3},{value:"Update Provisioning Parameters",id:"update-provisioning-parameters",level:3},{value:"Update-cluster",id:"update-cluster",level:3},{value:"Update Slurm Configuration on updated cluster (controller node)",id:"update-slurm-configuration-on-updated-cluster-controller-node",level:3}];function p(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h3,{id:"adding-worker-groups-to-an-existing-cluster",children:"Adding Worker Groups to an existing cluster"}),"\n",(0,t.jsx)(n.p,{children:"SageMaker HyperPod gives you the ability to update your clusters to increase the size of an existing worker group, or create a new worker group to add additional instance-types to your cluster."}),"\n",(0,t.jsxs)(n.p,{children:["To increase the size of an existing worker-group, you can simply update the InstanceCount using the ",(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-operate-console-ui.html#sagemaker-hyperpod-operate-console-ui-edit-clusters",children:"SageMaker HyperPod console"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["If you wish to add a new worker-group with a new instance type to your cluster, we recommend you follow the steps provided here which use the ",(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/reference/sagemaker/update-cluster.html",children:(0,t.jsx)(n.code,{children:"update-cluster"})})," CLI command. When you update a cluster to create separate worker-groups for different instance types, this is called a ",(0,t.jsx)(n.em,{children:"heterogenous cluster"}),"."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Heterogenous Cluster Example",src:r(9339).A+"",width:"1436",height:"807"})}),"\n",(0,t.jsx)(n.p,{children:"For heterogenous clusters, it is a recommended best practice to create a new partition for each worker-group / instance type, so that your ml-researchers can target partitions with the desired instance-types when they launch their jobs."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"$ sinfo\nPARTITION      AVAIL  TIMELIMIT  NODES  STATE NODELIST\ndev*              up   infinite      2   idle ...\nml.p5.48xlarge    up   infinite      2   idle ...\nml.g5.48xlarge    up   infinite      2   idle ...\n"})}),"\n",(0,t.jsx)(n.p,{children:"Having seperate partitions for each instance-type allows you to submit jobs targeting specific instances by specifying a partition as part of your sbatch job:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sbatch -p ml.p5.48xlarge submit.sh\n"})}),"\n",(0,t.jsx)(n.h3,{id:"update-cluster-configuration",children:"Update Cluster Configuration"}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["The following steps assume you are using the same deployment environment used to ",(0,t.jsx)(n.a,{href:"/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup",children:"create your cluster"}),". This could be a SageMaker Studio editor instance or a local development machine, depending on how you deployed your cluster originally. The below scripts assume you have an ",(0,t.jsx)(n.code,{children:"env_vars"})," file defined, and access to your s3 lifecycle script bucket."]})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Set enviornment variables in your local directory"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Set environment variables from cluster setup \nsource env_vars\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:["Our HyperPod clusters come with a ",(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-operate-cli-command-create-cluster.html",children:(0,t.jsx)(n.code,{children:"cluster-config.json"})}),". We will use that configuration file to write a new file called ",(0,t.jsx)(n.code,{children:"update-cluster-config.json"}),". The new file will include additional worker-groups which we will reference to update the cluster. Select the instance type you would like to use for your new worker-group from the below options and follow the scripts to create a new file called ",(0,t.jsx)(n.code,{children:"update-cluster-config.json"}),"."]}),"\n"]}),"\n","\n",(0,t.jsxs)(a.A,{children:[(0,t.jsx)(i.A,{value:"trn1.32xlarge",label:"trn1.32xlarge",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Source environment variables\nsource env_vars\n\n# Create additional worker group configuration\nadditional_worker_group=$(cat <<EOF\n{\n  "InstanceGroupName": "worker-group-2",\n  "InstanceType": "ml.trn1.32xlarge",\n  "InstanceCount": 1,\n  "LifeCycleConfig": {\n    "SourceS3Uri": "s3://${BUCKET}/src",\n    "OnCreate": "on_create.sh"\n  },\n  "ExecutionRole": "${ROLE}",\n  "ThreadsPerCore": 1\n}\nEOF\n)\n\n# Copy cluster-config.json to a temporary file\ncp cluster-config.json temp-cluster-config.json\n\n# Add additional worker group and remove VpcConfig section\njq --argjson additional_worker_group "$additional_worker_group" \'.InstanceGroups += [$additional_worker_group] | del(.VpcConfig)\' temp-cluster-config.json > update-cluster-config.json\n\n# Remove the temporary file\nrm temp-cluster-config.json\n'})})}),(0,t.jsx)(i.A,{value:"p4d.24xlarge",label:"p4d.24xlarge",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Source environment variables\nsource env_vars\n\n# Create additional worker group configuration\nadditional_worker_group=$(cat <<EOF\n{\n  "InstanceGroupName": "worker-group-2",\n  "InstanceType": "ml.p4d.24xlarge",\n  "InstanceCount": 1,\n  "LifeCycleConfig": {\n    "SourceS3Uri": "s3://${BUCKET}/src",\n    "OnCreate": "on_create.sh"\n  },\n  "ExecutionRole": "${ROLE}",\n  "ThreadsPerCore": 1\n}\nEOF\n)\n\n# Copy cluster-config.json to a temporary file\ncp cluster-config.json temp-cluster-config.json\n\n# Add additional worker group and remove VpcConfig section\njq --argjson additional_worker_group "$additional_worker_group" \'.InstanceGroups += [$additional_worker_group] | del(.VpcConfig)\' temp-cluster-config.json > update-cluster-config.json\n\n# Remove the temporary file\nrm temp-cluster-config.json\n'})})}),(0,t.jsx)(i.A,{value:"p5.48xlarge",label:"p5.48xlarge",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Source environment variables\nsource env_vars\n\n# Create additional worker group configuration\nadditional_worker_group=$(cat <<EOF\n{\n  "InstanceGroupName": "worker-group-2",\n  "InstanceType": "ml.p5.48xlarge",\n  "InstanceCount": 1,\n  "LifeCycleConfig": {\n    "SourceS3Uri": "s3://${BUCKET}/src",\n    "OnCreate": "on_create.sh"\n  },\n  "ExecutionRole": "${ROLE}",\n  "ThreadsPerCore": 1\n}\nEOF\n)\n\n# Copy cluster-config.json to a temporary file\ncp cluster-config.json temp-cluster-config.json\n\n# Add additional worker group and remove VpcConfig section\njq --argjson additional_worker_group "$additional_worker_group" \'.InstanceGroups += [$additional_worker_group] | del(.VpcConfig)\' temp-cluster-config.json > update-cluster-config.json\n\n# Remove the temporary file\nrm temp-cluster-config.json\n'})})}),(0,t.jsx)(i.A,{value:"g5.48xlarge",label:"g5.48xlarge",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Source environment variables\nsource env_vars\n\n# Create additional worker group configuration\nadditional_worker_group=$(cat <<EOF\n{\n  "InstanceGroupName": "worker-group-2",\n  "InstanceType": "ml.g5.48xlarge",\n  "InstanceCount": 1,\n  "LifeCycleConfig": {\n    "SourceS3Uri": "s3://${BUCKET}/src",\n    "OnCreate": "on_create.sh"\n  },\n  "ExecutionRole": "${ROLE}",\n  "ThreadsPerCore": 1\n}\nEOF\n)\n\n# Copy cluster-config.json to a temporary file\ncp cluster-config.json temp-cluster-config.json\n\n# Add additional worker group and remove VpcConfig section\njq --argjson additional_worker_group "$additional_worker_group" \'.InstanceGroups += [$additional_worker_group] | del(.VpcConfig)\' temp-cluster-config.json > update-cluster-config.json\n\n# Remove the temporary file\nrm temp-cluster-config.json\n'})})})]}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Verify your updated configuration file."}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cat update-cluster-config.json\n"})}),"\n",(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsx)(n.p,{children:"Note it should contain the same name of your existing cluster and worker groups, as well as the new worker group created in step 2. We also removed the VpcConfig (not required for the update-cluster API we will call in the next step). Example:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n    "ClusterName": "ml-cluster",\n    "InstanceGroups": [\n      {\n        "InstanceGroupName": "controller-machine",\n        "InstanceType": "ml.m5.12xlarge",\n        "InstanceCount": 1,\n        "LifeCycleConfig": {\n          "SourceS3Uri": "s3://${BUCKET}/src",\n          "OnCreate": "on_create.sh"\n        },\n        "ExecutionRole": "${ROLE}",\n        "ThreadsPerCore": 2\n      },\n      {\n        "InstanceGroupName": "worker-group-1",\n        "InstanceType": "ml.p5.48xlarge",\n        "InstanceCount": 2,\n        "LifeCycleConfig": {\n          "SourceS3Uri": "s3://${BUCKET}/src",\n          "OnCreate": "on_create.sh"\n        },\n        "ExecutionRole": "${ROLE}",\n        "ThreadsPerCore": 1\n      },\n      {\n        "InstanceGroupName": "worker-group-2",\n        "InstanceType": "ml.g5.48xlarge",\n        "InstanceCount": 2,\n        "LifeCycleConfig": {\n          "SourceS3Uri": "s3://${BUCKET}/src",\n          "OnCreate": "on_create.sh"\n        },\n        "ExecutionRole": "${ROLE}",\n        "ThreadsPerCore": 1\n      },\n    ]\n}\n'})})]}),"\n",(0,t.jsx)(n.h3,{id:"update-provisioning-parameters",children:"Update Provisioning Parameters"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-ref.html#sagemaker-hyperpod-ref-provisioning-forms-slurm",children:(0,t.jsx)(n.code,{children:"provisioning_parameters.json"})})," is a configuration for provisioning Slurm nodes on HyperPod. To register the new worker group and partition with the Slurm Controller you need to update the ",(0,t.jsx)(n.code,{children:"provisioning_parameters.json"})," before you call the update-cluster API."]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Start by copying the current ",(0,t.jsx)(n.code,{children:"provisoning_parameters.json"})," file to our local directory:"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# copy file from S3 bucket \naws s3 cp s3://${BUCKET}/src/provisioning_parameters.json provisioning_parameters.json\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:["Open ",(0,t.jsx)(n.code,{children:"provisoning_parameters.json"})," file in your preferred text editor (vim, vi, nano) and add the new worker group you just created to this file. In the example file below we have added ",(0,t.jsx)(n.code,{children:"worker-group-2"})," (lines 10-13). An example of an updated ",(0,t.jsx)(n.code,{children:"provisioning_parameteres.json"})," file is below:"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n  "version": "1.0.0",\n  "workload_manager": "slurm",\n  "controller_group": "controller-machine",\n  "worker_groups": [\n    {\n      "instance_group_name": "worker-group-1",\n      "partition_name": "ml.p5.48xlarge"\n    },\n    {\n      "instance_group_name": "worker-group-2",\n      "partition_name": "ml.g5.48xlarge"\n    }\n  ],\n  "fsx_dns_name": "fs-XXXXXXXXXXXX.fsx.us-east-2.amazonaws.com",\n  "fsx_mountname": "XXXXXXX"\n}\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Upload the modified provisioning_parameters.json config to your LifeCycleScript S3 bucket."}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# copy to the S3 Bucket\naws s3 cp provisioning_parameters.json s3://${BUCKET}/src/\n"})}),"\n",(0,t.jsx)(n.h3,{id:"update-cluster",children:"Update-cluster"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["With the new ",(0,t.jsx)(n.code,{children:"provisioning_parameters.json"})," file uploaded to s3, and our new ",(0,t.jsx)(n.code,{children:"update-cluster-config.json"})," written locally, we can now execute our ",(0,t.jsx)(n.code,{children:"update-cluster"})," command."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"aws sagemaker update-cluster \\                \n    --cli-input-json file://update-cluster-config.json \\\n    --region $AWS_REGION\n"})}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["The cluster update will take approx 15-20 minutes to complete. You can monitor progress in HyperPod Console or by using ",(0,t.jsx)(n.code,{children:"aws sagemaker list-clusters"}),"."]})}),"\n",(0,t.jsx)(n.h3,{id:"update-slurm-configuration-on-updated-cluster-controller-node",children:"Update Slurm Configuration on updated cluster (controller node)"}),"\n",(0,t.jsx)(n.p,{children:'Once the new nodes are added to your cluster, ClusterStatus will change to "InService". Do not proceed with the following steps until ClusterStatus changes to\n"InService".'}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Connect to the controller node of your cluster (for a reminder of these steps, see ",(0,t.jsx)(n.a,{href:"/ai-on-sagemaker-hyperpod/docs/getting-started/orchestrated-by-slurm/ssh-into-hyperpod",children:"ssh into cluster"}),"):"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# SSH into controller node \n./easy-ssh.sh -c controller-machine <YOUR CLUSTER NAME>\n"})}),"\n",(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsxs)(n.p,{children:["By default, instances from your new worker-group will be added to the default slurm partition in your cluster (dev*). An example of what you might see output by ",(0,t.jsx)(n.code,{children:"sinfo"})," is below:"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"PARTITION      AVAIL  TIMELIMIT  NODES  STATE NODELIST\ndev*              up   infinite      6   idle ip-10-1-5-148,ip-10-1-8-91,ip-10-1-22-213,ip-10-1-25-35,ip-10-1-46-41,ip-10-1-123-39\nml.p5.48xlarge    up   infinite      2   idle ip-10-1-22-213,ip-10-1-123-39\n"})})]}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:["Create a backup of ",(0,t.jsx)(n.code,{children:"slurm.conf"})," and ",(0,t.jsx)(n.code,{children:"provisoning_parameters.json"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"#create a copy of the current slurm.conf configuration on the controller node\nsudo cp /opt/slurm/etc/slurm.conf /opt/slurm/etc/slurm.conf.bak\n\n#create a copy of the current provisoning_parameters.json on the controller node\nsudo cp /opt/ml/config/provisioning_parameters.json /opt/ml/config/provisioning_parameters.json.bak\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsxs)(n.li,{children:["Copy the latest ",(0,t.jsx)(n.code,{children:"provisioning_parameters.json"})," from your s3 LifeSycle Script bucket to /opt/ml/config/ on the controller node."]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["You can get the path for your s3 bucket in the SageMaker HyperPod console. Select your cluster > instance group > Lifecycle configuration.\nYou can also find this from your local deployment environment with: ",(0,t.jsx)(n.code,{children:'cat update-cluster-config.json | grep "SourceS3Uri"'}),"."]})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"#copy updated slurm configuration onto the head node\nsudo aws s3 cp <s3URI>/provisioning_parameters.json /opt/ml/config/\nsudo cat /opt/ml/config/provisioning_parameters.json\n"})}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["You should see the new worker groups reflected in ",(0,t.jsx)(n.code,{children:"/opt/ml/config/provisioning_parameters.json"})," when cat'ed above."]})}),"\n",(0,t.jsxs)(n.ol,{start:"4",children:["\n",(0,t.jsxs)(n.li,{children:["Edit slurm.conf. Delete the existing node name records (these will be repopulated when we restart the cluster agent). if you need to revert back to the old configuration for any reason, we created a backup called ",(0,t.jsx)(n.code,{children:"/opt/slurm/etc/slurm.conf.bak"})," in step 2."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo sed -i '/NodeName/d' /opt/slurm/etc/slurm.conf\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"5",children:["\n",(0,t.jsx)(n.li,{children:"Stop the slurm controller daemon, restart cluster agent, and restart slurm controller daemon."}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsxs)(n.p,{children:["Note that stopping ",(0,t.jsx)(n.code,{children:"slurmctld"})," below will temporarily disrupt job submission and slurm queue. This will not impact any running jobs, however it may impact any jobs that are submitted to the slurm queue during the period which slurmctld is stopped (approx 60 seconds)."]})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo systemctl stop slurmctld\nsudo systemctl restart sagemaker-cluster-agent\nsudo systemctl start slurmctld\n"})}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["It will take approx 2 minutes for the sagemaker cluster agent to restart and slurmctld to recognize the new configuration. If you get get the following error, wait approx 60 seconds and try ",(0,t.jsx)(n.code,{children:"sinfo"})," again: ",(0,t.jsx)(n.code,{children:"slurm_load_partitions: Unexpected message received"})]})}),"\n",(0,t.jsxs)(n.p,{children:["Running ",(0,t.jsx)(n.code,{children:"sinfo"})," should now reflect your new slurm configuration, example provided below:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"PARTITION      AVAIL  TIMELIMIT  NODES  STATE NODELIST\ndev*              up   infinite      3   idle ip-10-1-100-222,ip-10-1-112-66,ip-10-1-125-167\nml.p5.48xlarge    up   infinite      2   idle ip-10-1-100-222,ip-10-1-112-66\nml.g5.48xlarge    up   infinite      1   idle ip-10-1-125-167\n"})}),"\n",(0,t.jsx)(n.p,{children:"Congratulations! You have successfully updated your sagemaker HyperPod cluster to add a new worker-group and create a new partition!"})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>i});var o=r(6540);const t={},s=o.createContext(t);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(s.Provider,{value:n},e.children)}},9339:(e,n,r)=>{r.d(n,{A:()=>o});const o=r.p+"assets/images/heterogenous-cluster-22c8ea42369ca0e99fca09ab244bd73f.png"},9365:(e,n,r)=>{r.d(n,{A:()=>a});r(6540);var o=r(4164);const t={tabItem:"tabItem_Ymn6"};var s=r(4848);function a({children:e,hidden:n,className:r}){return(0,s.jsx)("div",{role:"tabpanel",className:(0,o.A)(t.tabItem,r),hidden:n,children:e})}}}]);