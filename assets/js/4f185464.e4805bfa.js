"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[3727],{4726:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"eks-blueprints/inference/inference-operator/amazon-s3-and-amazon-fsx","title":"Deploying model from S3 or FSX","description":"You can deploy model artifacts directly from S3 or FSX to your HyperPod cluster using the InferenceEndpointConfig resource. The inference operator will use the S3 CSI driver to provide the model files to the pods in the cluster. Using this configuration, the operator will download the files located under the prefix deepseek15b as set by the modelLocation parameter.","source":"@site/docs/01-eks-blueprints/inference/inference-operator/amazon-s3-and-amazon-fsx.md","sourceDirName":"01-eks-blueprints/inference/inference-operator","slug":"/eks-blueprints/inference/inference-operator/amazon-s3-and-amazon-fsx","permalink":"/docs/eks-blueprints/inference/inference-operator/amazon-s3-and-amazon-fsx","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"SageMaker JumpStart","permalink":"/docs/eks-blueprints/inference/inference-operator/sagemaker-jumpstart"},"next":{"title":"Mistral 7B with Load Balancer","permalink":"/docs/eks-blueprints/inference/load-balancer-inference/inference-with-loadbalancer"}}');var s=o(4848),t=o(8453);const a={},i="Deploying model from S3 or FSX",l={},c=[{value:"Prerequisite",id:"prerequisite",level:2},{value:"Deploy the model",id:"deploy-the-model",level:2},{value:"Invoking the model",id:"invoking-the-model",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"deploying-model-from-s3-or-fsx",children:"Deploying model from S3 or FSX"})}),"\n",(0,s.jsxs)(n.p,{children:["You can deploy model artifacts directly from S3 or FSX to your HyperPod cluster using the InferenceEndpointConfig resource. The inference operator will use the S3 CSI driver to provide the model files to the pods in the cluster. Using this configuration, the operator will download the files located under the prefix ",(0,s.jsx)(n.code,{children:"deepseek15b"})," as set by the ",(0,s.jsx)(n.code,{children:"modelLocation"})," parameter."]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisite",children:"Prerequisite"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Clone the repository ",(0,s.jsx)(n.a,{href:"https://github.com/aws-samples/sagemaker-genai-hosting-examples/tree/main",children:"sagemaker-genai-hosting-examples"})," and open the directory under sagemaker-genai-hosting-examples/SageMakerHyperPod/hyperpod-inference"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/aws-samples/sagemaker-genai-hosting-examples.git\ncd sagemaker-genai-hosting-examples/SageMakerHyperPod/hyperpod-inference\n"})}),"\n",(0,s.jsx)(n.h2,{id:"deploy-the-model",children:"Deploy the model"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Prepare model artifacts"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"When deploying a model from S3, we define the prefix that the model artifacts are available under."}),"\n",(0,s.jsx)(n.p,{children:"You can upload the DeepSeek Qwen 1.5b artifacts to your S3 bucket or FSX ID, below is an example of copying it on S3 bucket with the following:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"s3_bucket=<bucket_name>\naws s3 sync s3://jumpstart-cache-prod-us-east-2/deepseek-llm/deepseek-llm-r1-distill-qwen-1-5b/artifacts/inference-prepack/v2.0.0 s3://$s3_bucket/deepseek15b\n"})}),"\n",(0,s.jsx)(n.p,{children:"Alternatively, you can copy the model to FSxL by creating a job pod with the FSX PVC mounted:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: copy-model-to-fsx\nspec:\n  template:\n    spec:\n      containers:\n      - name: aws-cli\n        image: amazon/aws-cli:latest\n        command: ["/bin/bash"]\n        args: \n        - -c\n        - |\n          aws s3 sync s3://jumpstart-cache-prod-us-east-2/deepseek-llm/deepseek-llm-r1-distill-qwen-1-5b/artifacts/inference-prepack/v2.0.0 /fsx/deepseek15b\n        volumeMounts:\n        - name: fsx-storage\n          mountPath: /fsx\n        env:\n        - name: AWS_DEFAULT_REGION\n          value: "us-east-1"  # Replace with your region\n      volumes:\n      - name: fsx-storage\n        persistentVolumeClaim:\n          claimName: fsx-claim\n      restartPolicy: Never\n  backoffLimit: 3\n'})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsxs)(n.li,{children:["Then in the ",(0,s.jsx)(n.code,{children:"deploy_S3_inference_operator.yaml"})," or ",(0,s.jsx)(n.code,{children:"deploy_fsx_lustre_inference_operator.yaml"})," we configure our S3 bucket/FSx ID as the ",(0,s.jsx)(n.code,{children:"s3Storage:bucketName"})," or ",(0,s.jsx)(n.code,{children:"fsxStorage:fileSystemId"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"spec:\n  modelName: deepseek15b\n  endpointName: deepseek15b\n  instanceType: ml.g5.8xlarge\n  invocationEndpoint: invocations\n  modelSourceConfig:\n    modelSourceType: s3\n    s3Storage:\n      bucketName: <bucket name>\n"})}),"\n",(0,s.jsx)(n.p,{children:"Or for FSxL use"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"spec:\n  endpointName: deepseek15b\n  instanceType: ml.g5.8xlarge\n  invocationEndpoint: invocations\n  modelName: deepseek15b\n  modelSourceConfig:\n    fsxStorage:\n      fileSystemId: <fs-ID1234abcd>\n    modelLocation: deepseek-1-5b\n    modelSourceType: fsx\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsx)(n.li,{children:"Then we apply it to our cluster"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f deploy_S3_inference_operator.yaml \n"})}),"\n",(0,s.jsx)(n.p,{children:"Or"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f deploy_fsx_inference_operator.yaml \n"})}),"\n",(0,s.jsxs)(n.ol,{start:"4",children:["\n",(0,s.jsx)(n.li,{children:"Check status of your deployment"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl describe InferenceEndpointConfig deepseek15b -n default\n"})}),"\n",(0,s.jsx)(n.h2,{id:"invoking-the-model",children:"Invoking the model"}),"\n",(0,s.jsx)(n.p,{children:'Once the model is deployed, you can invoke the SageMaker Endpoint that is created using the InvokeEndpoint API. In the YAML file, this is defined as "deepseek15b".'}),"\n",(0,s.jsx)(n.p,{children:"For example, using boto3, this would be:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import boto3\nimport json\n\nclient = boto3.client('sagemaker-runtime')\n\nresponse = client.invoke_endpoint(\n    EndpointName='deepseek15b',\n    ContentType='application/json',\n    Accept='application/json',\n    Body=json.dumps({\n    \t\"inputs\": \"Hi, what can you help me with?\"\n    })\n)\n\nprint(response['Body'].read().decode('utf-8'))\n"})})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>i});var r=o(6540);const s={},t=r.createContext(s);function a(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);