"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[799],{5723:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>i,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"Tips/Slurm/delete-cluster-nodes","title":"Delete Cluster Nodes","description":"The SageMaker BatchDeleteClusterNode API allows you to delete specific nodes within a SageMaker HyperPod cluster. BatchDeleteClusterNodes accepts a cluster name and a list of node IDs.","source":"@site/docs/08-Tips/Slurm/19-delete-cluster-nodes.md","sourceDirName":"08-Tips/Slurm","slug":"/Tips/Slurm/delete-cluster-nodes","permalink":"/ai-on-sagemaker-hyperpod/docs/Tips/Slurm/delete-cluster-nodes","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":19,"frontMatter":{"title":"Delete Cluster Nodes","weight":59},"sidebar":"tutorialSidebar","previous":{"title":"Enable Slurm epilog Script","permalink":"/ai-on-sagemaker-hyperpod/docs/Tips/Slurm/slurm-epilogue"},"next":{"title":"Troubleshoot IAM Permissions","permalink":"/ai-on-sagemaker-hyperpod/docs/Tips/Slurm/troubeshoot-permissions"}}');var o=s(4848),r=s(8453);const a={title:"Delete Cluster Nodes",weight:59},l=void 0,i={},d=[{value:"Prepare cluster for node deletion",id:"prepare-cluster-for-node-deletion",level:2},{value:"Set node state to DOWN",id:"set-node-state-to-down",level:5},{value:"Confirm InstanceID of DOWN nodes:",id:"confirm-instanceid-of-down-nodes",level:5},{value:"Execute Node Deletion",id:"execute-node-deletion",level:5}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h5:"h5",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.p,{children:["The SageMaker ",(0,o.jsx)(n.a,{href:"https://docs.aws.amazon.com/sagemaker/latest/dg/smcluster-scale-down.html#smcluster-scale-down-batchdelete",children:"BatchDeleteClusterNode API"})," allows you to delete specific nodes within a SageMaker HyperPod cluster. BatchDeleteClusterNodes accepts a cluster name and a list of node IDs."]}),"\n",(0,o.jsx)(n.admonition,{type:"info",children:(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Note:"})," The following instructions apply to cluster created after 6/20/2024. If your cluster was created before this date, you will need to run ",(0,o.jsx)(n.code,{children:"aws sagemaker update-cluster-software"})," before executing below steps. See ",(0,o.jsx)(n.a,{href:"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-release-notes.html#sagemaker-hyperpod-release-notes-20240620",children:"release notes"}),"."]})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'aws sagemaker batch-delete-cluster-nodes --cluster-name "cluster-name" --node-ids \'["i-111112222233333", "i-111112222233333"]\'\n'})}),"\n",(0,o.jsx)(n.p,{children:"Below are the recommended steps to use this API safely on a production HyperPod Cluster, without disrupt running jobs on your cluster."}),"\n",(0,o.jsx)(n.h2,{id:"prepare-cluster-for-node-deletion",children:"Prepare cluster for node deletion"}),"\n",(0,o.jsx)(n.h5,{id:"set-node-state-to-down",children:"Set node state to DOWN"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Run the following on the compute/login node of your HyperPod Cluter"})}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"List the current nodes and their status withing the cluster:"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"sinfo\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:"The following command will generate a list of ALL IDLE Node IP Addresses, which can be passed to scontrol:"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'export IDLE_NODES_TO_TERMINATE=$(sinfo --noheader --state=idle -o "%N")\n'})}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsx)(n.li,{children:"It is best practice to set the nodes in DOWN state in Slurm before Terminating them. This will ensure the slurm scheduler will not allocate new jobs on these nodes. Run the below command to set all IDLE nodes in the cluster to DOWN."}),"\n"]}),"\n",(0,o.jsx)(n.admonition,{type:"warning",children:(0,o.jsxs)(n.p,{children:["The below example will set all IDLE nodes to Down. If you want to specify specific nodes to terminate, you can do so instead with: ",(0,o.jsx)(n.code,{children:'sudo scontrol update NodeName=<Node_IP> State=Down Reason="Termination"'})]})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'sudo scontrol update NodeName=$IDLE_NODES_TO_TERMINATE State=Down Reason="Termination"\n'})}),"\n",(0,o.jsxs)(n.ol,{start:"4",children:["\n",(0,o.jsx)(n.li,{children:"Confirm the nodes are set to DOWN in Slurm"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'export NODES_SET_FOR_TERMINATION=$(sinfo -R --noheader -o "%N")\necho $NODES_SET_FOR_TERMINATION\n'})}),"\n",(0,o.jsxs)(n.ol,{start:"5",children:["\n",(0,o.jsx)(n.li,{children:"Broadcast a message to other users on the cluster:"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'sudo wall "The nodes $NODES_SET_FOR_TERMINATION have been set to DOWN and are scheduled for termination. Please take note"\n'})}),"\n",(0,o.jsx)(n.h5,{id:"confirm-instanceid-of-down-nodes",children:"Confirm InstanceID of DOWN nodes:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Create and run the following script. The ",(0,o.jsx)(n.code,{children:"get-node-status.sh"})," script will Retrieve the InstanceID for each of the Cluster Nodes. This script will parse the ",(0,o.jsx)(n.code,{children:"/opt/ml/config/resource_config.json"})," file created by HyperPod to get the instance IDs."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Create the script:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'cat <<\'EOF\' > get-node-status.sh\n#!/bin/bash\n\n# Print the header for the table\nprintf "%-20s %-20s %-30s %-10s %-40s\\n" "IP Address" "Instance ID" "Instance Name" "Status" "Reason"\nprintf "%-20s %-20s %-30s %-10s %-40s\\n" "--------------------" "--------------------" "------------------------------" "----------" "----------------------------------------"\n\n# Extract details and check node status with scontrol\nsudo cat /opt/ml/config/resource_config.json | jq -r \'.InstanceGroups[].Instances[] | "\\(.CustomerIpAddress) \\(.InstanceId) \\(.InstanceName)"\' | while read -r ip instance_id instance_name; do\n    # Format IP address by replacing dots with dashes\n    formatted_ip="ip-${ip//./-}"\n\n    # Run scontrol to get the node status using formatted IP\n    node_status=$(scontrol show node "$formatted_ip" | grep -oP \'State=\\K\\w+\')\n\n    # Get the reason from sinfo -R, match the formatted IP with the nodes\n    node_reason=$(sinfo -R --noheader | grep "$formatted_ip" | awk \'{print $1}\')\n\n    # Print each line in a formatted way\n    printf "%-20s %-20s %-30s %-10s %-40s\\n" "$formatted_ip" "$instance_id" "$instance_name" "${node_status:-unknown}" "${node_reason:-unknown}"\ndone\nEOF\n'})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:"Run the script:"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"bash get-node-status.sh\n"})}),"\n",(0,o.jsx)(n.h5,{id:"execute-node-deletion",children:"Execute Node Deletion"}),"\n",(0,o.jsx)(n.admonition,{type:"info",children:(0,o.jsxs)(n.p,{children:["To execute the batch delete-cluster-node command, you will need to call the API from a development environment with the AWS CLI installed. You can use ",(0,o.jsx)(n.a,{href:"https://aws.amazon.com/cloudshell/",children:"AWS CloudShell"})," from within your AWS Account, which comes pre-installed with the AWS CLI"]})}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Execute the batch-delete-cluster-nodes api:"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"#Confirm cluster name\naws sagemaker list-clusters --region <YOUR_REGION>\n\n#Confirm cluster nodes\n\naws sagemaker list-cluster-nodes --cluster-name <CLUSTER_NAME> --region <YOUR_REGION>\naws sagemaker batch-delete-cluster-nodes --cluster-name <YOUR_CLUSTER_NAME> --node-ids [InstanceIDs] --region <YOUR_REGION>\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsxs)(n.li,{children:["When the ",(0,o.jsx)(n.code,{children:"batch-delete-cluster-nodes"})," API has been executed successfully, you will see an output similar to the following:"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'$ aws sagemaker batch-delete-cluster-nodes --cluster-name ml-cluster --node-ids i-04486002ebdb59e0a i-0b8bc2c52dc7b1fa2\n{\n    "Successful": [\n        "i-04486002ebdb59e0a",\n        "i-0b8bc2c52dc7b1fa2"\n    ]\n}\n'})}),"\n",(0,o.jsx)(n.admonition,{type:"info",children:(0,o.jsx)(n.p,{children:"If you are using Reserved Instances for your HyperPod Cluster (via a Neogatiated PPA), you will continue to be billed for nodes which have been deleted from your cluster. If you wish to no longer be billed for the deleted instances, contact your AWS Account / Support team to notify them of the node termination and request for billing to be termianted for the deleted nodes."})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>l});var t=s(6540);const o={},r=t.createContext(o);function a(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);