---
title: "a. Easy cluster setup"
weight: 2
---


## Overview

**Note: If you run the steps provided on this page, you may *skip* the steps listed under Option B (i.e., go straight to [e. AWS Console](/01-cluster/05-console.md)). If you're interested in what the provided script does, please check out the "Optional Reads" section below**

In this section, we provide you with a [script](https://github.com/aws-samples/awsome-distributed-training/tree/main/1.architectures/7.sagemaker-hyperpod-eks/automate-smhp-eks) that will walk you through the following:

1. Installing the right packages in your environment (egs. aws cli, helm, kubectl, eksctl) 
2. Installing the Lifecycle Scripts used by HyperPod nodes during bootstrapping
3. Creating the cluster configuration, and uploading all your assets to Amazon S3 for cluster creation
4. Configuring your EKS cluster, including setting up your cluster context (it also lets you choose the right context, if the incorrect one is chosen), adding your IAM user (and others), installing HyperPod dependencies on your EKS cluster (via helm)
5. At re\:Invent 2024, we launched [SageMaker HyperPod training plans](https://aws.amazon.com/blogs/machine-learning/speed-up-your-cluster-procurement-time-with-amazon-sagemaker-hyperpod-training-plans/). This script also includes automation to use your training plans! 

:::alert{header="Asks when you run this script" type="info"}
 The script guides you through the entire onboarding process so you can create a SageMaker HyperPod EKS cluster by yourself. There are detailed instructions and questions (example pictured). Please read through the descriptions, and make sure your answers conform to the suggested format(s). There's a demo of an entire run through below for your reference.

:image[Automate Script Description]{src="/static/images/01-cluster/automate-smhp-eks-demo.gif" height=650} 
:::

:::alert{header="" type="info"}

### Important Information
You will be asked questions by the script for customizing your HyperPod cluster. Stay on the lookout for questions that ask you for your input. For example,
```
# In this example, you are asked to input the instance type for your cluster's worker group. The default value is ml.c5.4xlarge
Enter the instance type for your worker group [ml.c5.4xlarge]:
```

**Anything you see in square brackets [] are default values to questions. If you want to choose the default value, you may hit ENTER. Else, type in a different value.**
:::

## Setup Environment
To run this automation script, clone [awsome-distributed-training](https://github.com/aws-samples/awsome-distributed-training), or directly download the script (as shown below), and run it on your local (Linux/macOS) terminal. It is recommended to run this script from your own directory, rather than running `cd` into the cloned repository directory. Make sure you're configured as an aws user or role before running the script.
```bash
# Clone the repository
mkdir hyperpod && cd hyperpod

curl -O https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/refs/heads/main/1.architectures/7.sagemaker-hyperpod-eks/automate-smhp-eks/automate-eks-cluster-creation.sh

# Run the script
./automate-eks-cluster-creation.sh
```

Voila! Once you get through the script, you should see a directory that looks like
```bash
hyperpod/
|-- automate-cluster-creation.sh   # This script!
|-- awsome-distributed-training/   # The script clones the repo with all the Lifecycle Scripts!ÃŸ
|-- cluster-config.json            # Cluster configuration generated by this script. Run `vi cluster-config.json` to make changes
|-- env_vars                       # Your environment variables used to create the SMHP cluster
`-- ...
```

:::alert{header="Important" type="warning"}
The last input of the script gives you the option on whether or not you want the cluster created for you. The default option is `yes` unless you submit `no`. This means that you may hit ENTER to let the script create your HyperPod cluster for you. If you did not hit ENTER, paste the following command to create your cluster: 

```bash
aws sagemaker create-cluster \
    --cli-input-json file://cluster-config.json \
    --region $AWS_REGION
```

:::


### Monitor cluster creation status

You can check the status of your cluster using either the SageMaker console, or the command below. Feel free to add a `watch -n 1` before this to refresh the output every second.

```bash
aws sagemaker list-clusters --output table
```

**Cluster creation typically takes 12-20 minutes.**

You'll see output similar to the following:

```
-------------------------------------------------------------------------------------------------------------------------------------------------
|                                                                 ListClusters                                                                  |
+-----------------------------------------------------------------------------------------------------------------------------------------------+
||                                                              ClusterSummaries                                                               ||
|+----------------------------------------------------------------+----------------------+----------------+------------------------------------+|
||                           ClusterArn                           |     ClusterName      | ClusterStatus  |           CreationTime             ||
|+----------------------------------------------------------------+----------------------+----------------+------------------------------------+|
||  arn:aws:sagemaker:us-west-2:159553542841:cluster/uwme6r18mhic |  ml-cluster          |  Creating     |  2024-12-04T16:59:09.433000+00:00   ||
|+----------------------------------------------------------------+----------------------+----------------+------------------------------------+|
```


The main file that will be used is the `cluster-config.json`. 
:::alert{header="Want to change the cluster configuration?" type="info"}
 If you'd like to make changes to the `cluster-config.json` and `provisioning_parameters.json`, you can open them up in a text editor like `vim` and make the changes. 
 ```bash
 # Edit the cluster configuration file
 vim cluster-config.json
 ```
:::

This is the end of the *required parts* of this section. You may skip to [1e. AWS Console](/01-cluster/05-console.md).
===
---

## Optional Reads: More information 

While waiting for your cluster to create, if you're interested in learning more about what the script does, feel free to take a look at the sections below! 

#### What environment variables are set?
:::::alert{header="Note:"}
The `create_config.sh` script checks for existing values of the following environment variable variables before using the default values.
| Environment Variable | Type | Default Value | Description | 
|:---|:---|:---|:---|
|AWS_REGION| string | The default region configured for the AWS CLI |The region where you will deploy your HyperPod cluster|
|ACCEL_INSTANCE_TYPE | string | ml.g5.12xlarge | The accelerated compute instance type you want to use|
|ACCEL_COUNT |integer| 1 |The number of accelerated compute nodes you want to deploy|
|ACCEL_VOLUME_SIZE| integer | 500 (GB) | The size of the EBS volume attached to the accelerated compute nodes| 
|GEN_INSTANCE_TYPE | string | ml.m5.2xlarge |  The general purpose compute instance type you want to use|
|GEN_COUNT |integer| 1 |The number of general purpose compute nodes you want to deploy|
|GEN_VOLUME_SIZE| integer | 500 (GB) | The size of the EBS volume attached to the general purpose compute nodes| 
|NODE_RECOVERY| string | Automatic | Enable node auto-recovery. Set to "None" to disable. |

If you don't want to use the default values, set the environment variables in your bash shell session before running the `create_config.sh` script.

For example, if you want to use another accelerated compute instance type besides `ml.g5.12xlarge`, run one of the following commands: 

::::tabs{variant="container" activeTabId="ml.m5.2xlarge"}

:::tab{id="ml.m5.2xlarge" label="ml.m5.2xlarge (Workshop Studio)"}

```bash 
export ACCEL_INSTANCE_TYPE=ml.m5.2xlarge
```
:::

:::tab{id="ml.trn1.32xlarge" label="ml.trn1.32xlarge"}

```bash 
export ACCEL_INSTANCE_TYPE=ml.trn1.32xlarge
```
:::

:::tab{id="ml.p4d.24xlarge" label="ml.p4d.24xlarge"}

```bash 
export ACCEL_INSTANCE_TYPE=ml.p4d.24xlarge
```
:::

:::tab{id="ml.p5.48xlarge" label="ml.p5.48xlarge"}

```bash 
export ACCEL_INSTANCE_TYPE=ml.p5.48xlarge
```
:::

:::tab{id="ml.g5.8xlarge" label="ml.g5.8xlarge"}

```bash 
export ACCEL_INSTANCE_TYPE=ml.g5.8xlarge
```
:::

::::

If you did not deploy the CloudFormation stack in Full Deployment Mode, ensure that you set the following environment variables prior to running the `create_config.sh` script: 

:::expand{header="Integrative Deployment Mode Environment Variables" defaultExpanded=false}
```bash
export EKS_CLUSTER_ARN=<YOUR_EKS_CLUSTER_ARN_HERE>
export EKS_CLUSTER_NAME=<YOUR_EKS_CLUSTER_NAME_HERE>
```
:::

:::expand{header="Minimal Deployment Mode Environment Variables" defaultExpanded=false}
```bash
export EKS_CLUSTER_ARN=<YOUR_EKS_CLUSTER_ARN_HERE>
export EKS_CLUSTER_NAME=<YOUR_EKS_CLUSTER_NAME_HERE>
export VPC_ID=<YOUR_VPC_ID_HERE>
export SUBNET_ID=<YOUR_SUBNET_ID_HERE>
export SECURITY_GROUP=<YOUR_SECURITY_GROUP_ID_HERE>
```
:::

:::::


#### What Dependencies get installed on your EKS cluster?
:::::alert{header="Note:"}
Several dependencies like the health monitoring agent and various role based access control policies for basic and deep health checks are required to be installed prior to HyperPod cluster creation. As a result, if you are using a new EKS cluster with no worker nodes attached yet, you will notice that all deployed pods are in a pending state. This is expected behavior, and all pods should update to a running state when HyperPod worker nodes are added later in the workshop. 
:::::
---
:::expand{header="Health Monitoring Agent" defaultExpanded=false}

HyperPod health-monitoring agent continuously monitors the health status of each GPU/Trn-based node. When it detects any failures (such as GPU failure, Kernel deadlock and driver crash), the agent marks the node as unhealthy.

Verify deployment: 
```bash
 kubectl get ds health-monitoring-agent -n aws-hyperpod
```
:::
---
:::expand{header="NVIDIA device plugin for Kubernetes" defaultExpanded=false}

The [NVIDIA device plugin for Kubernetes](https://github.com/NVIDIA/k8s-device-plugin) is a Daemonset that allows you to automatically:

- Expose the number of GPUs on each nodes of your cluster
- Keep track of the health of your GPUs
- Run GPU enabled containers in your Kubernetes cluster.

Verify deployment: 
```bash
kubectl get ds hyperpod-dependencies-nvidia-device-plugin -n kube-system
```
:::

---
:::expand{header="Neuron device plugin" defaultExpanded=false}

The [Neuron device plugin](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/containers/tutorials/k8s-setup.html) exposes Neuron cores & devices on [Trainium](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/trainium.html) (`ml.trn1.32xlarge`) instances to kubernetes as a resource.

Verify deployment:
```bash
kubectl get ds neuron-device-plugin-daemonset -n kube-system
```
:::
---
:::expand{header="EFA Kubernetes device plugin" defaultExpanded=false}

[Elastic Fabric Adapter (EFA)](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html) is a network interface for Amazon EC2 instances that uses a custom-built operating system bypass hardware interface to enhance the performance of inter-instance communications, allowing High Performance Computing (HPC) applications using the Message Passing Interface (MPI) and Machine Learning (ML) applications using [NVIDIA Collective Communications Library (NCCL)](https://developer.nvidia.com/nccl) to scale to thousands of CPUs or GPUs. 

Integrating EFA with applications running on Amazon EKS clusters can reduce the time to complete large scale distributed training workloads without having to add additional instances to your cluster. 

The [EFA Kubernetes device plugin](https://github.com/aws/eks-charts/tree/master/stable/aws-efa-k8s-device-plugin) detects and advertises EFA interfaces as allocatable resources to Kubernetes. An application can consume the extended resource type vpc.amazonaws.com/efa in a Pod request spec just like CPU and memory. Once requested, the plugin automatically assigns and mounts an EFA interface to the Pod. Using the device plugin simplifies EFA setup and does not require a Pod to run in privileged mode.

Verify deployment: 
```bash
kubectl get ds hyperpod-dependencies-aws-efa-k8s-device-plugin
```
:::
---
:::expand{header="Kubeflow Training Operator" defaultExpanded=false}

The [Kubeflow Training Operator](https://github.com/kubeflow/training-operator) is a tool designed for running and scaling machine learning model training on Kubernetes. It allows you to train models built with frameworks like TensorFlow and PyTorch by leveraging Kubernetes resources, simplifying the process of training complex models in a distributed way. You will use the Kubeflow training operator throughout this workshop to create various [PyTorch training jobs](https://www.kubeflow.org/docs/components/training/user-guides/pytorch/) running on SageMaker HyperPod nodes. 

Verify deployment: 
```bash
 kubectl get deploy hyperpod-dependencies-training-operators -n kubeflow
```
View the custom resource definitions for each supported ML framework: 
```bash 
kubectl get crd | grep kubeflow
```
```
mpijobs.kubeflow.org                         2024-07-11T22:35:03Z
mxjobs.kubeflow.org                          2024-07-11T22:00:35Z
paddlejobs.kubeflow.org                      2024-07-11T22:00:36Z
pytorchjobs.kubeflow.org                     2024-07-11T22:00:37Z
tfjobs.kubeflow.org                          2024-07-11T22:00:38Z
xgboostjobs.kubeflow.org                     2024-07-11T22:00:39Z

```
:::
---
:::expand{header="Kubeflow MPI Operator" defaultExpanded=false}

The [Kubeflow MPI Operator](https://github.com/kubeflow/mpi-operator) makes it easy to run allreduce-style distributed training on Kubernetes. 

Verify deployment: 
```bash 
kubectl get deploy hyperpod-dependencies-mpi-operator
```

The MPI Operator comes prepackaged with the MPIJob v2beta1 custom resource definition (CRD). 

Verify the version of the MPIJob CRD:
```bash
kubectl get crd mpijobs.kubeflow.org -n kubeflow -o jsonpath='{.status.storedVersions[]}'
```
:::
---
:::expand{header="Kubernetes PriorityClass" defaultExpanded=false}

Pods can have priority. Priority indicates the importance of a Pod relative to other Pods. If a Pod cannot be scheduled, the scheduler tries to preempt (evict) lower priority Pods to make scheduling of the pending Pod possible.

A [PriorityClass](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass) is a non-namespaced object that defines a mapping from a priority class name to the integer value of the priority. 

Verify deployment: 
```bash
kubectl get priorityclass
```
:::

#### Cluster Configuration
::::tabs{variant="container" activeTabId="simplified"}

:::tab{id="simplified" label="Simplified (Workshop Studio)"}

- Single instance group
- Deep Health Checks are disabled.
- Modify threadsPerCore(set it to 2 for G5/P4/P5)

```json
cat > cluster-config.json << EOL
{
    "ClusterName": "ml-cluster",
    "Orchestrator": { 
      "Eks": 
      {
        "ClusterArn": "${EKS_CLUSTER_ARN}"
      }
    },
    "InstanceGroups": [
      {
        "InstanceGroupName": "worker-group-1",
        "InstanceType": "${ACCEL_INSTANCE_TYPE}",
        "InstanceCount": ${ACCEL_COUNT},
        "InstanceStorageConfigs": [
          {
            "EbsVolumeConfig": {
              "VolumeSizeInGB": ${ACCEL_VOLUME_SIZE}
            }
          }
        ],
        "LifeCycleConfig": {
          "SourceS3Uri": "s3://${BUCKET_NAME}",
          "OnCreate": "on_create.sh"
        },
        "ExecutionRole": "${EXECUTION_ROLE}",
        "ThreadsPerCore": 2
      }
    ],
    "VpcConfig": {
      "SecurityGroupIds": ["$SECURITY_GROUP"],
      "Subnets":["$SUBNET_ID"]
    },
    "NodeRecovery": "${NODE_RECOVERY}"
}
EOL
```

:::

:::tab{id="full" label="Full"}

- Two instance groups
- Deep Health Checks are enabled
- Modify threadsPerCore(set it to 2 for G5/P4/P5)

```json
cat > cluster-config.json << EOL
{
    "ClusterName": "ml-cluster",
    "Orchestrator": { 
      "Eks": 
      {
        "ClusterArn": "${EKS_CLUSTER_ARN}"
      }
    },
    "InstanceGroups": [
      {
        "InstanceGroupName": "worker-group-1",
        "InstanceType": "${ACCEL_INSTANCE_TYPE}",
        "InstanceCount": ${ACCEL_COUNT},
        "InstanceStorageConfigs": [
          {
            "EbsVolumeConfig": {
              "VolumeSizeInGB": ${ACCEL_VOLUME_SIZE}
            }
          }
        ],
        "LifeCycleConfig": {
          "SourceS3Uri": "s3://${BUCKET_NAME}",
          "OnCreate": "on_create.sh"
        },
        "ExecutionRole": "${EXECUTION_ROLE}",
        "ThreadsPerCore": 2,
        "OnStartDeepHealthChecks": ["InstanceStress", "InstanceConnectivity"]
      },
      {
        "InstanceGroupName": "worker-group-2",
        "InstanceType": "${GEN_INSTANCE_TYPE}",
        "InstanceCount": ${GEN_COUNT},
        "InstanceStorageConfigs": [
          {
            "EbsVolumeConfig": {
              "VolumeSizeInGB": ${GEN_VOLUME_SIZE}
            }
          }
        ],
        "LifeCycleConfig": {
          "SourceS3Uri": "s3://${BUCKET_NAME}",
          "OnCreate": "on_create.sh"
        },
        "ExecutionRole": "${EXECUTION_ROLE}",
        "ThreadsPerCore": 1
      }
    ],
    "VpcConfig": {
      "SecurityGroupIds": ["$SECURITY_GROUP"],
      "Subnets":["$SUBNET_ID"]
    },
    "NodeRecovery": "${NODE_RECOVERY}"
}
EOL
```

:::

::::

##### Cluster Configuration Parameters:

- You can configure up to 20 instance groups under the `InstanceGroups` parameter. 
- For `Orchestrator.Eks.ClusterArn`, specify the ARN of the EKS cluster you want to use as the orchestrator. 
- For `OnStartDeepHealthChecks`, add `InstanceStress` and `InstanceConnectivity` to enable deep health checks. 
- For `NodeRecovery`, specify `Automatic` to enable automatic node recovery. HyperPod replaces or reboots instances (nodes) that fail the basic health or deep health checks (when enabled). 
- For the `VpcConfig` parameter, specify the information of the VPC used in the EKS cluster. The subnets must be private