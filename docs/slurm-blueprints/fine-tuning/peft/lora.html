<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-slurm-blueprints/fine-tuning/peft/lora/lora" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">LoRA - Tranium | AI on SageMaker HyperPod</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://awslabs.github.io/img/header_background.png"><meta data-rh="true" name="twitter:image" content="https://awslabs.github.io/img/header_background.png"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/docs/slurm-blueprints/fine-tuning/peft/lora"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="machine learning, generative ai, genai, sagemaker, hyperpod, SageMaker HyperPod,         model training, model inference, nemo framework, pytorch, pytorch framework"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="LoRA - Tranium | AI on SageMaker HyperPod"><meta data-rh="true" name="description" content="This example showcases how to train Llama 3 models using AWS Trainium instances and ü§ó Optimum Neuron. ü§ó Optimum Neuron is the interface between the ü§ó Transformers library and AWS Accelerators including AWS Trainium and AWS Inferentia. It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks."><meta data-rh="true" property="og:description" content="This example showcases how to train Llama 3 models using AWS Trainium instances and ü§ó Optimum Neuron. ü§ó Optimum Neuron is the interface between the ü§ó Transformers library and AWS Accelerators including AWS Trainium and AWS Inferentia. It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks."><link data-rh="true" rel="icon" href="/img/Amazon-Sagemaker-Icon.jpg"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/docs/slurm-blueprints/fine-tuning/peft/lora"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/docs/slurm-blueprints/fine-tuning/peft/lora" hreflang="en"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/docs/slurm-blueprints/fine-tuning/peft/lora" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"SLURM Blueprints","item":"https://awslabs.github.io/docs/category/slurm-blueprints"},{"@type":"ListItem","position":2,"name":"Fine Tuning","item":"https://awslabs.github.io/docs/category/fine-tuning-1"},{"@type":"ListItem","position":3,"name":"LoRA","item":"https://awslabs.github.io/docs/slurm-blueprints/fine-tuning/peft/lora/"}]}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="AI on SageMaker HyperPod RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="AI on SageMaker HyperPod Atom Feed"><link rel="stylesheet" href="/assets/css/styles.3e9092dc.css">
<script src="/assets/js/runtime~main.020652ce.js" defer="defer"></script>
<script src="/assets/js/main.ad341d2a.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="dark";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/Amazon-Sagemaker-Icon.jpg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/Amazon-Sagemaker-Icon.jpg" alt="AI on SageMaker HyperPod" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/Amazon-Sagemaker-Icon.jpg" alt="AI on SageMaker HyperPod" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI on SageMaker HyperPod</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Orchestrated by EKS</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/getting-started/orchestrated-by-eks/initial-cluster-setup">Initial cluster setup</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/trainium/aws-trainium">AWS Trainium</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/ddp/distributed-data-parallel">Distributed Data Parallel</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/fsdp/fully-sharded-data-parallel">Fully Sharded Data Parallel</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme">NVIDIA Megatron LM</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/ray-train/ray-train-readme">Ray Train</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Orchestrated by SLURM</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup">Initial cluster setup</a></li><li><a class="dropdown__link" href="/docs/slurm-blueprints/training/trainium/Llama3-70B">AWS Trainium</a></li><li><a class="dropdown__link" href="/docs/slurm-blueprints/training/ddp/distributed-data-parallel">Distributed Data Parallel</a></li><li><a class="dropdown__link" href="/docs/slurm-blueprints/training/fsdp/fully-sharded-data-parallel">Fully Sharded Data Parallel</a></li><li><a class="dropdown__link" href="/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme">NVIDIA Megatron LM</a></li></ul></div><a class="navbar__item navbar__link" href="/resources">Useful links</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/awslabs/ai-on-sagemaker-hyperpod" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/Introduction">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/getting-started">Getting Started</a><button aria-label="Expand sidebar category &#x27;Getting Started&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/eks-blueprints">EKS Blueprints</a><button aria-label="Expand sidebar category &#x27;EKS Blueprints&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/docs/category/slurm-blueprints">SLURM Blueprints</a><button aria-label="Collapse sidebar category &#x27;SLURM Blueprints&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/category/training-1">Training</a><button aria-label="Expand sidebar category &#x27;Training&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/category/fine-tuning-1">Fine Tuning</a><button aria-label="Collapse sidebar category &#x27;Fine Tuning&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/docs/slurm-blueprints/fine-tuning/peft/lora">PEFT</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/slurm-blueprints/fine-tuning/peft/lora">LoRA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/slurm-blueprints/fine-tuning/peft/qlora">QLoRA</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/slurm-blueprints/fine-tuning/preference-aligment/dpo">Preference Alignment</a></div></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/sagemaker-hyperpod-recipes">Sagemaker Hyperpod Recipes</a><button aria-label="Expand sidebar category &#x27;Sagemaker Hyperpod Recipes&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/add-ons">Add-Ons</a><button aria-label="Expand sidebar category &#x27;Add-Ons&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/validation-and-testing">Validation and Testing</a><button aria-label="Expand sidebar category &#x27;Validation and Testing&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/infrastructure-as-a-code">Infrastructure as a Code</a><button aria-label="Expand sidebar category &#x27;Infrastructure as a Code&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/tips">Tips</a><button aria-label="Expand sidebar category &#x27;Tips&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/category/slurm-blueprints"><span>SLURM Blueprints</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/category/fine-tuning-1"><span>Fine Tuning</span></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PEFT</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">LoRA</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>PEFT fine tuning of Llama 3 (trn1.32xlarge)</h1></header>
<p>This example showcases how to train Llama 3 models using AWS Trainium instances and ü§ó Optimum Neuron. ü§ó Optimum Neuron is the interface between the ü§ó Transformers library and AWS Accelerators including AWS Trainium and AWS Inferentia. It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="prerequisites">Prerequisites<a href="#prerequisites" class="hash-link" aria-label="Direct link to Prerequisites" title="Direct link to Prerequisites">‚Äã</a></h2>
<p>Before running this training, you&#x27;ll need to create a SageMaker HyperPod cluster with at least 1 trn1.32xlarge / trn1n.32xlarge instance group. Instructions can be found in the <a href="/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup">Cluster Setup</a> section.</p>
<p>You will also need to complete the following prerequisites for configuring and deploying your SageMaker HyperPod cluster for fine tuning:</p>
<ul>
<li>Submit a service quota increase request to get access to Trainium instances in your AWS Region. You will need to request an increase for Amazon EC2 Trn1 instances, ml.trn1.32xlarge or ml.trn1n.32xlarge.</li>
<li>Locally, install the AWS Command Line Interface (AWS CLI); the required minimum version needed is 2.14.3.</li>
<li>Locally, Install the AWS Systems Manager Session Manager Plugin in order to SSH into your cluster.</li>
</ul>
<p>Additionally, since Llama 3 is a gated model users have to register in Hugging Face and obtain an <a href="https://huggingface.co/docs/hub/en/security-tokens" target="_blank" rel="noopener noreferrer">access token</a> before running this example. You will also need to review and accept the license agreement on the <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" target="_blank" rel="noopener noreferrer">meta-llama/Meta-llama-3-8B-Instruct</a> model page.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a href="#setup" class="hash-link" aria-label="Direct link to Setup" title="Direct link to Setup">‚Äã</a></h2>
<p>In this section, we will setup our training environment on the cluster. Begin by logging into your cluster by following the <a href="/docs/getting-started/orchestrated-by-slurm/ssh-into-hyperpod">SSH into Cluster</a> section.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-download-training-scripts">Step 1: Download training scripts<a href="#step-1-download-training-scripts" class="hash-link" aria-label="Direct link to Step 1: Download training scripts" title="Direct link to Step 1: Download training scripts">‚Äã</a></h3>
<p>Begin by downloading the training scripts from the aws-awesome-distributed repo:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token builtin class-name" style="color:rgb(189, 147, 249)">cd</span><span class="token plain"> ~/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">git</span><span class="token plain"> clone https://github.com/aws-samples/awsome-distributed-training</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">mkdir</span><span class="token plain"> ~/peft_ft </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token builtin class-name" style="color:rgb(189, 147, 249)">cd</span><span class="token plain"> ~/peft_ft</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">cp</span><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">-r</span><span class="token plain"> ~/awsome-distributed-training/3.test_cases/pytorch/optimum-neuron/llama3/slurm/fine-tuning/submit_jobs </span><span class="token builtin class-name" style="color:rgb(189, 147, 249)">.</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-setup-python-environment">Step 2: Setup Python Environment<a href="#step-2-setup-python-environment" class="hash-link" aria-label="Direct link to Step 2: Setup Python Environment" title="Direct link to Step 2: Setup Python Environment">‚Äã</a></h3>
<p>Setup a virtual python environment and install your training dependencies. Make sure this repo is stored on the shared FSx volume of your cluster so all nodes have access to it.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sbatch submit_jobs/0.create_env.sh</span><br></span></code></pre></div></div>
<p>View the logs created by the scripts in this lab by running this command below. You can update it for the step you are currently running:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token function" style="color:rgb(80, 250, 123)">tail</span><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">-f</span><span class="token plain"> logs/0.create_env.out </span><br></span></code></pre></div></div>
<p>Before proceeding to the next step throughout this lab, check if the current job has finished by running:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">squeue</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-3-download-the-model">Step 3: Download the model<a href="#step-3-download-the-model" class="hash-link" aria-label="Direct link to Step 3: Download the model" title="Direct link to Step 3: Download the model">‚Äã</a></h3>
<p>Next, you will download the model to your FSx file volume. Begin by logging into Hugging Face using your access token mentioned in the prerequisite steps. With your access token set, you should now be able to download the model.</p>
<p>First modify the <code>submit_jobs/1.download_model.sh</code> script to include the Hugging Face access token before running it:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token builtin class-name" style="color:rgb(189, 147, 249)">export</span><span class="token plain"> </span><span class="token assign-left variable" style="color:rgb(189, 147, 249);font-style:italic">HF_TOKEN</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;&lt;Your Hugging Face Token&gt;&quot;</span><br></span></code></pre></div></div>
<p>Then trigger the script to download the Llama3 model.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sbatch submit_jobs/1.download_model.sh</span><br></span></code></pre></div></div>
<p>Now that your SageMaker HyperPod cluster is deployed and your environment is setup up, you can start preparing to execute your fine tuning job.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="training">Training<a href="#training" class="hash-link" aria-label="Direct link to Training" title="Direct link to Training">‚Äã</a></h2>
<p><img decoding="async" loading="lazy" alt="Tranium" src="/assets/images/trn1-71450bb2a8e0922f022c3299b207551f.png" width="768" height="386" class="img_ev3q"></p>
<p>In this section, you will begin training your Llama 3 model on a Trainium <code>trn1.32xlarge</code> instance.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-compile-the-model">Step 1: Compile the model<a href="#step-1-compile-the-model" class="hash-link" aria-label="Direct link to Step 1: Compile the model" title="Direct link to Step 1: Compile the model">‚Äã</a></h3>
<p>Before you begin training on Trainium with Neuron, you will need to pre-compile your model with the <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/training/pytorch-neuron-parallel-compile.html" target="_blank" rel="noopener noreferrer">neuron_parallel_compile CLI</a> which reduces the compilatin time during execution. This will trace through the model‚Äôs training code and apply optimizations to improve performance.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sbatch submit_jobs/2.compile_model.sh</span><br></span></code></pre></div></div>
<p>The compilation process will generate NEFF (Neuron Executable File Format) files that will speed up your model‚Äôs fine tuning job.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-fine-tuning">Step 2: Fine Tuning<a href="#step-2-fine-tuning" class="hash-link" aria-label="Direct link to Step 2: Fine Tuning" title="Direct link to Step 2: Fine Tuning">‚Äã</a></h3>
<p>With your model compiled, you can now begin fine tuning your Llama 3 model.</p>
<p>For the purposes of this workshop, we will use the <a href="https://huggingface.co/datasets/databricks/databricks-dolly-15k" target="_blank" rel="noopener noreferrer">dolly 15k dataset</a>. As part of the training process, the script below will download the dataset and format it in a way that the model expects. Each data point will contain an <strong>instruction</strong> that guides the model‚Äôs task, optional <strong>context</strong> that provides background information, and <strong>response</strong> that represent the desired output.</p>
<p>Now submit the fine tuning job:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sbatch submit_jobs/3.finetune.sh</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-3-model-weight-consolidation">Step 3: Model Weight Consolidation<a href="#step-3-model-weight-consolidation" class="hash-link" aria-label="Direct link to Step 3: Model Weight Consolidation" title="Direct link to Step 3: Model Weight Consolidation">‚Äã</a></h3>
<p>After training has completed, you will have a new directory for your model checkpoints. This directory will contain the model checkpoint shards from each neuron device that were generated during training. Use the model consolidation script to combine the shards into a single <code>model.safetensors</code> file.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sbatch submit_jobs/4.model_consolidation.sh</span><br></span></code></pre></div></div>
<p>The <code>model.safetensors</code> file will contain the LoRA weights of your model that were updated during training.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-4-merge-lora-weights">Step 4: Merge LoRA Weights<a href="#step-4-merge-lora-weights" class="hash-link" aria-label="Direct link to Step 4: Merge LoRA Weights" title="Direct link to Step 4: Merge LoRA Weights">‚Äã</a></h3>
<p>After consolidating the model shards, merge the LoRA adapter weights back to your base Llama 3 model:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sbatch submit_jobs/5.merge_lora_weights.sh</span><br></span></code></pre></div></div>
<p>Your final fine tuned model weights will be saved to the final_model_path directory. You can find or update the path in the script <code>submit_jobs/5.merge_lora_weights.sh</code> using the argument <code>--final_model_path</code>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-5-validate-your-trained-model">Step 5: Validate your trained model<a href="#step-5-validate-your-trained-model" class="hash-link" aria-label="Direct link to Step 5: Validate your trained model" title="Direct link to Step 5: Validate your trained model">‚Äã</a></h3>
<p>Now that your model is fine tuned, see how its generations differ from the base model for the dolly-15k dataset.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sbatch submit_jobs/6.inference.sh</span><br></span></code></pre></div></div>
<p>This will generate a prediction for the question ‚ÄúWho are you?‚Äù, comparing the response of the base model to the fine tuned model. It will also pass a system prompt to the model to always respond like a pirate.</p>
<p>Before fine tuning:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &#x27;role&#x27;: &#x27;assistant&#x27;, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &#x27;content&#x27;: &quot;Arrrr, me hearty! Me name be Captain Chat, the scurviest pirate chatbot to </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ever sail the Seven Seas! Me be here to regale ye with tales o&#x27; adventure, answer yer </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    questions, and swab the decks o&#x27; yer doubts! So hoist the colors, me matey, and let&#x27;s </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    set sail fer a swashbucklin&#x27; good time!&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre></div></div>
<p>After fine tuning:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &#x27;role&#x27;: &#x27;assistant&#x27;, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &#x27;content&#x27;: &quot;Arrr, shiver me timbers! Me be Captain Chat, the scurviest pirate chatbot to ever sail the Seven Seas! Me been programmin&#x27; me brain </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    with the finest pirate lingo and booty-ful banter to make ye feel like ye just stumbled</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    upon a chest overflowin&#x27; with golden doubloons! So hoist the colors, me hearty, and </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    let&#x27;s set sail fer a swashbucklin&#x27; good time!&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> } </span><br></span></code></pre></div></div>
<p>And that&#x27;s it! You&#x27;ve successfully fine tuned a Llama 3 model on Amazon SageMaker HyperPod using PEFT with Neuron.</p>
<hr></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/category/fine-tuning-1"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Fine Tuning</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/slurm-blueprints/fine-tuning/peft/qlora"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">QLoRA</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#prerequisites" class="table-of-contents__link toc-highlight">Prerequisites</a></li><li><a href="#setup" class="table-of-contents__link toc-highlight">Setup</a><ul><li><a href="#step-1-download-training-scripts" class="table-of-contents__link toc-highlight">Step 1: Download training scripts</a></li><li><a href="#step-2-setup-python-environment" class="table-of-contents__link toc-highlight">Step 2: Setup Python Environment</a></li><li><a href="#step-3-download-the-model" class="table-of-contents__link toc-highlight">Step 3: Download the model</a></li></ul></li><li><a href="#training" class="table-of-contents__link toc-highlight">Training</a><ul><li><a href="#step-1-compile-the-model" class="table-of-contents__link toc-highlight">Step 1: Compile the model</a></li><li><a href="#step-2-fine-tuning" class="table-of-contents__link toc-highlight">Step 2: Fine Tuning</a></li><li><a href="#step-3-model-weight-consolidation" class="table-of-contents__link toc-highlight">Step 3: Model Weight Consolidation</a></li><li><a href="#step-4-merge-lora-weights" class="table-of-contents__link toc-highlight">Step 4: Merge LoRA Weights</a></li><li><a href="#step-5-validate-your-trained-model" class="table-of-contents__link toc-highlight">Step 5: Validate your trained model</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/getting-started/orchestrated-by-eks/initial-cluster-setup">Orchestrated by EKS</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup">Orchestrated by SLURM</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Sites with Sagemaker AI content</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/aws-samples/awsome-distributed-training" target="_blank" rel="noopener noreferrer" class="footer__link-item">Awsome Distributed Training<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://github.com/aws/sagemaker-hyperpod-recipes" target="_blank" rel="noopener noreferrer" class="footer__link-item">SageMaker HyperPod Recipes<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Other AWS related sites</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://aws.training" target="_blank" rel="noopener noreferrer" class="footer__link-item">AWS Training &amp; Certification<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://aws.amazon.com/sagemaker/ai/hyperpod" target="_blank" rel="noopener noreferrer" class="footer__link-item">Amazon SageMaker HyperPod<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://repost.aws" target="_blank" rel="noopener noreferrer" class="footer__link-item">AWS re:Post<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2025 AWS WWSO ML Frameworks team. Built with ‚ù§Ô∏è at AWS.</div></div></div></footer></div>
</body>
</html>