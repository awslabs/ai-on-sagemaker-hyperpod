<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-slurm-blueprints/training/megatron-lm/megatron-lm-readme" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">NVIDIA Megatron-LM | AI on Sagemaker Hyperpod</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://awslabs.github.io/img/header_background.png"><meta data-rh="true" name="twitter:image" content="https://awslabs.github.io/img/header_background.png"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="machine learning, generative ai, genai, sagemaker, hyperpod, sagemaker hyperpod,         model training, model inference, nemo framework, pytorch, pytorch framework"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="NVIDIA Megatron-LM | AI on Sagemaker Hyperpod"><meta data-rh="true" name="description" content="MegatronLM is a framework from Nvidia that can be used to train LLMs. We recommend that you read papers on the framework to know the different knobs you can tune and in particular these articles:"><meta data-rh="true" property="og:description" content="MegatronLM is a framework from Nvidia that can be used to train LLMs. We recommend that you read papers on the framework to know the different knobs you can tune and in particular these articles:"><link data-rh="true" rel="icon" href="/img/Amazon-Sagemaker-Icon.jpg"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme" hreflang="en"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"SLURM Blueprints","item":"https://awslabs.github.io/docs/category/slurm-blueprints"},{"@type":"ListItem","position":2,"name":"Training","item":"https://awslabs.github.io/docs/category/training-1"},{"@type":"ListItem","position":3,"name":"NVIDIA Megatron-LM","item":"https://awslabs.github.io/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme"}]}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="AI on Sagemaker Hyperpod RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="AI on Sagemaker Hyperpod Atom Feed"><link rel="stylesheet" href="/assets/css/styles.acb24679.css">
<script src="/assets/js/runtime~main.9ef3172e.js" defer="defer"></script>
<script src="/assets/js/main.0e3a5677.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="dark";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/Amazon-Sagemaker-Icon.jpg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/Amazon-Sagemaker-Icon.jpg" alt="AI on Sagemaker Hyperpod" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/Amazon-Sagemaker-Icon.jpg" alt="AI on Sagemaker Hyperpod" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI on Sagemaker Hyperpod</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Orchestrated by EKS</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/getting-started/orchestrated-by-eks/initial-cluster-setup">Initial cluster setup</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/trainium/aws-trainium">AWS Trainium</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/ddp/distributed-data-parallel">Distributed Data Parallel</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/fsdp/fully-sharded-data-parallel">Fully Sharded Data Parallel</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme">NVIDIA Megatron LM</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/ray-train/ray-train-readme">Ray Train</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Orchestrated by SLURM</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup">Initial cluster setup</a></li><li><a class="dropdown__link" href="/docs/slurm-blueprints/training/trainium/Llama3-70B">AWS Trainium</a></li><li><a class="dropdown__link" href="/docs/slurm-blueprints/training/ddp/distributed-data-parallel">Distributed Data Parallel</a></li><li><a class="dropdown__link" href="/docs/slurm-blueprints/training/fsdp/fully-sharded-data-parallel">Fully Sharded Data Parallel</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme">NVIDIA Megatron LM</a></li></ul></div><a class="navbar__item navbar__link" href="/resources">Useful links</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/awslabs/ai-on-sagemaker-hyperpod" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/Introduction">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/getting-started">Getting Started</a><button aria-label="Expand sidebar category &#x27;Getting Started&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/eks-blueprints">EKS Blueprints</a><button aria-label="Expand sidebar category &#x27;EKS Blueprints&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/docs/category/slurm-blueprints">SLURM Blueprints</a><button aria-label="Collapse sidebar category &#x27;SLURM Blueprints&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/category/training-1">Training</a><button aria-label="Collapse sidebar category &#x27;Training&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/slurm-blueprints/training/ddp/distributed-data-parallel">DDP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/slurm-blueprints/training/fsdp/fully-sharded-data-parallel">FSDP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme">Megatron-LM</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme">NVIDIA Megatron-LM</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/slurm-blueprints/training/trainium/Llama3-70B">Trainium</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/category/fine-tuning-1">Fine Tuning</a><button aria-label="Expand sidebar category &#x27;Fine Tuning&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/sagemaker-hyperpod-recipes">Sagemaker Hyperpod Recipes</a><button aria-label="Expand sidebar category &#x27;Sagemaker Hyperpod Recipes&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/add-ons">Add-Ons</a><button aria-label="Expand sidebar category &#x27;Add-Ons&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/validation-and-testing">Validation and Testing</a><button aria-label="Expand sidebar category &#x27;Validation and Testing&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/infrastructure-as-a-code">Infrastructure as a Code</a><button aria-label="Expand sidebar category &#x27;Infrastructure as a Code&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/category/slurm-blueprints"><span>SLURM Blueprints</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/category/training-1"><span>Training</span></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Megatron-LM</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">NVIDIA Megatron-LM</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>NVIDIA Megatron-LM</h1></header>
<p><a href="https://github.com/NVIDIA/Megatron-LM" target="_blank" rel="noopener noreferrer">MegatronLM</a> is a framework from Nvidia that can be used to train LLMs. We recommend that you read papers on the framework to know the different knobs you can tune and in particular these articles:</p>
<ul>
<li><a href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noopener noreferrer">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></li>
<li><a href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noopener noreferrer">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</a></li>
</ul>
<p>To run a test case you will go through a series of steps described below:</p>
<ol>
<li>Build the data preprocessing container.</li>
<li>Pre-process the data using a tokenizer and the preprocessing container.</li>
<li>Build the container for distributed training</li>
<li>Train!</li>
</ol>
<p>We describe the steps below for Slurm users running on a Nvidia GPU.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="preparation">Preparation<a href="#preparation" class="hash-link" aria-label="Direct link to Preparation" title="Direct link to Preparation">​</a></h2>
<p>This guide assumes that you&#x27;ve <a href="/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup">built a SageMaker HyperPod Slurm cluster</a> with GPU instances i.e. g5/p4d/p5 instance types. Please make sure you have the following before getting started:</p>
<ul>
<li>A Slurm cluster on AWS using Nvidia GPU&#x27;s i.e. <code>g5</code>, <code>p4d</code> or <code>p5</code>.</li>
<li>Docker, <a href="https://github.com/NVIDIA/pyxis" target="_blank" rel="noopener noreferrer">Pyxis</a> and <a href="https://github.com/NVIDIA/enroot" target="_blank" rel="noopener noreferrer">Enroot</a> installed. This is installed by default in the <a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/1.architectures/5.sagemaker-hyperpod/LifecycleScripts/base-config/utils" target="_blank" rel="noopener noreferrer">lifecycle scripts</a></li>
<li>An FSx for Lustre filesystem mounted on <code>/fsx</code>.</li>
</ul>
<p>If you completed <a href="/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup">Cluster Setup</a> using a Nvidia GPU config these steps are complete.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-preprocessing">Data Preprocessing<a href="#data-preprocessing" class="hash-link" aria-label="Direct link to Data Preprocessing" title="Direct link to Data Preprocessing">​</a></h2>
<p>Before running training jobs you need to retrieve input data and pre-process it. This section of the guide you will retrieve a container then you convert it into a squash file via <a href="https://github.com/NVIDIA/enroot" target="_blank" rel="noopener noreferrer">Enroot</a>, you will then retrieve input data and tokenize it using the GPT2 vocabulary.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="steps">Steps<a href="#steps" class="hash-link" aria-label="Direct link to Steps" title="Direct link to Steps">​</a></h3>
<ol>
<li>
<p>First SSH into one of the compute nodes. <strong>Note:</strong> you can build containers on the head node but you&#x27;re limited to only 100GB of storage so we recommend building them on a compute node.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># ssh into a compute node</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">salloc -N 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ssh $(srun hostname)</span><br></span></code></pre></div></div>
</li>
</ol>
<p>All next steps will be executed on the compute node.</p>
<ol start="2">
<li>
<p>Next we&#x27;ll clone the <a href="https://github.com/aws-samples/awsome-distributed-training/tree/main" target="_blank" rel="noopener noreferrer">Github repo</a> and cd into the right directory:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd ~</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">git clone https://github.com/aws-samples/awsome-distributed-training.git</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd awsome-distributed-training/3.test_cases/megatron/megatron-lm</span><br></span></code></pre></div></div>
</li>
<li>
<p>Next we&#x27;ll set an environment variable to point to our shared <code>/fsx/ubuntu</code> filesystem. This is used in the submission scripts later.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">export DATA_PATH=/fsx/ubuntu # FSx for Lustre shared file-system</span><br></span></code></pre></div></div>
</li>
<li>
<p>Build the container image with the command below</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">DOCKER_BUILDKIT=1 docker build -t megatron-training -f aws-megatron-lm.Dockerfile .</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Important</div><div class="admonitionContent_BuS1"><p>If you see the following error <code>ERROR: permission denied while trying to connect to the Docker daemon socket at...</code> when trying to run <code>docker</code>, you&#x27;ll need to add the user to the <code>docker</code> group by running:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo usermod -aG docker ${USER}</span><br></span></code></pre></div></div><p>Then log out with <code>exit</code> and log back in.</p></div></div>
</li>
<li>
<p>Once the image is built, you can check if it is present with <code>docker images</code>. You should see an output similar to this one:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[ubuntu@ip-10-0-10-78 ~]$ docker images</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">REPOSITORY            TAG       IMAGE ID       CREATED         SIZE</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">megatron-training   latest    de38623b2f85   2 minutes ago   20.7GB</span><br></span></code></pre></div></div>
</li>
<li>
<p>Create the squash file with the command below.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo enroot import -o megatron-training.sqsh  dockerd://megatron-training:latest</span><br></span></code></pre></div></div>
<p>The file will be stored in the current directory (if left as default). The output should look as below.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[ec2-user@ip-10-0-10-78 ~]$ enroot import -o megatron-training.sqsh  dockerd://megatron-training:latest</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">[INFO] Fetching image</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">e19aa13505c1710876982dc440226dc479da5177dc4770452cc79bedc8b5b41d</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">[INFO] Extracting image content...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">[INFO] Creating squashfs filesystem...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Parallel mksquashfs: Using 32 processors</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Creating 4.0 filesystem on /fsx/.../megatron-training.sqsh, block size 131072.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">[==========================================================/] 299550/299550 100%</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Exportable Squashfs 4.0 filesystem, gzip compressed, data block size 131072</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   uncompressed data, uncompressed metadata, uncompressed fragments, uncompressed xattrs</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   duplicates are not removed</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...</span><br></span></code></pre></div></div>
</li>
<li>
<p>Create a script with the code below to retrieve the input datasets and vocabulary. Let&#x27;s call it retrieve_vocab.sh.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">cat &lt;&lt;EOF&gt; retrieve_vocab.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#!/bin/bash</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">mkdir -p gpt2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd gpt2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https://huggingface.co/bigscience/misc-test-data/resolve/main/stas/oscar-1GB.jsonl.xz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">xz -d oscar-1GB.jsonl.xz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd ..</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">EOF</span><br></span></code></pre></div></div>
<p>Run chmod to make the script executable, then execute it. Remember this has to be executed within the directory that holds the data processing and training code.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">chmod a+x retrieve_vocab.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">./retrieve_vocab.sh  </span><br></span></code></pre></div></div>
</li>
<li>
<p>Now submit the file <code>1.data-preprocessing.sbatch</code> using the command below:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sbatch slurm/gpt3/1.data-preprocessing.sbatch</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Important</div><div class="admonitionContent_BuS1"><p>If you see an error <code>[ERROR] Command not found: nvidia-container-cli, see https://github.com/NVIDIA/libnvidia-container</code>, you need to install <code>nvidia-container-cli</code>. To do that run the following on each compute node:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">sed &#x27;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#x27; | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo apt-get install -y nvidia-container-toolkit</span><br></span></code></pre></div></div></div></div>
</li>
<li>
<p>You will see a new file in your current working directory called <code>slurm-XY.out</code> where <code>XY</code> is a number. This is your output file and will capture the <code>STDOUT</code> and <code>STDERR</code> from your job. You can check how it progresses via the command <code>tail -f slurm-XY.out</code> but with the relevant filename. The file content will be similar to the below:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">0: Opening /fsx/ubuntu/oscar-1GB.jsonl</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">0: Time to startup: 0.9956498146057129</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">0: Processed 1000 documents (101.28050670002645 docs/s, 1.258563987556778 MB/s).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">0: Processed 2000 documents (188.07992853480727 docs/s, 2.3571624257619614 MB/s).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">0: Processed 78000 documents (1293.9967304914383 docs/s, 16.67556064420713 MB/s).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">0: Processed 79000 documents (1298.6715286585202 docs/s, 16.763634765830606 MB/s).</span><br></span></code></pre></div></div>
</li>
<li>
<p>When you confirmed successful run of the preprocessing job, you can go back to the head node.</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">exit</span><br></span></code></pre></div></div>
</li>
</ol>
<p>Voilà! You have executed the preprocessing job. You will go through the steps to run your training job.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="training">Training<a href="#training" class="hash-link" aria-label="Direct link to Training" title="Direct link to Training">​</a></h2>
<p>Now that the data is preprocessed, we will pre-train a GPT-3 model Megatron-LM.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="steps-1">Steps<a href="#steps-1" class="hash-link" aria-label="Direct link to Steps" title="Direct link to Steps">​</a></h3>
<ol>
<li>
<p>First let&#x27;s adjust the number of GPU&#x27;s requested to fit the size of our cluster. Edit the file <code>2.distributed-training.sbatch</code> and adjust the line <code>#SBATCH --nodes=24</code> to the number of instances in your cluster. For example if I had <code>8 x p5.48xlarge</code> instances, I would put:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">#!/bin/bash</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># SPDX-License-Identifier: Apache-2.0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#SBATCH --nodes=8 # number of nodes to use, 8 p4d(e) = 64 A100 GPUs</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#SBATCH --job-name=megatron_gpt # name of your job</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#SBATCH --exclusive # job has exclusive use of the resource, no sharing</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#SBATCH --wait-all-nodes=1</span><br></span></code></pre></div></div>
</li>
<li>
<p>Next submit a training job:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sbatch 2.distributed-training.sbatch</span><br></span></code></pre></div></div>
</li>
<li>
<p>The training starts running and should produce an output similar to below if successful.</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">1:  iteration       25/73242187 | consumed samples:           50 | elapsed time per iteration (ms): 87.0 | learning rate: 1.638E-08 | global batch size:     2 | lm loss: 1.086954E+01 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">1:  iteration       26/73242187 | consumed samples:           52 | elapsed time per iteration (ms): 86.5 | learning rate: 1.704E-08 | global batch size:     2 | lm loss: 1.086217E+01 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">1:  iteration       27/73242187 | consumed samples:           54 | elapsed time per iteration (ms): 88.4 | learning rate: 1.769E-08 | global batch size:     2 | lm loss: 1.087129E+01 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |</span><br></span></code></pre></div></div>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="monitoring">Monitoring<a href="#monitoring" class="hash-link" aria-label="Direct link to Monitoring" title="Direct link to Monitoring">​</a></h2>
<p>Now that the job is running, we can monitor it in two ways, we can tail the log file to see how the training is progressing:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Control-C to stop tailing</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tail -f slurm-2.log</span><br></span></code></pre></div></div>
<p>We can also ensure it&#x27;s utilizing the GPU&#x27;s appropriately by SSH-ing into the compute node.</p>
<p>Grab the hostname by running <code>sinfo</code> and seeing which node it&#x27;s running on:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sinfo</span><br></span></code></pre></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">dev*         up   infinite      1  alloc ip-10-1-90-87</span><br></span></code></pre></div></div>
<p>Then ssh into that instance using the hostname from <code>sinfo</code>:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">ssh ip-10-1-90-87</span><br></span></code></pre></div></div>
<p>Once there we can monitor the accelerator usage by running <code>nvidia-smi</code>:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">nvidia-smi</span><br></span></code></pre></div></div>
<p>You&#x27;ll see very little usage of the GPU&#x27;s for the first few minutes as it sets up the case, then you&#x27;ll see constant usage after that:</p>
<p><img decoding="async" loading="lazy" alt="Nvidia-smi" src="/assets/images/nvidia-smi-4a0ae3baef20b60f0b5a85be9b2642e4.png" width="1618" height="1340" class="img_ev3q"></p></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/slurm-blueprints/training/fsdp/fully-sharded-data-parallel"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Fully Sharded Data Parallel</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/slurm-blueprints/training/trainium/Llama3-70B"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Llama-3 70B (trn1.32xlarge) using NxD</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#preparation" class="table-of-contents__link toc-highlight">Preparation</a></li><li><a href="#data-preprocessing" class="table-of-contents__link toc-highlight">Data Preprocessing</a><ul><li><a href="#steps" class="table-of-contents__link toc-highlight">Steps</a></li></ul></li><li><a href="#training" class="table-of-contents__link toc-highlight">Training</a><ul><li><a href="#steps-1" class="table-of-contents__link toc-highlight">Steps</a></li></ul></li><li><a href="#monitoring" class="table-of-contents__link toc-highlight">Monitoring</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/getting-started/orchestrated-by-eks/initial-cluster-setup">Orchestrated by EKS</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup">Orchestrated by SLURM</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Sites with Sagemaker AI content</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/aws-samples/awsome-distributed-training" target="_blank" rel="noopener noreferrer" class="footer__link-item">Awsome Distributed Training<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://github.com/aws/sagemaker-hyperpod-recipes" target="_blank" rel="noopener noreferrer" class="footer__link-item">Sagemaker Hyperpod Recipes<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Other AWS related sites</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://aws.training" target="_blank" rel="noopener noreferrer" class="footer__link-item">AWS Training &amp; Certification<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://aws.amazon.com/sagemaker/ai/hyperpod" target="_blank" rel="noopener noreferrer" class="footer__link-item">Amazon Sagemaker Hyperpod<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://repost.aws" target="_blank" rel="noopener noreferrer" class="footer__link-item">AWS re:Post<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 AWS WWSO ML Frameworks team. Built with ❤️ at AWS.</div></div></div></footer></div>
</body>
</html>