<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-slurm-blueprints/training/trainium/Llama3-70B/training" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Running Continual Pre-training with NeuronX Distributed | AI on SageMaker HyperPod</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://awslabs.github.io/img/header_background.png"><meta data-rh="true" name="twitter:image" content="https://awslabs.github.io/img/header_background.png"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/docs/slurm-blueprints/training/trainium/Llama3-70B/training"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="machine learning, generative ai, genai, sagemaker, hyperpod, SageMaker HyperPod,         model training, model inference, nemo framework, pytorch, pytorch framework"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Running Continual Pre-training with NeuronX Distributed | AI on SageMaker HyperPod"><meta data-rh="true" name="description" content="Okay, now that we&#x27;ve pre-processed the data and the model checkpoints, we are ready to submit a continual pre-training job. We have two sub-directories under /fsx/ubuntu/llama:"><meta data-rh="true" property="og:description" content="Okay, now that we&#x27;ve pre-processed the data and the model checkpoints, we are ready to submit a continual pre-training job. We have two sub-directories under /fsx/ubuntu/llama:"><link data-rh="true" rel="icon" href="/img/Amazon-Sagemaker-Icon.jpg"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/docs/slurm-blueprints/training/trainium/Llama3-70B/training"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/docs/slurm-blueprints/training/trainium/Llama3-70B/training" hreflang="en"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/docs/slurm-blueprints/training/trainium/Llama3-70B/training" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"SLURM Blueprints","item":"https://awslabs.github.io/docs/category/slurm-blueprints"},{"@type":"ListItem","position":2,"name":"Training","item":"https://awslabs.github.io/docs/category/training-1"},{"@type":"ListItem","position":3,"name":"Llama-3 70B (trn1.32xlarge) using NxD","item":"https://awslabs.github.io/docs/slurm-blueprints/training/trainium/Llama3-70B/"},{"@type":"ListItem","position":4,"name":"Running Continual Pre-training with NeuronX Distributed","item":"https://awslabs.github.io/docs/slurm-blueprints/training/trainium/Llama3-70B/training"}]}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="AI on SageMaker HyperPod RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="AI on SageMaker HyperPod Atom Feed"><link rel="stylesheet" href="/assets/css/styles.3e9092dc.css">
<script src="/assets/js/runtime~main.e05f9bb5.js" defer="defer"></script>
<script src="/assets/js/main.ad341d2a.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="dark";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/Amazon-Sagemaker-Icon.jpg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/Amazon-Sagemaker-Icon.jpg" alt="AI on SageMaker HyperPod" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/Amazon-Sagemaker-Icon.jpg" alt="AI on SageMaker HyperPod" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI on SageMaker HyperPod</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Orchestrated by EKS</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/getting-started/orchestrated-by-eks/initial-cluster-setup">Initial cluster setup</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/trainium/aws-trainium">AWS Trainium</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/ddp/distributed-data-parallel">Distributed Data Parallel</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/fsdp/fully-sharded-data-parallel">Fully Sharded Data Parallel</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme">NVIDIA Megatron LM</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/ray-train/ray-train-readme">Ray Train</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Orchestrated by SLURM</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup">Initial cluster setup</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/docs/slurm-blueprints/training/trainium/Llama3-70B">AWS Trainium</a></li><li><a class="dropdown__link" href="/docs/slurm-blueprints/training/ddp/distributed-data-parallel">Distributed Data Parallel</a></li><li><a class="dropdown__link" href="/docs/slurm-blueprints/training/fsdp/fully-sharded-data-parallel">Fully Sharded Data Parallel</a></li><li><a class="dropdown__link" href="/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme">NVIDIA Megatron LM</a></li></ul></div><a class="navbar__item navbar__link" href="/resources">Useful links</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/awslabs/ai-on-sagemaker-hyperpod" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/Introduction">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/getting-started">Getting Started</a><button aria-label="Expand sidebar category &#x27;Getting Started&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/eks-blueprints">EKS Blueprints</a><button aria-label="Expand sidebar category &#x27;EKS Blueprints&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/docs/category/slurm-blueprints">SLURM Blueprints</a><button aria-label="Collapse sidebar category &#x27;SLURM Blueprints&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/category/training-1">Training</a><button aria-label="Collapse sidebar category &#x27;Training&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/slurm-blueprints/training/ddp/distributed-data-parallel">DDP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/slurm-blueprints/training/fsdp/fully-sharded-data-parallel">FSDP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme">Megatron-LM</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/docs/slurm-blueprints/training/trainium/Llama3-70B">Trainium</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/slurm-blueprints/training/trainium/Llama3-70B">Llama-3 70B (trn1.32xlarge) using NxD</a><button aria-label="Collapse sidebar category &#x27;Llama-3 70B (trn1.32xlarge) using NxD&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/slurm-blueprints/training/trainium/Llama3-70B/prep">Setting up the software stack</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/slurm-blueprints/training/trainium/Llama3-70B/download-model">Downloading the Llama3-70b model</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/slurm-blueprints/training/trainium/Llama3-70B/download-dataset">Downloading the Wiki-corpus datasets</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/slurm-blueprints/training/trainium/Llama3-70B/training">Running Continual Pre-training with NeuronX Distributed</a></li></ul></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/category/fine-tuning-1">Fine Tuning</a><button aria-label="Expand sidebar category &#x27;Fine Tuning&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/sagemaker-hyperpod-recipes">Sagemaker Hyperpod Recipes</a><button aria-label="Expand sidebar category &#x27;Sagemaker Hyperpod Recipes&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/add-ons">Add-Ons</a><button aria-label="Expand sidebar category &#x27;Add-Ons&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/validation-and-testing">Validation and Testing</a><button aria-label="Expand sidebar category &#x27;Validation and Testing&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/infrastructure-as-a-code">Infrastructure as a Code</a><button aria-label="Expand sidebar category &#x27;Infrastructure as a Code&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/tips">Tips</a><button aria-label="Expand sidebar category &#x27;Tips&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/category/slurm-blueprints"><span>SLURM Blueprints</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/category/training-1"><span>Training</span></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Trainium</span></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/slurm-blueprints/training/trainium/Llama3-70B"><span>Llama-3 70B (trn1.32xlarge) using NxD</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Running Continual Pre-training with NeuronX Distributed</span></li></ul></nav><div class="theme-doc-markdown markdown"><header><h1>Running Continual Pre-training with NeuronX Distributed</h1></header><p>Okay, now that we&#x27;ve pre-processed the data and the model checkpoints, we are ready to submit a continual pre-training job. We have two sub-directories under <code>/fsx/ubuntu/llama</code>:</p>
<ul>
<li><code>tp_pp_llama_hf_pretrain</code>: <a href="https://github.com/aws-neuron/neuronx-distributed/tree/main/examples/training/llama/tp_pp_llama_hf_pretrain" target="_blank" rel="noopener noreferrer">link</a></li>
<li><code>tp_zero1_llama_hf_pretrain</code>: <a href="https://github.com/aws-neuron/neuronx-distributed/tree/main/examples/training/llama/tp_zero1_llama_hf_pretrain" target="_blank" rel="noopener noreferrer">link</a></li>
</ul>
<p>These are two parallelism strategies <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/tutorials/training/zero1_gpt2.html" target="_blank" rel="noopener noreferrer">ZeRO-1</a> and Model Parallelism. For this test case, let&#x27;s use the model parallelism strategy. Copy the relevant files:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token function" style="color:rgb(80, 250, 123)">mv</span><span class="token plain"> tp_pp_llama_hf_pretrain/* </span><span class="token builtin class-name" style="color:rgb(189, 147, 249)">.</span><br></span></code></pre></div></div>
<p>Running <code>ls /fsx/ubuntu/llama</code>, you should see template scripts per model inside the directory:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">13B_config_llama2  __pycache__               convert_checkpoints.py  llama3-70B-save.py  lr.py                  requirements_ptl.txt     run_llama2_70B_tp_pp.sh  save-llama3-70B-model.py  tp_pp_llama_hf_pretrain</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">70B_config_llama2  activation_checkpoint.py  get_dataset.py          logger.py           modeling_llama_nxd.py  results.json             run_llama3_70B_tp_pp.sh  tokenizer.json            tp_zero1_llama_hf_pretrain</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">70B_config_llama3  checkpoint_converter.py   lightning               logs                requirements.txt       run_llama2_13B_tp_pp.sh  run_llama_nxd.py         tokenizer_config.json     training_utils.py</span><br></span></code></pre></div></div>
<p>For this test case, we only need to modify a few lines of the <code>run_llama2_70B_tp_pp.sh</code> script.</p>
<ol>
<li>Firstly, let&#x27;s modify the path to the shared file system (default is <code>/shared</code>, we want to use <code>/fsx/ubuntu</code>), which we can do in place:</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token function" style="color:rgb(80, 250, 123)">sed</span><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">-i</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;s/\/shared/\/fsx\/ubuntu/g&#x27;</span><span class="token plain"> run_llama3_70B_tp_pp.sh</span><br></span></code></pre></div></div>
<ol start="2">
<li>Let&#x27;s modify the <code>torchrun</code> args.</li>
</ol>
<ul>
<li><strong>Enable Pretrained Weights</strong>: Neuron Distributed default initiates training without using pretrained weights. To enable the use of pretrained weights, set the value of the <code>--pretrained_weight</code> argument to 1.</li>
<li><strong>Change Checkpoint Frequency</strong>: Modify the value of the <code>--checkpoint_freq</code> argument to <code>m</code> (an integer) to save checkpoints every <code>m</code> steps.</li>
<li><strong>Manage Checkpoint Storage</strong>: The current version of Neuron Distributed generates checkpoints roughly 850 GB in size for the 70B model training. Saving all historical checkpoints can consume too much space. Modify the value of the <code>--num_kept_checkpoint</code> argument to <code>n</code> (an integer) to keep only the latest <code>n</code> checkpoints.</li>
<li><strong>Ensure Latest Checkpoint Loading</strong>: To ensure the training process always starts from the latest checkpoint, set the value of the <code>--loading_step argument</code> to <code>latest_if_exists</code>. This is crucial in the event of hardware failure. HyperPod also provides an <code>auto-resume</code> functionality. If a job fails due to hardware issues, HyperPod initiates node replacement and restarts the job using the same script. This script must load the latest checkpoints when training resumes. We will set <code>--auto-resume=1</code> in the <code>sbatch</code> file.</li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">torchrun </span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">$DISTRIBUTED_ARGS</span><span class="token plain"> run_llama_nxd.py </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(248, 248, 242)">..</span><span class="token plain">.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">--fuse_qkv</span><span class="token plain"> </span><span class="token number">1</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">--pretrained_weight</span><span class="token plain"> </span><span class="token number">1</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)"># Change value</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(248, 248, 242)">..</span><span class="token plain">.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">--checkpoint_freq</span><span class="token plain"> </span><span class="token number">5</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)"># change value</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">--num_kept_checkpoint</span><span class="token plain"> </span><span class="token number">2</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)"># Change value</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">--loading_step</span><span class="token plain"> latest_if_exists </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)"># Change value</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">--tb_dir</span><span class="token plain"> </span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">$tb_dir</span><span class="token plain"> </span><span class="token operator">|&amp;</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">tee</span><span class="token plain"> </span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">$LOG_PATH</span><span class="token plain">/log</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token builtin class-name" style="color:rgb(189, 147, 249)">exit</span><span class="token plain"> </span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">${</span><span class="token variable environment constant" style="color:rgb(189, 147, 249);font-style:italic">PIPESTATUS</span><span class="token variable punctuation" style="color:rgb(248, 248, 242);font-style:italic">[</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">0</span><span class="token variable punctuation" style="color:rgb(248, 248, 242);font-style:italic">]</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">}</span><span class="token plain">        </span><br></span></code></pre></div></div>
<p>Using this new <code>run_llama3_70B_tp_pp.sh</code> script, let&#x27;s first pre-compile the graphs (since Neuron is xla based) using the <code>neuron_parallel_compile</code>.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sbatch --job-name run_llama3_70B </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">       </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">--output</span><span class="token plain"> logs/run_llama3_70B.out </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">       </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">--exclusive</span><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">--nodes</span><span class="token plain"> </span><span class="token number">16</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">       --cpus-per-task </span><span class="token number">64</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">       </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">--wrap</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;srun neuron_parallel_compile bash </span><span class="token string variable" style="color:rgb(189, 147, 249);font-style:italic">$(</span><span class="token string variable builtin class-name" style="color:rgb(189, 147, 249);font-style:italic">pwd</span><span class="token string variable" style="color:rgb(189, 147, 249);font-style:italic">)</span><span class="token string" style="color:rgb(255, 121, 198)">/run_llama3_70B_tp_pp.sh&quot;</span><br></span></code></pre></div></div>
<p>This step takes ~25 min. Remember the idea is to pre-compile the model to build graphs, so that when you start your actual training (<code>torchrun your_model.py</code>), your model&#x27;s graphs are built and stored in a cache, so training will start a lot faster.</p>
<p>You know that the compilation step is complete when you see the following in <code>logs/run_llama3_70B.out</code></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">.........</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Compiler status PASS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2025-01-17 01:16:07.000934:  42821  INFO ||NEURON_PARALLEL_COMPILE||: worker 6 finished with num of tasks 1....</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2025-01-17 01:16:07.000970:  42821  INFO ||NEURON_CACHE||: Current remaining items are 0, locked are 2, failed are 0, done are 32, total is 34</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2025-01-17 01:16:07.000981:  32201  INFO ||NEURON_PARALLEL_COMPILE||: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;compilation_summary&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        &quot;true&quot;: 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;compilation_report&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        &quot;/fsx/ubuntu/cache_dir_neuron/neuronxcc-2.16.372.0+4a9b2326/MODULE_9993940051196728623+3cc9a3cb/model.hlo_module.pb&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;status&quot;: true,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;retry&quot;: 0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;compile_time&quot;: 9.770226240158081</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        &quot;/fsx/ubuntu/cache_dir_neuron/neuronxcc-2.16.372.0+4a9b2326/MODULE_7470829644182149778+3cc9a3cb/model.hlo_module.pb&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;status&quot;: true,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;retry&quot;: 0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;compile_time&quot;: 192.86727905273438</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;start_time&quot;: 1737076372.654127,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;compilation_time&quot;: 195.32730412483215</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2025-01-17 01:16:07.000981:  32201  INFO ||NEURON_PARALLEL_COMPILE||: Total graphs: 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2025-01-17 01:16:07.000981:  32201  INFO ||NEURON_PARALLEL_COMPILE||: Total successful compilations: 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2025-01-17 01:16:07.000981:  32201  INFO ||NEURON_PARALLEL_COMPILE||: Total failed compilations: 0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">..............</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Compiler status PASS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2025-01-17 01:16:20.000242:  48243  INFO ||NEURON_PARALLEL_COMPILE||: worker 0 finished with num of tasks 1....</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2025-01-17 01:16:20.000264:  48243  INFO ||NEURON_CACHE||: Current remaining items are 0, locked are 1, failed are 0, done are 33, total is 34</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Compiler status PASS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2025-01-17 01:16:38.000143:  48247  INFO ||NEURON_PARALLEL_COMPILE||: worker 4 finished with num of tasks 1....</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2025-01-17 01:16:38.000162:  48247  INFO ||NEURON_CACHE||: Current remaining items are 0, locked are 0, failed are 0, done are 34, total is 34</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2025-01-17 01:16:38.000177:  37654  INFO ||NEURON_PARALLEL_COMPILE||: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;compilation_summary&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        &quot;true&quot;: 8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;compilation_report&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        &quot;/fsx/ubuntu/cache_dir_neuron/neuronxcc-2.16.372.0+4a9b2326/MODULE_13896381258680072431+3cc9a3cb/model.hlo_module.pb&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;status&quot;: true,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;retry&quot;: 0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;compile_time&quot;: 207.77499103546143</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        &quot;/fsx/ubuntu/cache_dir_neuron/neuronxcc-2.16.372.0+4a9b2326/MODULE_15605214078085204741+3cc9a3cb/model.hlo_module.pb&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;status&quot;: true,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;retry&quot;: 0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;compile_time&quot;: 62.078277826309204</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        &quot;/fsx/ubuntu/cache_dir_neuron/neuronxcc-2.16.372.0+4a9b2326/MODULE_17180165851576277373+3cc9a3cb/model.hlo_module.pb&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;status&quot;: true,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;retry&quot;: 0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;compile_time&quot;: 13.69736123085022</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        &quot;/fsx/ubuntu/cache_dir_neuron/neuronxcc-2.16.372.0+4a9b2326/MODULE_17186493308924889042+3cc9a3cb/model.hlo_module.pb&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;status&quot;: true,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;retry&quot;: 0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;compile_time&quot;: 13.605631828308105</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        &quot;/fsx/ubuntu/cache_dir_neuron/neuronxcc-2.16.372.0+4a9b2326/MODULE_17656286827372360109+3cc9a3cb/model.hlo_module.pb&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;status&quot;: true,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;retry&quot;: 0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;compile_time&quot;: 224.9934914112091</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        &quot;/fsx/ubuntu/cache_dir_neuron/neuronxcc-2.16.372.0+4a9b2326/MODULE_15118541367256155893+3cc9a3cb/model.hlo_module.pb&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;status&quot;: true,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;retry&quot;: 0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;compile_time&quot;: 69.67782711982727</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        &quot;/fsx/ubuntu/cache_dir_neuron/neuronxcc-2.16.372.0+4a9b2326/MODULE_6234946551864249267+3cc9a3cb/model.hlo_module.pb&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;status&quot;: true,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;retry&quot;: 0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;compile_time&quot;: 73.62903022766113</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        &quot;/fsx/ubuntu/cache_dir_neuron/neuronxcc-2.16.372.0+4a9b2326/MODULE_2285528399009206869+3cc9a3cb/model.hlo_module.pb&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;status&quot;: true,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;retry&quot;: 0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;compile_time&quot;: 52.13106894493103</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;start_time&quot;: 1737076372.4143333,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;compilation_time&quot;: 225.76303887367249</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2025-01-17 01:16:38.000177:  37654  INFO ||NEURON_PARALLEL_COMPILE||: Total graphs: 8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2025-01-17 01:16:38.000177:  37654  INFO ||NEURON_PARALLEL_COMPILE||: Total successful compilations: 8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2025-01-17 01:16:38.000177:  37654  INFO ||NEURON_PARALLEL_COMPILE||: Total failed compilations: 0</span><br></span></code></pre></div></div>
<p>Now, let&#x27;s submit the real training job:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sbatch --job-name run_llama3_70B </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">       </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">--output</span><span class="token plain"> logs/run_llama3_70B.out </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">       </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">--exclusive</span><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">--nodes</span><span class="token plain"> </span><span class="token number">16</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">       </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">--wrap</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;srun --auto-resume=1 bash </span><span class="token string variable" style="color:rgb(189, 147, 249);font-style:italic">$(</span><span class="token string variable builtin class-name" style="color:rgb(189, 147, 249);font-style:italic">pwd</span><span class="token string variable" style="color:rgb(189, 147, 249);font-style:italic">)</span><span class="token string" style="color:rgb(255, 121, 198)">/run_llama3_70B_tp_pp.sh&quot;</span><br></span></code></pre></div></div>
<p>Once submitted, you can track the logs</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token function" style="color:rgb(80, 250, 123)">tail</span><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(189, 147, 249);font-style:italic">-f</span><span class="token plain"> logs/run_llama3_70B.out</span><br></span></code></pre></div></div>
<p>You will eventually see</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">step 1 step_time 244.7649691104889s throughput 3.935193537274158 seq/s loss 13.412860887125134 grad norm 1.9788274765014648</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">step 2 step_time 243.86105298995972s throughput 3.9816862016317915 seq/s loss 13.413119042292237 grad norm 1.9752838611602783</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">step 3 step_time 243.8063349723816s throughput 4.004761851017232 seq/s loss 13.412440737709403 grad norm 1.976004958152771</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">step 4 step_time 243.819584608078s throughput 4.018679872757218 seq/s loss 13.4128421805799 grad norm 1.9743479490280151</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">step 5 step_time 243.8718819618225s throughput 4.027843716250589 seq/s loss 13.411852965131402 grad norm 1.970628261566162</span><br></span></code></pre></div></div>
<p>which shows that training is running!</p>
<p>Additionally, you will see checkpoints being written. You directory structure would now look like</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">/fsx/ubuntu/llama3_70B/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> pretrained_weight</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    model</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> step_5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     checkpoint</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     done</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     model</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     optim</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     scheduler.pt</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     user_content.pt</span><br></span></code></pre></div></div></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/slurm-blueprints/training/trainium/Llama3-70B/download-dataset"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Downloading the Wiki-corpus datasets</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/category/fine-tuning-1"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Fine Tuning</div></a></nav></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/getting-started/orchestrated-by-eks/initial-cluster-setup">Orchestrated by EKS</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup">Orchestrated by SLURM</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Sites with Sagemaker AI content</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/aws-samples/awsome-distributed-training" target="_blank" rel="noopener noreferrer" class="footer__link-item">Awsome Distributed Training<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://github.com/aws/sagemaker-hyperpod-recipes" target="_blank" rel="noopener noreferrer" class="footer__link-item">SageMaker HyperPod Recipes<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Other AWS related sites</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://aws.training" target="_blank" rel="noopener noreferrer" class="footer__link-item">AWS Training &amp; Certification<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://aws.amazon.com/sagemaker/ai/hyperpod" target="_blank" rel="noopener noreferrer" class="footer__link-item">Amazon SageMaker HyperPod<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://repost.aws" target="_blank" rel="noopener noreferrer" class="footer__link-item">AWS re:Post<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright  2025 AWS WWSO ML Frameworks team. Built with  at AWS.</div></div></div></footer></div>
</body>
</html>