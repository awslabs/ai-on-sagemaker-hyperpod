<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-eks-blueprints/training/megatron-lm/megatron-lm-readme" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">NVIDIA Megatron-LM | AI on Sagemaker Hyperpod</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://awslabs.github.io/img/header_background.png"><meta data-rh="true" name="twitter:image" content="https://awslabs.github.io/img/header_background.png"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="machine learning, generative ai, genai, sagemaker, hyperpod, sagemaker hyperpod,         model training, model inference, nemo framework, pytorch, pytorch framework"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="NVIDIA Megatron-LM | AI on Sagemaker Hyperpod"><meta data-rh="true" name="description" content="MegatronLM is a framework from Nvidia designed for training large language models (LLMs). We recommend reading the following papers to understand the various tuning options available:"><meta data-rh="true" property="og:description" content="MegatronLM is a framework from Nvidia designed for training large language models (LLMs). We recommend reading the following papers to understand the various tuning options available:"><link data-rh="true" rel="icon" href="/img/Amazon-Sagemaker-Icon.jpg"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme" hreflang="en"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"EKS Blueprints","item":"https://awslabs.github.io/docs/category/eks-blueprints"},{"@type":"ListItem","position":2,"name":"Training","item":"https://awslabs.github.io/docs/category/training"},{"@type":"ListItem","position":3,"name":"NVIDIA Megatron-LM","item":"https://awslabs.github.io/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme"}]}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="AI on Sagemaker Hyperpod RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="AI on Sagemaker Hyperpod Atom Feed"><link rel="stylesheet" href="/assets/css/styles.acb24679.css">
<script src="/assets/js/runtime~main.a1723ce1.js" defer="defer"></script>
<script src="/assets/js/main.628d99d4.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="dark";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/Amazon-Sagemaker-Icon.jpg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/Amazon-Sagemaker-Icon.jpg" alt="AI on Sagemaker Hyperpod" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/Amazon-Sagemaker-Icon.jpg" alt="AI on Sagemaker Hyperpod" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI on Sagemaker Hyperpod</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Orchestrated by EKS</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/getting-started/orchestrated-by-eks/initial-cluster-setup">Initial cluster setup</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/trainium/aws-trainium">AWS Trainium</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/ddp/distributed-data-parallel">Distributed Data Parallel</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/fsdp/fully-sharded-data-parallel">Fully Sharded Data Parallel</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme">NVIDIA Megatron LM</a></li><li><a class="dropdown__link" href="/docs/eks-blueprints/training/ray-train/ray-train-readme">Ray Train</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Orchestrated by SLURM</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup">Initial cluster setup</a></li><li><a class="dropdown__link" href="/docs/slurm-blueprints/training/trainium/aws-trainium">AWS Trainium</a></li><li><a class="dropdown__link" href="/docs/slurm-blueprints/training/ddp/distributed-data-parallel">Distributed Data Parallel</a></li><li><a class="dropdown__link" href="/docs/slurm-blueprints/training/fsdp/fully-sharded-data-parallel">Fully Sharded Data Parallel</a></li><li><a class="dropdown__link" href="/docs/slurm-blueprints/training/megatron-lm/megatron-lm-readme">NVIDIA Megatron LM</a></li><li><a class="dropdown__link" href="/docs/slurm-blueprints/training/ray-train">Ray Train</a></li></ul></div><a class="navbar__item navbar__link" href="/resources">Useful links</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/awslabs/ai-on-sagemaker-hyperpod" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/Introduction">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/getting-started">Getting Started</a><button aria-label="Expand sidebar category &#x27;Getting Started&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/docs/category/eks-blueprints">EKS Blueprints</a><button aria-label="Collapse sidebar category &#x27;EKS Blueprints&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/category/training">Training</a><button aria-label="Collapse sidebar category &#x27;Training&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/eks-blueprints/training/ddp/distributed-data-parallel">ddp</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/eks-blueprints/training/fsdp/fully-sharded-data-parallel">fsdp</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme">megatron-lm</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/eks-blueprints/training/megatron-lm/megatron-lm-readme">NVIDIA Megatron-LM</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/eks-blueprints/training/ray-train/ray-train-readme">ray-train</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/eks-blueprints/training/trainium/aws-trainium">trainium</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/category/fine-tuning">Fine Tuning</a><button aria-label="Expand sidebar category &#x27;Fine Tuning&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/category/inference">Inference</a><button aria-label="Expand sidebar category &#x27;Inference&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/slurm-blueprints">SLURM Blueprints</a><button aria-label="Expand sidebar category &#x27;SLURM Blueprints&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/sagemaker-hyperpod-recipes">Sagemaker Hyperpod Recipes</a><button aria-label="Expand sidebar category &#x27;Sagemaker Hyperpod Recipes&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/add-ons">Add-Ons</a><button aria-label="Expand sidebar category &#x27;Add-Ons&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/dashboards">Dashboards</a><button aria-label="Expand sidebar category &#x27;Dashboards&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/validation-and-testing">Validation and Testing</a><button aria-label="Expand sidebar category &#x27;Validation and Testing&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/infrastructure-as-a-code">Infrastructure as a Code</a><button aria-label="Expand sidebar category &#x27;Infrastructure as a Code&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/category/eks-blueprints"><span>EKS Blueprints</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/category/training"><span>Training</span></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">megatron-lm</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">NVIDIA Megatron-LM</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Running Megatron-LM on HyperPod EKS</h1></header>
<p><a href="https://github.com/NVIDIA/Megatron-LM" target="_blank" rel="noopener noreferrer">MegatronLM</a> is a framework from Nvidia designed for training large language models (LLMs). We recommend reading the following papers to understand the various tuning options available:</p>
<ul>
<li><a href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noopener noreferrer">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></li>
<li><a href="https://arxiv.org/abs/2104.04473" target="_blank" rel="noopener noreferrer">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</a></li>
<li><a href="https://arxiv.org/pdf/2205.05198" target="_blank" rel="noopener noreferrer">Reducing Activation Recomputatio in Large Transformer Models</a></li>
</ul>
<p>To run a test case, follow these steps:</p>
<ol>
<li>Prepare your environment.</li>
<li>Build a container, download, and preprocess the data.</li>
<li>Train the model.</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-preparation">1. Preparation<a href="#1-preparation" class="hash-link" aria-label="Direct link to 1. Preparation" title="Direct link to 1. Preparation">​</a></h2>
<p>Ensure you have the following prerequisites:</p>
<ul>
<li>A functional HyperPod EKS cluster on AWS with EFA device plugin and NVIDIA device plugin deployed. This should be installed automatically.</li>
<li>Docker installed for building the container image.</li>
<li>An FSx for Lustre filesystem mounted via a persistent volume claim on <code>/fsx</code> in EKS pods. An example of setting up FSx on EKS is available <a href="https://docs.aws.amazon.com/eks/latest/userguide/fsx-csi-create.html" target="_blank" rel="noopener noreferrer">here</a>.</li>
<li>To run distributed training jobs as described in this guide, you must also have the <a href="https://www.kubeflow.org/docs/components/training/" target="_blank" rel="noopener noreferrer">Kubeflow Training Operator</a> installed and configured on your HyperPod EKS cluster. This should be installed automatically via our <a href="https://github.com/aws/sagemaker-hyperpod-cli/tree/main/helm_chart" target="_blank" rel="noopener noreferrer">helm charts</a>. If not installed, please install helm charts.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="11-clone-the-repository">1.1 Clone the Repository<a href="#11-clone-the-repository" class="hash-link" aria-label="Direct link to 1.1 Clone the Repository" title="Direct link to 1.1 Clone the Repository">​</a></h3>
<p>First, clone the awesome-distributed-training repository to get access to the Dockerfile and Kubernetes manifests:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">git clone https://github.com/aws-samples/awsome-distributed-training.git</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd awsome-distributed-training/3.test_cases/megatron/megatron-lm</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-building-the-container">2. Building the Container<a href="#2-building-the-container" class="hash-link" aria-label="Direct link to 2. Building the Container" title="Direct link to 2. Building the Container">​</a></h2>
<ol>
<li>
<p>You should now be in the <code>awsome-distributed-training/3.test_cases/megatron/megatron-lm</code> directory which contains the <code>aws-megatron-lm.Dockerfile</code>.</p>
</li>
<li>
<p>Build the container image:</p>
</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">docker build -t aws-megatron-lm -f aws-megatron-lm.Dockerfile .</span><br></span></code></pre></div></div>
<ol start="3">
<li>Tag and push the image to your container registry:</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">export AWS_REGION=us-east-1  # Set to the AWS region where your EKS cluster and ECR repository are located</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export ACCOUNT=$(aws sts get-caller-identity --query Account --output text)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export REGISTRY=${ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export ECR_REPOSITORY_NAME=aws-megatron-lm</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export REPO_URI=${REGISTRY}/${ECR_REPOSITORY_NAME}:latest</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Create ECR repository if it doesn&#x27;t exist</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">aws ecr describe-repositories --repository-names ${ECR_REPOSITORY_NAME} --region ${AWS_REGION} 2&gt;/dev/null || \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">aws ecr create-repository --repository-name ${ECR_REPOSITORY_NAME} --region ${AWS_REGION}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Login to ECR</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">aws ecr get-login-password --region ${AWS_REGION} | docker login --username AWS --password-stdin ${REGISTRY}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">docker tag ${ECR_REPOSITORY_NAME}:latest ${REGISTRY}/${ECR_REPOSITORY_NAME}:latest</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">docker push ${REGISTRY}/${ECR_REPOSITORY_NAME}:latest</span><br></span></code></pre></div></div>
<p>Now you are all set for distributed training with Megatron-LM on EKS! Proceed to the GPT3 training section for detailed instructions.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-gpt-model-training-on-hyperpod-eks-with-megatron-lm">3. GPT Model Training on HyperPod EKS with Megatron-LM<a href="#3-gpt-model-training-on-hyperpod-eks-with-megatron-lm" class="hash-link" aria-label="Direct link to 3. GPT Model Training on HyperPod EKS with Megatron-LM" title="Direct link to 3. GPT Model Training on HyperPod EKS with Megatron-LM">​</a></h2>
<p>Before proceeding with GPT training setup, please follow the steps described above to prepare your environment.
The following example assumes that you have a PVC named <code>fsx-claim</code> and the <code>REPO_URI</code> environment variable is exported.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="31-determine-compute-resources">3.1 Determine Compute Resources<a href="#31-determine-compute-resources" class="hash-link" aria-label="Direct link to 3.1 Determine Compute Resources" title="Direct link to 3.1 Determine Compute Resources">​</a></h3>
<p>Before running the training, you need to determine the compute resources available on your EKS cluster nodes. This will help you set the correct resource limits for GPUs and EFA (Elastic Fabric Adapter) network interfaces.</p>
<p>Export the following environment variables based on your instance type:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Example for p5.48xlarge</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export INSTANCE_TYPE=ml.p5.48xlarge</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export GPU_PER_NODE=8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export EFA_PER_NODE=32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export NUM_NODES=2</span><br></span></code></pre></div></div>
<p>You can refer to the following table to find the correct values for your instance type:</p>
<table><thead><tr><th>Instance Type</th><th>GPUs</th><th>EFA Interfaces</th></tr></thead><tbody><tr><td>ml.p5.48xlarge</td><td>8</td><td>32</td></tr><tr><td>ml.p5e.48xlarge</td><td>8</td><td>32</td></tr><tr><td>ml.p5en.48xlarge</td><td>8</td><td>16</td></tr><tr><td>ml.p6-b200.48xlarge</td><td>8</td><td>8</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="32-data-preprocessing">3.2 Data Preprocessing<a href="#32-data-preprocessing" class="hash-link" aria-label="Direct link to 3.2 Data Preprocessing" title="Direct link to 3.2 Data Preprocessing">​</a></h3>
<ol>
<li>
<p>Navigate to the GPT3 manifests directory and run the following snippet to create a job container that mounts the fsx volume and downloads the input datasets and vocabulary on it:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd kubernetes/gpt3</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-create-and-apply-the-data-download-job">Step 1: Create and Apply the Data Download Job<a href="#step-1-create-and-apply-the-data-download-job" class="hash-link" aria-label="Direct link to Step 1: Create and Apply the Data Download Job" title="Direct link to Step 1: Create and Apply the Data Download Job">​</a></h4>
<p>Generate the <code>getdata-job.yaml</code> manifest from the template and apply it:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">envsubst &lt; manifests/getdata-job.yaml-template &gt; manifests/getdata-job.yaml</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl apply -f manifests/getdata-job.yaml</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-verify-job-creation">Step 2: Verify Job Creation<a href="#step-2-verify-job-creation" class="hash-link" aria-label="Direct link to Step 2: Verify Job Creation" title="Direct link to Step 2: Verify Job Creation">​</a></h4>
<p>List jobs to confirm creation:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl get jobs</span><br></span></code></pre></div></div>
<p>You should see an entry for <code>getdata-job</code> with information about its status, completions, and age. To get more details about the pods created by the job, run:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl get pods -l job-name=getdata-job</span><br></span></code></pre></div></div>
<p>This will show the pod(s) managed by the job. If you want to describe the job and see events or issues, use:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl describe job getdata-job</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="step-3-monitor-job-progress">Step 3: Monitor Job Progress<a href="#step-3-monitor-job-progress" class="hash-link" aria-label="Direct link to Step 3: Monitor Job Progress" title="Direct link to Step 3: Monitor Job Progress">​</a></h4>
<p>Stream the logs to monitor download progress:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl logs -f job/getdata-job</span><br></span></code></pre></div></div>
<p>You should be able to see output similar to the following once the downloads have completed successfully:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Saving to: &#x27;gpt2-merges.txt&#x27;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> 0K .......... .......... .......... .......... .......... 11% 19.2M 0s</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">50K .......... .......... .......... .......... .......... 22% 55.9M 0s</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">100K .......... .......... .......... .......... .......... 33% 57.3M 0s</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">150K .......... .......... .......... .......... .......... 44% 66.1M 0s</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">200K .......... .......... .......... .......... .......... 56%  106M 0s</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">250K .......... .......... .......... .......... .......... 67%  132M 0s</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">300K .......... .......... .......... .......... .......... 78%  139M 0s</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">350K .......... .......... .......... .......... .......... 89%  133M 0s</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">400K .......... .......... .......... .......... .....     100%  122M=0.007s</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2025-06-20 08:59:58 (62.9 MB/s) - &#x27;gpt2-merges.txt&#x27; saved [456318/456318]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">total 940M</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">drwxr-xr-x 2 root root   33K Jun 20 09:00 .</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">drwxr-xr-x 5 root root   33K Jun 20 08:59 ..</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-rw-r--r-- 1 root root  446K Feb 18  2019 gpt2-merges.txt</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-rw-r--r-- 1 root root 1018K Feb 18  2019 gpt2-vocab.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-rw-r--r-- 1 root root  1.1G Jul 24  2021 oscar-1GB.jsonl</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Download completed.</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="step-4-cleanup">Step 4: Cleanup<a href="#step-4-cleanup" class="hash-link" aria-label="Direct link to Step 4: Cleanup" title="Direct link to Step 4: Cleanup">​</a></h4>
<p>Once the job status is <code>Completed</code>, delete the job and its pod:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl delete -f manifests/getdata-job.yaml</span><br></span></code></pre></div></div>
</li>
<li>
<p><strong>Preprocess the Data</strong></p>
<p>Launch the preprocessing job to convert the downloaded data for training.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">cat manifests/prepdata-job.yaml-template | envsubst &gt; manifests/prepdata-job.yaml</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl apply -f ./manifests/prepdata-job.yaml</span><br></span></code></pre></div></div>
<p>Check pods for <code>prepdata-job</code>:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl get pods -l job-name=prepdata-job</span><br></span></code></pre></div></div>
<p>Monitor the job&#x27;s progress by streaming its logs:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl logs -f job/prepdata-job</span><br></span></code></pre></div></div>
<p>The expected log output from the above command should look similar to the following when preprocessing completes successfully:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-rw-r--r--  1 root root 3.4K Jun 14 02:55 pretrain_vision_classify.py</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-rw-r--r--  1 root root 3.5K Jun 14 02:55 pretrain_vision_dino.py</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-rw-r--r--  1 root root 4.8K Jun 14 02:55 pretrain_vision_inpaint.py</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-rw-r--r--  1 root root 8.2K Jun 14 02:55 pretrain_vlm.py</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-rw-r--r--  1 root root  824 Jun 14 02:55 pyproject.toml</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-rw-r--r--  1 root root 4.0K Jun 14 02:55 setup.py</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">drwxr-xr-x  8 root root  200 Jun 14 02:55 tasks</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">drwxr-xr-x  4 root root   67 Jun 14 02:55 tests</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">drwxr-xr-x  6 root root 4.0K Jun 14 02:55 tools</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Data preprocessing completed.</span><br></span></code></pre></div></div>
<p>After the job status is <code>Completed</code>, clean up the job and its pod:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl delete -f prepdata-job.yaml</span><br></span></code></pre></div></div>
<p>Voilà! The preprocessing job has finished. You are now ready to proceed to the training step.</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="33-distributed-training">3.3 Distributed Training<a href="#33-distributed-training" class="hash-link" aria-label="Direct link to 3.3 Distributed Training" title="Direct link to 3.3 Distributed Training">​</a></h3>
<p>Now that the data is preprocessed, we will pretrain a GPT3 model MegatronLM.  Launch a PyTorchJob with the environment variables:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">export TENSOR_PARALLEL=8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export PIPELINE_PARALLEL=1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export NUM_LAYERS=36</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export HIDDEN_SIZE=4096</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export NUM_ATTENTION_HEADS=32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export SEQ_LENGTH=2048</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export MAX_POSITION_EMBEDDINGS=2048</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export MICRO_BATCH_SIZE=1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export GLOBAL_BATCH_SIZE=288</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cat manifests/pytorchjob.yaml-template | envsubst &gt; manifests/pytorchjob.yaml</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl apply -f ./manifests/pytorchjob.yaml</span><br></span></code></pre></div></div>
<p>The training starts running:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl get pods</span><br></span></code></pre></div></div>
<p>You should see one etcd and one worker pod.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">NAME                    READY   STATUS      RESTARTS   AGE</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">etcd-7787559c74-wpcb9   1/1     Running     0          3m10s</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">megatron-worker-0       1/1     Running     0          3m10s</span><br></span></code></pre></div></div>
<p>Log lines describing the iterations show that the training is working properly.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl logs -f megatron-worker-0</span><br></span></code></pre></div></div>
<p>An abbreviated sample log is shown below:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">using torch.float16 for parameters ...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">------------------------uments ------------------------</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">accumulate_allreduce_grads_in_fp32 .............. False</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">adam_beta1 ...................................... 0.9</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">adam_beta2 ...................................... 0.95</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-------------------- end of arguments ---------------------</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">setting number of micro-batches to constant 288</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt; building GPT2BPETokenizer tokenizer ...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt; padded vocab (size: 50257) with 943 dummy tokens (new size: 51200)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt; initializing torch distributed ...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt; initialized tensor model parallel with size 8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt; initialized pipeline model parallel with size 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt; setting random seeds to 1234 ...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt; compiling dataset index builder ...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">make: Entering directory &#x27;/workspace/Megatron-LM/megatron/core/datasets&#x27;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">time to initialize megatron (seconds): 15.424</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">[after megatron is initialized] datetime: 2024-07-16 22:14:01</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">building GPT model ...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt; number of parameters on (tensor, pipeline) model parallel rank (4, 0): 941594624</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt; building train, validation, and test datasets ...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt; datasets target sizes (minimum size):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    train:      146484375</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    validation: 5863680</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    test:       11520</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">iteration        1/  508626 | consumed samples:          288 | elapsed time per iteration (ms): 255940.5 | learning rate: 0.000E+00 | global batch size:   288 | loss scale: 4294967296.0 | number of skipped iterations:   1 | number of nan iterations:   0 |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">iteration        2/  508626 | consumed samples:          576 | elapsed time per iteration (ms): 243438.3 | learning rate: 0.000E+00 | global batch size:   288 | loss scale: 2147483648.0 | number of skipped iterations:   1 | number of nan iterations:   0 |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">iteration        3/  508626 | consumed samples:          864 | elapsed time per iteration (ms): 243344.4 | learning rate: 0.000E+00 | global batch size:   288 | loss scale: 1073741824.0 | number of skipped iterations:   1 | number of nan iterations:   0 |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...</span><br></span></code></pre></div></div>
<p>You can stop the training job by executing:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl delete -f ./pytorchjob.yaml</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-whats-next">4. What&#x27;s Next?<a href="#4-whats-next" class="hash-link" aria-label="Direct link to 4. What&#x27;s Next?" title="Direct link to 4. What&#x27;s Next?">​</a></h2>
<p>The example is based on the GPT3 example from MegatronLM&#x27;s <a href="https://github.com/NVIDIA/Megatron-LM/blob/main/examples/pretrain_gpt.sh" target="_blank" rel="noopener noreferrer">repository</a>. You can modify <code>NUM_ATTENTION_HEADS</code>, <code>NUM_LAYERS</code>, and <code>HIDDEN_SIZE</code> based on the Table 1 (Page 8) of the document <a href="https://arxiv.org/abs/2104.04473" target="_blank" rel="noopener noreferrer">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</a> to change the model size. You can launch training for different model sizes by setting the environment variables before applying the PyTorchJob, for example: <code>NUM_LAYERS=64 HIDDEN_SIZE=8192 NUM_ATTENTION_HEADS=48</code></p>
<table><thead><tr><th>Model size</th><th>Parameters</th></tr></thead><tbody><tr><td>1.7B</td><td><code>NUM_ATTENTION_HEADS=24 HIDDEN_SIZE=2304 NUM_LAYERS=24</code></td></tr><tr><td>3.6B</td><td><code>NUM_ATTENTION_HEADS=32 HIDDEN_SIZE=3072 NUM_LAYERS=30</code></td></tr><tr><td>7.5B</td><td><code>NUM_ATTENTION_HEADS=32 HIDDEN_SIZE=4096 NUM_LAYERS=36</code></td></tr><tr><td>18.4B</td><td><code>NUM_ATTENTION_HEADS=48 HIDDEN_SIZE=6144 NUM_LAYERS=40</code></td></tr><tr><td>39.1B</td><td><code>NUM_ATTENTION_HEADS=64 HIDDEN_SIZE=8192 NUM_LAYERS=48</code></td></tr><tr><td>76.1B</td><td><code>NUM_ATTENTION_HEADS=80 HIDDEN_SIZE=10240 NUM_LAYERS=60</code></td></tr><tr><td>145.6B</td><td><code>NUM_ATTENTION_HEADS=96 HIDDEN_SIZE=12288 NUM_LAYERS=80</code></td></tr><tr><td>310.1B</td><td><code>NUM_ATTENTION_HEADS=128 HIDDEN_SIZE=16384 NUM_LAYERS=96</code></td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="5-appendix">5. Appendix<a href="#5-appendix" class="hash-link" aria-label="Direct link to 5. Appendix" title="Direct link to 5. Appendix">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="51-benchmark-mode">5.1 Benchmark Mode<a href="#51-benchmark-mode" class="hash-link" aria-label="Direct link to 5.1 Benchmark Mode" title="Direct link to 5.1 Benchmark Mode">​</a></h3>
<p>To run in benchmark mode (i.e., train only, no validation and test), modify the PyTorchJob arguments in the <code>pytorchjob.yaml-template</code> file:</p>
<div class="language-diff codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-diff codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">-        --eval-iters 40 \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-        --eval-interval 1000 \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-        --split 98,2,0 \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+        --eval-iters 0 \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+        --split 100,0,0 \</span><br></span></code></pre></div></div>
<p>Incorrect settings will cause this error message to appear in the training logs:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Traceback (most recent call last):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  File &quot;/workspace/Megatron-LM/pretrain_gpt.py&quot;, line 198, in &lt;module&gt;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    pretrain(train_valid_test_datasets_provider,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  File &quot;/workspace/Megatron-LM/megatron/training.py&quot;, line 227, in pretrain</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    = build_train_valid_test_data_iterators(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  File &quot;/workspace/Megatron-LM/megatron/training.py&quot;, line 1283, in build_train_valid_test_data_iterators</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    build_train_valid_test_data_loaders(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  File &quot;/workspace/Megatron-LM/megatron/training.py&quot;, line 1244, in build_train_valid_test_data_loaders</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    train_ds, valid_ds, test_ds = build_train_valid_test_datasets(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  File &quot;/workspace/Megatron-LM/megatron/training.py&quot;, line 1214, in build_train_valid_test_datasets</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return build_train_valid_test_datasets_provider(train_val_test_num_samples)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  File &quot;/workspace/Megatron-LM/pretrain_gpt.py&quot;, line 186, in train_valid_test_datasets_provider</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ).build()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  File &quot;/workspace/Megatron-LM/megatron/core/datasets/blended_megatron_dataset_builder.py&quot;, line 56, in build</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return self._build_blended_dataset_splits()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  File &quot;/workspace/Megatron-LM/megatron/core/datasets/blended_megatron_dataset_builder.py&quot;, line 76, in _build_blended_dataset_splits</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return self._build_megatron_dataset_splits(blend[0], split, self.sizes)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  File &quot;/workspace/Megatron-LM/megatron/core/datasets/blended_megatron_dataset_builder.py&quot;, line 216, in _build_megatron_dataset_splits</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    self.build_generic_dataset(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  File &quot;/workspace/Megatron-LM/megatron/core/datasets/blended_megatron_dataset_builder.py&quot;, line 258, in build_generic_dataset</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    dataset = cls(*args)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  File &quot;/workspace/Megatron-LM/megatron/core/datasets/gpt_dataset.py&quot;, line 68, in __init__</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    super().__init__(indexed_dataset, indexed_indices, num_samples, index_split, config)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  File &quot;/workspace/Megatron-LM/megatron/core/datasets/megatron_dataset.py&quot;, line 42, in __init__</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    assert num_samples &gt; 0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">AssertionError</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="52-adjust-training-steps">5.2 Adjust Training Steps<a href="#52-adjust-training-steps" class="hash-link" aria-label="Direct link to 5.2 Adjust Training Steps" title="Direct link to 5.2 Adjust Training Steps">​</a></h3>
<p>By default, the PyTorchJob specifies the number of samples, then the number of training steps equals to <code>--train_samples</code> / <code>--global-batch-size</code>. To directly specify the number of steps, modify the arguments in the <code>pytorchjob.yaml-template</code> file. Note that <code>samples</code> and <code>iters</code> are mutually exclusive.</p>
<div class="language-diff codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-diff codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">-        --train-samples 146484375 \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-        --lr-decay-samples 126953125 \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-        --lr-warmup-samples 183105 \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+        --train-iters 50 \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+        --lr-decay-iters 45 \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+        --lr-warmup-iters 2 \</span><br></span></code></pre></div></div>
<p>Following the same pattern, you can train other models. Pretraining scripts for models like
Bert, ICT, and T5 are already included in the Megatron-LM container under <code>/workspace/Megatron-LM</code>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="6-kubernetes-manifests">6. Kubernetes Manifests<a href="#6-kubernetes-manifests" class="hash-link" aria-label="Direct link to 6. Kubernetes Manifests" title="Direct link to 6. Kubernetes Manifests">​</a></h2>
<p>The training setup uses three main Kubernetes manifest templates located in the <code>kubernetes/gpt3/manifests/</code> directory of the cloned repository:</p>
<ul>
<li><strong><code>getdata-job.yaml-template</code></strong> - Downloads training data and vocabulary files</li>
<li><strong><code>prepdata-job.yaml-template</code></strong> - Preprocesses data for training</li>
<li><strong><code>pytorchjob.yaml-template</code></strong> - Runs distributed training using PyTorchJob</li>
</ul>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>The manifest templates use environment variable substitution with <code>envsubst</code>. Make sure all required environment variables are exported before generating the final manifests.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="7-directory-structure">7. Directory Structure<a href="#7-directory-structure" class="hash-link" aria-label="Direct link to 7. Directory Structure" title="Direct link to 7. Directory Structure">​</a></h2>
<p>After cloning the repository, your directory structure should look like this:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">awsome-distributed-training/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">└── 3.test_cases/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    └── megatron/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        └── megatron-lm/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            ├── aws-megatron-lm.Dockerfile</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            ├── README.md</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            ├── kubernetes/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            │   ├── README.md</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            │   └── gpt3/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            │       ├── README.md</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            │       └── manifests/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            │           ├── getdata-job.yaml-template</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            │           ├── prepdata-job.yaml-template</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            │           └── pytorchjob.yaml-template</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            └── slurm/ </span><br></span></code></pre></div></div>
<p>For additional examples and configurations, refer to the <a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/megatron/megatron-lm" target="_blank" rel="noopener noreferrer">awesome-distributed-training repository</a> and the <a href="https://github.com/NVIDIA/Megatron-LM" target="_blank" rel="noopener noreferrer">Megatron-LM GitHub repository</a>.</p></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/eks-blueprints/training/fsdp/fully-sharded-data-parallel"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Fully Sharded Data Parallelism (FSDP)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/eks-blueprints/training/ray-train/ray-train-readme"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Ray Train</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#1-preparation" class="table-of-contents__link toc-highlight">1. Preparation</a><ul><li><a href="#11-clone-the-repository" class="table-of-contents__link toc-highlight">1.1 Clone the Repository</a></li></ul></li><li><a href="#2-building-the-container" class="table-of-contents__link toc-highlight">2. Building the Container</a></li><li><a href="#3-gpt-model-training-on-hyperpod-eks-with-megatron-lm" class="table-of-contents__link toc-highlight">3. GPT Model Training on HyperPod EKS with Megatron-LM</a><ul><li><a href="#31-determine-compute-resources" class="table-of-contents__link toc-highlight">3.1 Determine Compute Resources</a></li><li><a href="#32-data-preprocessing" class="table-of-contents__link toc-highlight">3.2 Data Preprocessing</a></li><li><a href="#33-distributed-training" class="table-of-contents__link toc-highlight">3.3 Distributed Training</a></li></ul></li><li><a href="#4-whats-next" class="table-of-contents__link toc-highlight">4. What&#39;s Next?</a></li><li><a href="#5-appendix" class="table-of-contents__link toc-highlight">5. Appendix</a><ul><li><a href="#51-benchmark-mode" class="table-of-contents__link toc-highlight">5.1 Benchmark Mode</a></li><li><a href="#52-adjust-training-steps" class="table-of-contents__link toc-highlight">5.2 Adjust Training Steps</a></li></ul></li><li><a href="#6-kubernetes-manifests" class="table-of-contents__link toc-highlight">6. Kubernetes Manifests</a></li><li><a href="#7-directory-structure" class="table-of-contents__link toc-highlight">7. Directory Structure</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/getting-started/orchestrated-by-eks/initial-cluster-setup">Orchestrated by EKS</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/getting-started/orchestrated-by-slurm/initial-cluster-setup">Orchestrated by SLURM</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Sites with Sagemaker AI content</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/aws-samples/awsome-distributed-training" target="_blank" rel="noopener noreferrer" class="footer__link-item">Awsome Distributed Training<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://github.com/aws/sagemaker-hyperpod-recipes" target="_blank" rel="noopener noreferrer" class="footer__link-item">Sagemaker Hyperpod Recipes<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Other AWS related sites</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://aws.training" target="_blank" rel="noopener noreferrer" class="footer__link-item">AWS Training &amp; Certification<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://aws.amazon.com/sagemaker/ai/hyperpod" target="_blank" rel="noopener noreferrer" class="footer__link-item">Amazon Sagemaker Hyperpod<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://repost.aws" target="_blank" rel="noopener noreferrer" class="footer__link-item">AWS re:Post<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 AWS WWSO ML Frameworks team. Built with ❤️ at AWS.</div></div></div></footer></div>
</body>
</html>